[{"title":"Alert Management","type":0,"sectionRef":"#","url":"docs/Alerts_notifications/alert_management","content":"","keywords":""},{"title":"Coming Soon...​","type":1,"pageTitle":"Alert Management","url":"docs/Alerts_notifications/alert_management#coming-soon","content":""},{"title":"Sending notifications to Slack","type":0,"sectionRef":"#","url":"docs/Alerts_notifications/slack","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Sending notifications to Slack","url":"docs/Alerts_notifications/slack#overview","content":"SnappyFlow sends alerts/notifications to Slack and alerts can be configured to fit your existing business processes "},{"title":"Step 1: Create Slack Token​","type":1,"pageTitle":"Sending notifications to Slack","url":"docs/Alerts_notifications/slack#step-1-create-slack-token","content":""},{"title":"Create New App For Your Workspace​","type":1,"pageTitle":"Sending notifications to Slack","url":"docs/Alerts_notifications/slack#create-new-app-for-your-workspace","content":"Visit https://api.slack.com/appsClick on create new appEnter your Slack app nameSelect the workspace from the drop-downClick Create App "},{"title":"Provide Permission Scope​","type":1,"pageTitle":"Sending notifications to Slack","url":"docs/Alerts_notifications/slack#provide-permission-scope","content":"Select OAuth &amp; Permissions under Features section from left side menu. Scroll down to Scope section and find Bot Token Scope. Click Add an Oauth Scope and select chat:write:public, im:read, mpim:read, groups:read, channels:read scope from drop-down It automatically adds chat:write scope along with it. note There is another scope below the Bot Token Scope called as user Token Scope. Select bot token scope and not the user token scope. "},{"title":"Install Slack App To Workspace & Generate Token​","type":1,"pageTitle":"Sending notifications to Slack","url":"docs/Alerts_notifications/slack#install-slack-app-to-workspace--generate-token","content":"scroll-up and Click Install App to Workspace.Click Allow to Provide permission for newly created App.Copy the Bot User OAuth Access Token and paste token in APM Slack settings. "},{"title":"Features and Functionality(optional)​","type":1,"pageTitle":"Sending notifications to Slack","url":"docs/Alerts_notifications/slack#features-and-functionalityoptional","content":"Select &quot;App Home&quot; under Features section from left side menu.Scroll down to Your App’s Presence in Slack sectionEnable Always show My Bot as Online in App Home "},{"title":"Step 2: Add Slack Token to SnappyFLow​","type":1,"pageTitle":"Sending notifications to Slack","url":"docs/Alerts_notifications/slack#step-2-add-slack-token-to-snappyflow","content":"Under the Manage tab in SnappyFlow portal, select your profile and click on notifications.Select notification type as Slack and give a name of your choice and add the token generated in step 1 in the Token field  "},{"title":"Step 3: Add Slack notification to the Project​","type":1,"pageTitle":"Sending notifications to Slack","url":"docs/Alerts_notifications/slack#step-3--add-slack-notification-to-the-project","content":"  "},{"title":"Step 4: Setup Alerts​","type":1,"pageTitle":"Sending notifications to Slack","url":"docs/Alerts_notifications/slack#step-4-setup-alerts","content":"From the Application Dashboard, In the Alert Definition page , Edit the Alerts and check the “Notify” option  "},{"title":"Sending notifications to PagerDuty","type":0,"sectionRef":"#","url":"docs/Alerts_notifications/pager_duty","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Sending notifications to PagerDuty","url":"docs/Alerts_notifications/pager_duty#overview","content":"SnappyFlow sends alerts/notifications to PagerDuty and alerts can be configured to fit your existing business processesThe integration is simple to setup and takes less than 5 minutesFilter conditions can be added to alerts before they are sent to PagerDuty cutting down on unnecessary notifications "},{"title":"How it Works​","type":1,"pageTitle":"Sending notifications to PagerDuty","url":"docs/Alerts_notifications/pager_duty#how-it-works","content":"Any alerts configured in your SnappyFlow account for a given profile/user can be sent to PagerDuty. Once an alert is generated in SnappyFlow, a notification is triggered and communicated to PagerDuty via an API call. An integration key (generated by PagerDuty) is used to facilitate this API call. "},{"title":"Requirements​","type":1,"pageTitle":"Sending notifications to PagerDuty","url":"docs/Alerts_notifications/pager_duty#requirements","content":"PagerDuty account access with User, Admin, Manager, Global Admin or Account Owner rolesAlerts configured in your SnappyFlow portal for specific users/profilesSnappyFlow trial account users need to upgrade their trial account to Premium access. To do so, login to SnappyFlow portal and click on “Go to Portal”. On the portal page, click on your account on the top right corner and select “Upgrade Account”. Once the account is upgraded, you will receive a confirmation email stating the same "},{"title":"Support​","type":1,"pageTitle":"Sending notifications to PagerDuty","url":"docs/Alerts_notifications/pager_duty#support","content":"If you need help with this integration, please contact support@snappyflow.io "},{"title":"Integration Walkthrough​","type":1,"pageTitle":"Sending notifications to PagerDuty","url":"docs/Alerts_notifications/pager_duty#integration-walkthrough","content":"Integration with SnappyFlow is a simple two-step process. Create or use an existing service in PagerDuty to generate an integration keyCreate a new notification in SnappyFlow using the integration key "},{"title":"In PagerDuty​","type":1,"pageTitle":"Sending notifications to PagerDuty","url":"docs/Alerts_notifications/pager_duty#in-pagerduty","content":"In your PagerDuty account portal From the Configuration menu, select Services There are two ways to add an integration to a service: If you are adding your integration to an existing service: Click the name of the Service you want to add the integration to. Then, select the Integrations tab and click the New Integration button If you are creating a new service for your integration: Navigate to Services, select Service Directory and click the +New Service button Give the service a name depending on the application, component or team that you wish to open incidents against (examples: &quot;MobileApp&quot;, &quot;Shopping Cart&quot; or &quot;BizOps&quot;). Please note that when an incident is triggered, this is the service name it will be associated with Add a Description of what this service represents in your infrastructure Enter an Integration Name in the format SnappyFlow-name (e.g., SnappyFlow-MobileApp) and select SnappyFlow from the Integration Type menu Click the Add Service button to save your new integration. You will be redirected to the Service home page. Click on Integrations tab  An Integration Key will be generated on this screen. Keep this key saved in a safe place, as it will be used when you configure the integration with SnapyFlow in the next section "},{"title":"In SnappyFlow​","type":1,"pageTitle":"Sending notifications to PagerDuty","url":"docs/Alerts_notifications/pager_duty#in-snappyflow","content":"Login to SnappyFlow and click on Go To PortalNavigate to Manage Tab and click on Profiles icon on the leftYou will now see a list of profiles/users  Select a profile by clicking on the downward facing expand button. Only alerts configured for this profile will be sent to PagerDutyClick on notifications and a list of existing notifications will be displayed. Click on Add New button on the right  Under Type, select PagerDuty  Provide a name for this notificationPaste the integration key generated in PagerDuty and click on Add buttonOnce added, the integration will appear with a green Active status  Once the integration is setup, SnappyFlow sends out a test notification to PagerDutyTo add specific applications, navigate to Applications Tab and click on Edit Project button (highlighted by the red box below)  In the Edit Project window, click on Notification tab and select PagerDuty from the drop-down menu option. Click on Configure button to proceed  Select the integrations that were created and click on the Tick button  Click on Save and Close button to complete the process. All alerts from this project will start appearing in PagerDuty "},{"title":"Removing PagerDuty integration from SnappyFlow​","type":1,"pageTitle":"Sending notifications to PagerDuty","url":"docs/Alerts_notifications/pager_duty#removing-pagerduty-integration-from-snappyflow","content":"Navigate to the profile where PagerDuty notifications are setup and click on Trash icon to delete the integration and stop sending notifications to PagerDuty.  "},{"title":"SLO","type":0,"sectionRef":"#","url":"docs/Alerts_notifications/slo","content":"","keywords":""},{"title":"Coming Soon...​","type":1,"pageTitle":"SLO","url":"docs/Alerts_notifications/slo#coming-soon","content":""},{"title":"Getting Started","type":0,"sectionRef":"#","url":"docs/Alerts_notifications/getting_started","content":"","keywords":""},{"title":"Alerts in SnappyFlow​","type":1,"pageTitle":"Getting Started","url":"docs/Alerts_notifications/getting_started#alerts-in-snappyflow","content":" "},{"title":"Defining the scope of alerts​","type":1,"pageTitle":"Getting Started","url":"docs/Alerts_notifications/getting_started#defining-the-scope-of-alerts","content":" "},{"title":"Key Concepts​","type":1,"pageTitle":"Getting Started","url":"docs/Alerts_notifications/getting_started#key-concepts","content":"To use SnappyFlow alerts well, it will help you to understand the terms we use: Alert Terms\tCommentAlert Metric\tA monitored data source like CPU utilization or RAM utilization. Condition\tAn alert condition includes a) a monitored metric and b) a thresholds that define the behavior that is considered a violation. For instance, an NGINX alert condition could be defined as response time for an http request exceeding the 95th percentile of all response times. Threshold\tWhen you create a condition, there is a required critical-level threshold. In the above example 95th percentile is the threshold. Violation\tA violation occurs when the value of a data source crosses an alert condition’s threshold. This leads to the creation of a violation event. A violation does not automatically create a notification. Alerts history lists all the violations, with the most recent ones at the top. Notification\tAt the alert policy level, you choose if a violation needs to be notified. SnappyFlow offers several notification channels such as email, Slack and webhooks. You can also include charts about the violation to provide context and share them with your team. "},{"title":"Basic Terminology​","type":1,"pageTitle":"Getting Started","url":"docs/Alerts_notifications/getting_started#basic-terminology","content":""},{"title":"Primary metric​","type":1,"pageTitle":"Getting Started","url":"docs/Alerts_notifications/getting_started#primary-metric","content":"This is the metric on which alerts are defined. Primary Metric is sampled at the defined “periodicity” and for the duration of “sampling period”. Users can define Primary Metric in following ways: Single real number variable: E.g. CPUUtil calculates average of the variable over the sampling period Expression: E.g. DiskUtil= usage*100/capacity calculates the expression using average values of the variables in the expression Aggregations: All variables implicitly use average aggregation. However, users can explicitly a specific aggregation using suffixes such as .min, .max, .sum, .count, .95P, .term(last value)  "},{"title":"Periodicity​","type":1,"pageTitle":"Getting Started","url":"docs/Alerts_notifications/getting_started#periodicity","content":"Periodicity at which the alert is fired. It can be defined as 5m (5 minutes) or 1h (1 hour) or 1d (1 day). "},{"title":"Sampling Period​","type":1,"pageTitle":"Getting Started","url":"docs/Alerts_notifications/getting_started#sampling-period","content":"Sampling window in which primary metric is evaluated. It can be defined as 5m (5 minutes) or 1h (1 hour) or 1d (1 day). "},{"title":"Secondary metric​","type":1,"pageTitle":"Getting Started","url":"docs/Alerts_notifications/getting_started#secondary-metric","content":"Users can choose to collect additional metrics along with the primary metric. Users can add secondary metrics to alert text to provide additional diagnostics. For example, it is useful to accompany disk latency metric with read/write IOPS and throughput. Secondary Metric is also very useful to define multi-variable alert conditions. "},{"title":"Alert Conditions​","type":1,"pageTitle":"Getting Started","url":"docs/Alerts_notifications/getting_started#alert-conditions","content":"Defines alert conditions for severity (sev1/2/3) alerts. A condition compares present value(PV)ofaprimarymetricwithathreshold(PV) of a primary metric with a threshold (PV)ofaprimarymetricwithathreshold(TH). Multiple ways to express thresholds are supported. Absolute value: E.g. CPUUtil GT 90Time based thresholds: E.g. TCPReset GT (1h,avg) which denotes “check if average of TCPReset calculated over the sampling period is GT average of TCPReset calculated over the last 1 hour preceding the sampling period”. Time period can be defined as Xm (e.g. 30m), Xh (e.g. 1h) or Xd(e.g. 2 d)Time based threshold with limits: E.g. TCPReset GT max(200,(1h,avg)) which means enable the alert only when TCPReset &gt; 200Past reference time: E.g. Response_Time.95P &gt; dod(1h,95P) compares the 95 th percentile of response time over the sampling period with the 95 th percentile of response time over a 1 hour period exactly one day back. Expressions supported are dod for Day on Day, wow for Week on Week and mom for Month on Month "},{"title":"Constructing an Alert text​","type":1,"pageTitle":"Getting Started","url":"docs/Alerts_notifications/getting_started#constructing-an-alert-text","content":"Creating a meaningful alert message is important for diagnostics. Users can create effective alerts using parameterization. For e.g., “CPU utilization is high. PV=PV,TH=PV, TH=PV,TH=TH”. $PV is the current value of the primary metric and $TH is the defined alert threshold“Read latency is high. PV=PV,TH=PV, TH=PV,TH=TH,read_IOPS=$read_IOPS, Write_IOPS= $write_IOPS”. In this example,read_IOPS and write_IOPS are secondary metrics collected to provide additional context for analysis "},{"title":"Scope of an Alert​","type":1,"pageTitle":"Getting Started","url":"docs/Alerts_notifications/getting_started#scope-of-an-alert","content":"Alert can be executed in the following modes. Apply check to all instances: Enabling this option will cause the check to be operated on all the instances in the application. Condition tag_Name == * is inserted in the where clause of the alertApply check for all values of custom tag: Enabling this option will cause the check to be operated on all unique values of the custom tag. E.g. If user wants to specify an alert to check the health of each index in Elasticsearch and raise a separate alert for each issue found, user will have to specify the condition check index_health != green where index_name==* . In this example, the custom tag is index_nameApply check to a specific data point: This option is used when the check is very specific to a single data point e.g. check if Cluster_status != Green. No automatic tags are inserted if this option is enabled "},{"title":"Dashboard Management","type":0,"sectionRef":"#","url":"docs/Dashboards/dashboard_management","content":"","keywords":""},{"title":"Coming Soon...​","type":1,"pageTitle":"Dashboard Management","url":"docs/Dashboards/dashboard_management#coming-soon","content":""},{"title":"Forwarding AWS Lambda logs to SnappyFlow","type":0,"sectionRef":"#","url":"docs/Integrations/aws_lambda","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Forwarding AWS Lambda logs to SnappyFlow","url":"docs/Integrations/aws_lambda#overview","content":"The AWS Lambda Extension is designed to forward logs from your AWS Lambda functions to SnappyFlow without requiring an external transport such as CloudWatch Logs. This lightweight extension runs alongside your AWS Lambda functions. Submitting Lambda logs with the extension is supported in all Lambda runtimes.  "},{"title":"Installation​","type":1,"pageTitle":"Forwarding AWS Lambda logs to SnappyFlow","url":"docs/Integrations/aws_lambda#installation","content":""},{"title":"Enable Lambda extension in your function​","type":1,"pageTitle":"Forwarding AWS Lambda logs to SnappyFlow","url":"docs/Integrations/aws_lambda#enable-lambda-extension-in-your-function","content":"As Lambda Layer Add the Lambda Layer for the SnappyFlow extension to your AWS Lambda function with the following ARN: arn:aws:lambda:&lt;AWS_REGION&gt;:106947364898:layer:sf-lambda-extension:3 Copy Replace &lt;AWS_REGION&gt; with the same AWS region as your Lambda Function, for example, us-west-2 As Container Image Add the SnappyFlow Lambda extension to your container image by adding the following to your Dockerfile: COPY --from=snappyflowml/sf-lambda-extension:latest /opt/extensions/ /opt/extensions Copy Add the environment variable SF_PROFILE_KEY and set the value to your profile key copied from SnappyFlow Add environment variables APP_NAME and PROJECT_NAME with appropriate values "},{"title":"Monitoring ActiveMQ Message Broker running on Instances","type":0,"sectionRef":"#","url":"docs/Integrations/activemq","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Monitoring ActiveMQ Message Broker running on Instances","url":"docs/Integrations/activemq#overview","content":"Activemq sfAgent plugin provides metrics related to message traffic distribution and other internal transactions among the brokers. Metrics collected by the plugin are organized across the following categories Broker stats: contain transactional data and metrics related to broker stateTopic stats: provide metrics for analyzing internal transactions associated with each topicQueue stats: provide metrics for analyzing internal transactions associated with each queueJVM stats: contain all JVM related metrics like garbage collection details, memory pools, loaded/unloaded classes etc. Activemq logger plugin collects general logs comprising state change and broker specific information generated by the activemq message broker "},{"title":"Prerequisites​","type":1,"pageTitle":"Monitoring ActiveMQ Message Broker running on Instances","url":"docs/Integrations/activemq#prerequisites","content":"Activemq Metric Plugin is based on Jolokia agent which requires JMX monitoring to be enable locally. Following property needs to be included during the start of activemq process -Dcom.sun.management.jmxremote Copy JCMD command must be installed in the machine "},{"title":"Configuration​","type":1,"pageTitle":"Monitoring ActiveMQ Message Broker running on Instances","url":"docs/Integrations/activemq#configuration","content":"sfAgent section provides steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: &lt;Profile_key&gt; tags: Name: &lt;instance_name&gt; appName: &lt;app_name&gt; projectName: &lt;project_name&gt; metrics: plugins: - name: activemq enabled: true interval: 300 config: process: activemq port: 8161 documentsTypes: - brokerStats - topicStats - queueStats - jvmStats logging: plugins: - name: activemq-log enabled: true config: log_path: &lt;..activemq logpath..&gt; log_level: - error - warning - info - warn Copy "},{"title":"Parameters required in metrics plugin​","type":1,"pageTitle":"Monitoring ActiveMQ Message Broker running on Instances","url":"docs/Integrations/activemq#parameters-required-in-metrics-plugin","content":"process: Activemq process name (It should be part of java main class) port: Broker Port documentTypes: User can either leave this empty to collect all documentTypes or mention specific documentTypes to collect. Available options for plugin type activemq are brokerStats, topicStats, queueStats, jvmStats Logger plugin requires log path to be specified. Wildcard characters are supported "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"Monitoring ActiveMQ Message Broker running on Instances","url":"docs/Integrations/activemq#viewing-data-and-dashboards","content":"Data collected by plugins can be viewed in SnappyFlow’s browse data section Metrics plugin: activemqdocumentType: brokerStats, topicStats, queueStats, jvmStats Dashboard template: ActiveMQ Logs Plugin: activemqdocumentType: activemq-logs "},{"title":"See Also​","type":1,"pageTitle":"Monitoring ActiveMQ Message Broker running on Instances","url":"docs/Integrations/activemq#see-also","content":"Zookeeper Elasticsearch Kafka-REST Kafka-Connect "},{"title":"Elastic Load Balancers","type":0,"sectionRef":"#","url":"docs/Integrations/elb/elb_rds","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Elastic Load Balancers","url":"docs/Integrations/elb/elb_rds#overview","content":"Amazon Elastic Load Balancer(ELB) is used to automatically distribute incoming app traffic across AWS Instances which may be in different availability zones. ELB monitoring will collect all metrics provided by AWS for all three load balancer types i.e Classic, Network and Application Load Balancer. "},{"title":"Prerequisites​","type":1,"pageTitle":"Elastic Load Balancers","url":"docs/Integrations/elb/elb_rds#prerequisites","content":""},{"title":"CloudWatch Access for IAM Role​","type":1,"pageTitle":"Elastic Load Balancers","url":"docs/Integrations/elb/elb_rds#cloudwatch-access-for-iam-role","content":"Provide Read only access for CloudWatch to the dedicated IAM Role used for APM. You can use AWS managed polices that addresses many common use cases by providing standalone IAM policies that are created and administered by AWS. Attach this AWS policy CloudWatchReadOnlyAccess to IAM role to get read access for all CloudWatch else create the below custom policy and attach it to IAM.  { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Action&quot;: [ &quot;cloudwatch:Describe*&quot;, &quot;cloudwatch:Get*&quot;, &quot;cloudwatch:List*&quot;, &quot;logs:Get*&quot;, &quot;elasticloadbalancing:Describe*&quot; ], &quot;Effect&quot;: &quot;Allow&quot;, &quot;Resource&quot;: &quot;*&quot; } ] } Copy note Health check interval should be less than 300 Secs, since querying for data is 5mins interval it might report incorrect data from AWS. "},{"title":"sfPoller Configuration​","type":1,"pageTitle":"Elastic Load Balancers","url":"docs/Integrations/elb/elb_rds#sfpoller-configuration","content":"Select ELB Endpoint Type in Add Endpoints and add the load balancer name: Add Endpoint Select ELB Endpoint Enter the loadbalancer name Select the plugin from the dropdown under Plugins tab and config the polling interval. Plugin configuration for ELB services this includes Classic, Network and Application plugin. You can enable/disable any of the plugin based on your needs and instance support. Cloudwatch-classic - Collects data for classic load balancer Cloudwatch-network - collects data for Network load balancersCloudwatch-application - collects data for Application load balancers. Add the appropriate plugins "},{"title":"View Data and Dashboards​","type":1,"pageTitle":"Elastic Load Balancers","url":"docs/Integrations/elb/elb_rds#view-data-and-dashboards","content":"All CloudWatch metrics are collected and tagged based on their ELB type to get displayed in their respective dashboard template. Use ELB_Network, ELB_Application and ELB_Classic for data visualization as per the ELB. "},{"title":"Monitoring Apache Server on Instances","type":0,"sectionRef":"#","url":"docs/Integrations/apache/overview","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Monitoring Apache Server on Instances","url":"docs/Integrations/apache/overview#overview","content":"Apache Server’s monitoring involves monitoring of the following elements: Apache Access Logs Apache Error Logs Apache Server Health "},{"title":"Pre-requisites​","type":1,"pageTitle":"Monitoring Apache Server on Instances","url":"docs/Integrations/apache/overview#pre-requisites","content":"Ensure Apache access logs are in format expected by sfAgent parser. Edit configuration file and set log format as follows: LogFormat &quot;%h %l %u %t \\&quot;%r\\&quot; %&gt;s %b \\&quot;%{Referer}i\\&quot; \\&quot;%{User-Agent}i\\&quot; %D %v&quot; combined CustomLog &quot;logs/access_log&quot; combined Copy After configuring log format, the expected log entry would be: 45.112.52.50 - - [28/Jun/2020:23:34:10 -0700] &quot;GET / HTTP/1.1&quot; 302 242 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36&quot; 271 Copy Apache configuration file can be found in these paths: Ubuntu: /etc/apache2/mods-enabled/status.conf Centos: /etc/httpd/conf/httpd.conf Check if Apache status module is enabled This is required to monitor Apache server health. Apache web server exposes metrics through its status module, mod_status. If apache server is running and mod_status is enabled, apache server’s status page should be available at http://127.0.0.1/server-status. Alternatively, you can check is mod_status is enabled by running the following commands: Ubuntu(or Debian based systems): sudo apache2ctl -M | grep status_module Copy Centos/RHEL/Fedora sudo httpd -M | grep status_module Copy if output of above command is status_module , then apache status module is enabled. If mod_status is not enabled , follow next step to enable it. Enable Apache status module In order to enable mod_status , edit the status module’s configuration file (on Debian platforms), or your main Apache configuration file (all other Unix-like platforms). Debian users can find the status module’s configuration file in /etc/apache2/mods-enabled/status.conf Users of other platforms (such as Red Hat–based systems) will find their main configuration file in /etc/apache2/apache2.conf, /etc/httpd/conf/httpd.conf, or /etc/apache2/httpd.conf. In the main configuration file, allow access from local or from a specific ip address as shown below: &lt;Location /server-status&gt; SetHandler server-status Require local # Require all granted Require ip x.x.x.x &lt;/Location&gt; Copy Check your configuration file for errors with the following command: apachectl configtest Copy Perform a graceful restart to apply the changes without interrupting live connections: (apachectl -k graceful or service apache2 graceful) Copy After enabling mod_status and restarting Apache, status page is accessible at http://localhost/server-status or http://ipaddress/server-status. "},{"title":"Configuration​","type":1,"pageTitle":"Monitoring Apache Server on Instances","url":"docs/Integrations/apache/overview#configuration","content":"sfAgent section provides steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: &lt;profile key&gt; generate_name: true tags: Name: &lt;unique instance name or will be generated from IP&gt; appName: &lt;add application name&gt; projectName: &lt;add project name&gt; metrics: plugins: - name: apache enabled: true interval: 300 config: port: 80 secure: false location: server-status logging: plugins: - name: apache-access enabled: true config: geo_info: true log_path: /var/log/apache2/access.log, var/log/apache2/access_log ua_parser: false - name: apache-error enabled: true config: log_level: - notice - warning - error log_path: /var/log/apache2/error.log, /var/log/httpd/error_log Copy "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"Monitoring Apache Server on Instances","url":"docs/Integrations/apache/overview#viewing-data-and-dashboards","content":"Data generated by plugin can be viewed in browse data page inside the respective application under plugin=apache and documentType=apache. Logger data is available inside plugin=apache-access with documentType=apacheAccess and plugin=apache-error with documentType=apacheError. Dashboard for this data can be instantiated by Importing dashboard template Apache_Server and Apache_Access to the application dashboard. "},{"title":"Monitoring HAProxy on Instances","type":0,"sectionRef":"#","url":"docs/Integrations/haproxy","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Monitoring HAProxy on Instances","url":"docs/Integrations/haproxy#overview","content":"HAProxy is a free, very fast and reliable solution offering high availability, load balancing and proxying for TCP and Http-based applications. HAProxy monitoring involves monitoring of the following aspects: HAProxy Access Logs HAProxy Logs HAProxy Metrics "},{"title":"Prerequisites​","type":1,"pageTitle":"Monitoring HAProxy on Instances","url":"docs/Integrations/haproxy#prerequisites","content":"Configure HAProxy Access Logs Configure HAProxy Stats Set socket path and enable logs to send desired log file under global section in below haproxy config path. Path:/etc/haproxy/haproxy.conf Example Global log /dev/log local0 stats socket /run/haproxy/admin.sock mode 660 level admin Copy Configure the HAProxy in following manner to enable server logs. Refer link for configuration of haproxy to send access logs to a log file in centos also configure Rsyslog to collect haproxy logs. Create a directory to run haproxy service using sudo. mkdir /run/haproxy Under listen section add below lines to capture access logs: Capture request header User-Agent len 128. log-format %ci: %cp\\ [%tr]\\ %ft\\ %b/%s\\ %TR/%Tw/%Tc/%Tr/%Ta\\ %ST\\ %B\\ %CC\\ %CS\\ %tsc\\ %ac/%fc/%bc/%sc/%rc\\ %sq/%bq\\ %hr\\ %hs\\ %{+Q}r Copy Add below line to frontend configuration to capture requests: capture request header User-Agent len 128.  "},{"title":"Configuration​","type":1,"pageTitle":"Monitoring HAProxy on Instances","url":"docs/Integrations/haproxy#configuration","content":"sfAgent section provides steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory. key: &lt;profile key&gt; generate_name: true tags: Name: &lt;unique instance name or will be generated from IP&gt; appName: &lt;add application name&gt; projectName: &lt;add project name&gt; metrics: metrics: plugins: - name: haproxy enabled: true interval: 30 logging: plugins: - name: haproxy-access enabled: true config: geo_info: true log_path: /var/log/haproxy.log ua_parser: false - name: haproxy-general enabled: true config: log_level: - emerg - alert - error log_path: /var/log/haproxy.log Copy "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"Monitoring HAProxy on Instances","url":"docs/Integrations/haproxy#viewing-data-and-dashboards","content":"Data collected by plugins can be viewed in SnappyFlow’s browse data section under metrics section plugin: haproxy, haproxy-access, haproxy-general documentType: frontEndStats, backEndStats, systemInfo, backEndServerDetails, haproxyAccessLogs, haproxyGeneralLogs Dashboard template: HAProxy_Server, HAProxy_Access Test Matrix Ubuntu: HAProxy version (1.6.3) Centos: HAProxy version (1.5.18) "},{"title":"See Also​","type":1,"pageTitle":"Monitoring HAProxy on Instances","url":"docs/Integrations/haproxy#see-also","content":"Linux Monitoring Nginx Apache Server For help with plugins, please reach out to support@snappyflow.io. "},{"title":"IIS Web Server and Access Logging Setup","type":0,"sectionRef":"#","url":"docs/Integrations/iis/iis_setup","content":"","keywords":""},{"title":"IIS Server Setup​","type":1,"pageTitle":"IIS Web Server and Access Logging Setup","url":"docs/Integrations/iis/iis_setup#iis-server-setup","content":"Open Server Manager from Start menu.  Select &quot;Add roles and features&quot; on Server Manager Dashboard.  A wizard will open, click on next.  Click on next.  Again, click on next.  Select &quot;Web Server(IIS)&quot; in Server Roles.  Click on &quot;Add Features&quot;  Click on next.  Click on next.  In Role Services, select all features under &quot;Common HTTP features&quot;, &quot;Health and Diagnostics&quot; and &quot;Performance&quot;.   Install.  Wait for installation to complete.  After Installation, close the wizard.  Navigate to Server Manager Dashboard, IIS Web Server is installed and can be seen in the left pane.  "},{"title":"Starting Performance Counters​","type":1,"pageTitle":"IIS Web Server and Access Logging Setup","url":"docs/Integrations/iis/iis_setup#starting-performance-counters","content":"Click on IIS. Your server will be listed under &quot;SERVERS&quot;.  Right click on your server, and select &quot;Start Performance Counters&quot;.  "},{"title":"IIS Access Logging Setup​","type":1,"pageTitle":"IIS Web Server and Access Logging Setup","url":"docs/Integrations/iis/iis_setup#iis-access-logging-setup","content":"Right click on your server, and select &quot;Internet Information Services(IIS)&quot;.  Navigate to your web site in left pane, and select it.  Double click on &quot;Logging&quot; Option.  Click on &quot;Select Fields…&quot;.  Select all fields.  After selecting all fields, click on &quot;Ok&quot;.  Click on &quot;Apply&quot; in the right pane under &quot;Actions&quot;.  IIS Web server setup is complete with access logging. "},{"title":"Getting Started","type":0,"sectionRef":"#","url":"docs/Dashboards/getting_started","content":"","keywords":""},{"title":"A quick introduction​","type":1,"pageTitle":"Getting Started","url":"docs/Dashboards/getting_started#a-quick-introduction","content":" "},{"title":"Create your first dashboard in SnappyFlow​","type":1,"pageTitle":"Getting Started","url":"docs/Dashboards/getting_started#create-your-first-dashboard-in-snappyflow","content":" "},{"title":"Create a Summary Box​","type":1,"pageTitle":"Getting Started","url":"docs/Dashboards/getting_started#create-a-summary-box","content":" "},{"title":"Basic Dashboard Concepts​","type":1,"pageTitle":"Getting Started","url":"docs/Dashboards/getting_started#basic-dashboard-concepts","content":" "},{"title":"Types of Aggregation​","type":1,"pageTitle":"Getting Started","url":"docs/Dashboards/getting_started#types-of-aggregation","content":"Use the following extensions to achieve the required aggregation on a metric Aggregation\tExtension\tCommentsAverage\t.avg or no extension\tNot allowed for text fields Percentile\t.95P, .99P etc\tNot allowed for text fields Max\t.max\tNot allowed for text fields Min\t.min\tNot allowed for text fields Count\t.count Terminal\t.term\tRetrieves the last value of the metric from DBDocument type has to be specified in the query Current\t.curr\tRetrieves the last value of the metric in the last 8 mins. If no value is available, the result is displayed as NADocument type has to be specified in the query "},{"title":"Global vs Custom timeline​","type":1,"pageTitle":"Getting Started","url":"docs/Dashboards/getting_started#global-vs-custom-timeline","content":"A component by default uses the timeline selected in the global component However, a user can evaluate a component for a specific custom using the following steps Edit the component and go to the advanced section Select “Custom Time” Go to the JSON Editor and edit the time fields as shown below. In this example, we evaluate the component for last 1 hour. User can specify 30m for 30 minutes, 1d for 1day etc. "},{"title":"Constructing Queries​","type":1,"pageTitle":"Getting Started","url":"docs/Dashboards/getting_started#constructing-queries","content":"SnappyFlow provides SQL like query language to create components through a JSON editor provided. Features in JSON editor are selectively exposed based on options selected in advanced section Every document that is collected in SnappyFlow will have these tags: _plugin, _documentType, _tag_projectName, _tag_appName Whenever you create a new component, it will always have _tag_appName == $value by default, which is automatically substituted with current application name. This ensures that data is being queried for the current application only Examples Example Use Case\tQueryCollect a metric for a specific instance\tselect CPUUtil where _tag_Name == instance1 Collect metrics for all instances in an application\tselect CPUUtil where _tag_Name == * Collect metric based on a dropdown condition\tselect CPUUtil where _tag_Name == DropDownXX.$value Note: DropDownXX is the name of the dropdown. DropDown.$value is the value selected in the dropdown Collect a terminal value of a metric\tSelect clusterStatus.term where _plugin == elasticsearch and _documentType == clusterStats Note: Whenever you query a terminal value, a _documentType has to be specified Create a logic expression\tSelect path.count where code &gt;= 400 and code &lt; 600 Equal: == Not Equal: !== Greater or Less: &gt;, &gt;=, &lt;, &lt;= Variable belongs to a list: X ==(A,B,C..) String operations\tquery_string == message:&quot;Index to be deleted&quot; Query_string: Directive or operator for search by keyword Message: Field name in which the pattern has to be searched Search Text: Specify exact text in quotes To see all supported options, refer to Log Overview Search Building queries for nested data Note: When using a nested field, enclose metric within square brackets. Enable Add Nested Fields option in the advanced section. This will add the section nestedFields in JSON editor. Specify the nested fields used in the query under nestedFields section. Combined query: SnappyFlow allows upto 2 queries for SummaryBox and Tables and can be enabled from the advanced section. This is useful when querying terminal values from two different documentTypes Render: This feature is useful when using combined query for tables and summary charts It has 2 uses: change the order of rendering metrics when combined query is usedcollect a number of metrics, transform the metrics and render only a subset of the collected metrics. Enable the feature from advanced section "},{"title":"Histogram Intervals​","type":1,"pageTitle":"Getting Started","url":"docs/Dashboards/getting_started#histogram-intervals","content":"Histogram interval is relevant for a Line chart and Bar chart histogram. It is a sub interval in which a metric is evaluated or aggregated. Every histogram interval will correspond to a point in the chart. By default, histogram interval is calculated based on time range represented in the graph. If user deselects ‘Adaptive Interval’ option from the advanced section, the histogram interval is fixed at 60s. Time Range represented in the chart\tLine Chart Interval\tBar Chart Interval&lt;= 1 minute\t1s\t1s &lt;= 5 minutes\t2s\t4s &lt;= 10 minutes\t4s\t10s &lt;= 20 minutes\t10s\t10s &lt;= 30 minutes\t30s\t30s &lt;= 1 hr\t30s\t60s &lt;= 3 hrs\t30s\t5m &lt;= 6 hrs\t60s\t5m &lt;= 8 hrs\t5m\t5m &lt;= 12 hrs\t5m\t10m &lt;= 1 day\t5m\t30m &lt;= 7 days\t30m\t3h &lt;= 30 days\t2h\t12h &lt;= 60 days\t4h\t1d &lt;= 90 days\t8h\t1d &lt;= 120 days\t8h\t1d &lt;= 240 days\t16h\t1d &gt; 240 days\t1d\t1d "},{"title":"Metric Properties​","type":1,"pageTitle":"Getting Started","url":"docs/Dashboards/getting_started#metric-properties","content":"User can enable the metric properties from advanced section for: Formatting the data rendered in the componentApplying text/box color properties Property\tDescription\tExample Use Case\tCommentsmetricsFilter\tUsed to display specified portions of the string in the component or to shorten the displayed string\tConsider a table component displaying pod details with _tag_podName as the table key. If the pod names have the form apmmanager-apm-sfapm-apm, we can shorten the displayed name using metricsFilter\tApplicable for Table, Tabbed table, Line chart, Bar chart components For Example: This will display apmmanager-apm-sfapm-apm-5cd8946d64-2smb9 as apm-5cd8946d64-2smb9 decimal\tBy default, data is shown upto 1 decimal placeUser can override this setting by using this option Applicable for Table, Tabbed table, Summary box, Line chart and Bar chart histogram components If we have a table component with a header CPU (%), data under this column will be rounded to 3 decimal places. Note: To change the number of decimals for all metrics in the component, use default: &lt;num_of_decimals&gt; format\tFormat time field from epoch milliseconds/epoch seconds format to DateTime format Applicable for Table, Tabbed table, Bar charts, Key based line chart componentsAvailable options are DateTime, DateTime(ms) and DateTime(us) In the above example, the time displayed under Last Seen column of table is formatted to DateTime format color\tDisplay table cell with colors based on conditions\tRefer the section “Display table cell with colors based on conditions” in the Table Component section below\tApplicable for Table, Tabbed table and Excel Table components rate\tUsed to display the urate/unit rate of the metric in Table and Summary Box components\tUsage: &lt;var_name&gt;: urate\tApplicable for Summary Box and Table components urate is calculated as value of the metric/(time range used in query in seconds) nullValues\tNull Values are displayed as NA by defaultUse this option to override the default setting\tThe null values will be displayed as -- instead of NA\tApplicable for Table component boxColor\tDisplay box color for Summary box based on condition\tThe condition should be specified in the same way as the table cell color. Refer Table Component section below.\tApplicable for Summary Box textColor\tDisplay text color for Summary box based on condition\tThe condition should be specified in the same way as the table cell color.Refer Table Component section below.\tApplicable for Summary Box "},{"title":"Table Component​","type":1,"pageTitle":"Getting Started","url":"docs/Dashboards/getting_started#table-component","content":""},{"title":"Aggregation Table​","type":1,"pageTitle":"Getting Started","url":"docs/Dashboards/getting_started#aggregation-table","content":"All tables by default have aggregated option selected In this option, metrics are computed for a bucket that corresponds to a unique value of the key Example: If we need documents such as (Group: A, Name: x, Age: 10, Weight: 20) , (Group B, Name: y, Age: 12, Weight: 22), (Group A, Name: xx, Age: 25, Weight: 100), (Group B, Name: yy, Age: 15, Weight: 70), a table for Select Age.avg, Weight.avg where Group == * provides an output as shown below. Group\tAge\tWeightA\t17.5\t60 B\t13.5\t46 ​ *Group is the table key* in the above example "},{"title":"Non Aggregation Table:​","type":1,"pageTitle":"Getting Started","url":"docs/Dashboards/getting_started#non-aggregation-table","content":"User can choose this option by disabling “Aggregation” in advanced option or by leaving the “Table Key” field as empty This option is used to represent values from a sequence of JSON in a tabular format, without performing any aggregations Example: For the same example above, a query “Select Name, Group, Age, Weight” would provide the result as shown below that merely represents the data in a tabular form. The ordering by default will be in descending order of time Group\tName\tAge\tWeightA\tx\t10\t20 B\ty\t12\t22 C\txx\t25\t100 D\tyy\t15\t70 "},{"title":"Change Sort Field​","type":1,"pageTitle":"Getting Started","url":"docs/Dashboards/getting_started#change-sort-field","content":"Data for Aggregation tables by default is sorted in ascending order of table key In order to change the sorting field, enable “Change Sort Field” in advanced section Go to JSON editor and enter the raw metric (not a transformed field) that should be used for sorting Please note that a metric that has a .term/.count aggregation cannot be specified as a sort field By choosing “Change Sort Order” in advanced section, user can change the option to descending or ascending for the sort key. Please see in the example below "},{"title":"Filter​","type":1,"pageTitle":"Getting Started","url":"docs/Dashboards/getting_started#filter","content":"In many cases, a query may return a large number of results and we would want to filter the results from the database based on a certain values of a column Enable filter by selecting “Add Filter” option in advanced options Click on the filter icon Define filters for specific columns. We can specify one or more filters at a time. In the example below we are asking to filter all API path values in databases that contain the key “snappyflow” and number of 4xx errors is GT 5 Please note the following limitations and rules to follow If combined query is used, filters can be applied only to table keyFilter cannot be applied to a variable that is a transformation of more than one metric "},{"title":"Display table cell with colors based on conditions​","type":1,"pageTitle":"Getting Started","url":"docs/Dashboards/getting_started#display-table-cell-with-colors-based-on-conditions","content":"Example- we are computing the total number of 4XX and 5XX errors in the query below and we would like color the cell containing #Errors in red if the number of errors are GT 0 Go to the advanced section and enable metric properties. This will bring up a section called metric properties under which add the condition as shown below for color This will cause the color to rendered for the cell if #Errors is GT 0 "},{"title":"Hyperlink​","type":1,"pageTitle":"Getting Started","url":"docs/Dashboards/getting_started#hyperlink","content":"Hyperlink allows you to navigate from a table to another pane. The value of the hyperlink are propagated to a dropdown of the pane Go to Edit Component’s advanced section and enable Hyperlink checkbox, which can be found under General category A hyperlink section is enabled in the JSON editor. You will need to define the redirection target which comprises of Group (Group is a collection of panes. If the target pane does not belong a group, leave this field empty), Pane (dashboard pane name), Component (dropdown in the pane where the value of the hyperlink has to be propagated) Hyperlinks are enabled for the table. Click on the value “200” will redirect to the pane “Transaction Analysis” which belongs to group “Nginx Access” and render this pane with DropDown11 value = 200 "},{"title":"IIS Windows","type":0,"sectionRef":"#","url":"docs/Integrations/iis/iis_windows","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"IIS Windows","url":"docs/Integrations/iis/iis_windows#overview","content":"IIS monitoring uses WMI query to fetch metrics aggregated across all of the sites, or on a per-site basis. IIS plugin collects metrics for active connections, bytes sent and received, request count by HTTP method, access-log and more. "},{"title":"Pre-requisites​","type":1,"pageTitle":"IIS Windows","url":"docs/Integrations/iis/iis_windows#pre-requisites","content":"Web IIS Server 8.0 or above Performance counters should be enabled in IIS manager "},{"title":"Configuration​","type":1,"pageTitle":"IIS Windows","url":"docs/Integrations/iis/iis_windows#configuration","content":"Add the plugin configuration in config.yaml file under &quot;C:\\Program Files (x86)\\Sfagent\\&quot; directory as follows to enable this plugin sfAgent section provides steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under &quot;C:/Program Files (x86)/Sfagent/&quot; directory key: &lt;profile key&gt; generate_name: true tags: Name: &lt;unique instance name or will be generated from IP&gt; appName: &lt;add application name&gt; projectName: &lt;add project name&gt; metrics: plugins: - name: iis enabled: true interval: 300 logging: plugins: - name: iis-access enabled: true config: log_path: /log/filepath ua_parser: false url_normalizer: false Copy IIS Access Log Options:​ User-Agent Analysis: To get the host machine details like browser, Operating system and device by analysis the user-agent. To enable, set the option &quot;ua_parser&quot; to true in the above configuration. If enabled, by default it runs on port 8586.URL Normalizer (not supported in container deployment): Normalize incoming URL paths. To enable, set the option &quot;url_normalizer&quot; to true in the above configuration. If enabled, by default it runs on port 8587.  "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"IIS Windows","url":"docs/Integrations/iis/iis_windows#viewing-data-and-dashboards","content":"Dashboard for this data can be instantiated by Importing dashboard template IIS to the application dashboard. "},{"title":"Monitoring HCP Consul and Envoy Metrics on SnappyFlow","type":0,"sectionRef":"#","url":"docs/Integrations/hcp_consul","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Monitoring HCP Consul and Envoy Metrics on SnappyFlow","url":"docs/Integrations/hcp_consul#overview","content":"HCP (Hashicorp Cloud Platform) Consul is a service mesh and service discovery solution provided by Hashicorp. It enables platform operators to quickly deploy a fully managed, secure-by-default service mesh, helping developers discover and securely connect any application on any runtime, including Kubernetes, Nomad, and Amazon ECS. SnappyFlow supports monitoring of HCP Consul and Envoy metrics through StatsD plugin. The StatsD plugin aggregates all the metrics exposed by HCP Consul and pushes them to SnappyFlow for data visualization and alerting. SnappyFlow provides a built-in dashboard template and a default StatsD rule file, customized to HCP Consul and Envoy metrics to get started easily. "},{"title":"Pre-Requisites​","type":1,"pageTitle":"Monitoring HCP Consul and Envoy Metrics on SnappyFlow","url":"docs/Integrations/hcp_consul#pre-requisites","content":"Install sfAgent on target machineSince StatsD plugin is used, a custom rules file is required for Monitoring HCP Consul. Default rule files are available at /opt/sfagent/statsd_rules/ folder. "},{"title":"Configuring StatsD plugin​","type":1,"pageTitle":"Monitoring HCP Consul and Envoy Metrics on SnappyFlow","url":"docs/Integrations/hcp_consul#configuring-statsd-plugin","content":"The config.yaml file under /opt/sfagent/ contains all configuration information for sfAgent. Edit this config file and add profile key, tags (Hostnames, Application Names, Project Names), and the StatsD plugin configuration details under metrics as provided in the code block below. Default rules files are provided under /opt/sfagent/statsd_rules/. In the config.yaml file, you may find other plugins pre populated by sfAgent. To monitor envoy metrics use /opt/sfagent/statsd_rules/envoy_rules.txt To monitor consul metrics use /opt/sfagent/statsd_rules/consul_rules.txt To monitor consul and envoy metric use opt/sfagent/statsd_rules/consul_envoy_rules.txt key: &lt;input_profile_key_from_snappyflow_account&gt; tags: Name: &lt;provide_host_name&gt; appName: &lt;provide_application_name&gt; projectName: &lt;provide_project_name&gt; metrics: plugins: - name: statsd enabled: true config: port: 8125 flushinterval: 30 ruleFile: &lt;path_to_rule_file&gt; Copy key: Enter the profile key from SnappyFLow account tags: Provide necessary tags port: The UDP port on which statsd client sends metrics. sfAgent runs a statsd server listening on this port for the UDP datagrams. Default value is 8125 flushInterval: SnappyFlow’s statsd plugin collects all the metrics received in the last N seconds and sends the data to SnappyFlow as a single document ruleFile: Provide the path to the statsd rules file. Please contact support@snappyflow.io to create a rule file for a custom statsd client caution The StatsD configuration and rules files need to be validated for any changes to rules files to take effect. Post validation, sfAgent needs to be restarted. To validate StatsD configuration, run sudo /opt/sfagent/sfagent -check-statsd Copy To restart sfAgent, run service sfagent restart Copy A typical config file with pre-popluated linux plugins  "},{"title":"Verifying plugin operation​","type":1,"pageTitle":"Monitoring HCP Consul and Envoy Metrics on SnappyFlow","url":"docs/Integrations/hcp_consul#verifying-plugin-operation","content":"Once the StatsD plugin is configured, rule files validated and sfAgent restarted, you should now see the newly created Consul Application and Project under the names as defined in the config.yaml file. Tagged and discovered endpoints are automatically added to SnappyFlow  Clicking on the project takes you to the inventory page which gives a list of endpoints and services running under that project name. Verifying that StatsD plugin is running  Another way to check if SnappyFlow is receiving metrics is to check the Browse Data tab. Filter by StatsD plugin to see all raw data coming into SnappyFlow. Checking for raw data  "},{"title":"Setting up dashboards​","type":1,"pageTitle":"Monitoring HCP Consul and Envoy Metrics on SnappyFlow","url":"docs/Integrations/hcp_consul#setting-up-dashboards","content":"SnappyFlow provides a built in dashboard template for Consul and Envoy metrics. To setup dashboards, go to the project and under metrics, click on the 3 dots menu and select Import from Template option. Search for consul and clone and save the template to your dashboard. Importing built-in dashboards  HCP Envoy dashboard  For help with plugins, please reach out to support@snappyflow.io. "},{"title":"Go Profiler","type":0,"sectionRef":"#","url":"docs/Integrations/go/profiler","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Go Profiler","url":"docs/Integrations/go/profiler#overview","content":"SnappyFlow profiler collects Profiles (supported: cpu, heap, block, mutex, goroutine, threadcreate)Runtime metrics  and sends them to SnappyFlow for further visualization and analysis. cpu and heap profiles are enabled by default and other profiles can be enabled as required. godoc: https://pkg.go.dev/github.com/snappyflow/sf-go-profiler/profiler sample code: https://github.com/snappyflow/sf-go-profiler/tree/main/example/main.go "},{"title":"Getting started​","type":1,"pageTitle":"Go Profiler","url":"docs/Integrations/go/profiler#getting-started","content":"Pre-requisites Install and configure snappyflow agent on vm or as a sidecar in the container, as it is required to send data to snappyflow-apm Run the below command to download or update sf-go-profiler package in your current project. go get -u -v github.com/snappyflow/sf-go-profiler/profiler Copy Example import &quot;github.com/snappyflow/sf-go-profiler/profiler&quot; main(){ profile := profiler.NewProfilerConfig(&quot;server&quot;) profile.Start() defer profile.Stop() // rest of the application code } Copy profiling can conditionally enabled when required using golang flags import ( &quot;github.com/snappyflow/sf-go-profiler/profiler&quot; &quot;flag&quot; ) main(){ enableprofile := flag.Bool(&quot;profile&quot;,false,&quot;enable profiler&quot;) if *enableprofile { profile := profiler.NewProfilerConfig(&quot;server&quot;) // below line disables collection of go runtime metrics // profile.DisableRuntimeMetrics() // below line disables profiling // profile.DisableProfiles() profile.Start() defer profile.Stop() } // rest of the application code } Copy runtime metrics can be disable by calling DisableRuntimeMetrics() similarly profiling can be disabled by calling DisableProfiles() on profile config object.  profile := profiler.NewProfilerConfig(&quot;server&quot;) // below line disables collection of go runtime metrics profile.DisableRuntimeMetrics() // below line disables profiling profile.DisableProfiles() profile.Start() defer profile.Stop() Copy enable other supported profiles as required  // enable block profile and set given block profile rate profile.EnableBlockProfile(100) // enable mutex profile and set given mutex profile fraction profile.EnableMutexProfile(1000) // enable goroutine profile profile.EnableGoRoutineProfile() // enable threadcreate profile profile.EnableThreadCreateProfile() Copy since only heap and cpu profiles are enabled by default, all supported profiles can be enabled by call to function EnableAllProfiles(), this sets block profile rate to DefaultBlockProfileRate and mutex profile fraction to DefaultMutexProfileFraction  // enable all supported profiles profile.EnableAllProfiles() Copy "},{"title":"Sample runtime metrics collected​","type":1,"pageTitle":"Go Profiler","url":"docs/Integrations/go/profiler#sample-runtime-metrics-collected","content":"reference: https://pkg.go.dev/runtime#MemStats { &quot;alloc_mb&quot;: 8.4275, &quot;frees&quot;: 28575, &quot;gc_cpu_fraction&quot;: 0.0001, &quot;go_version&quot;: &quot;go1.16.4&quot;, &quot;interval&quot;: 60, &quot;last_gc&quot;: 1631099627396, &quot;live_objects&quot;: 27146, &quot;mallocs&quot;: 27361, &quot;max_pause_gc_ms&quot;: 0.1033, &quot;min_pause_gc_ms&quot;: 0.0366, &quot;num_cpu&quot;: 2, &quot;num_gc&quot;: 2, &quot;num_goroutines&quot;: 23, &quot;pid&quot;: 23201, &quot;sys_mb&quot;: 71.5791, &quot;time&quot;: 1631099686505, &quot;total_alloc_mb&quot;: 8.8994, &quot;total_pause_gc_ms&quot;: 0.14, } Copy "},{"title":"Clickhouse on Kubernetes","type":0,"sectionRef":"#","url":"docs/Integrations/clickhouse_kubernetes","content":"","keywords":""},{"title":"Clickhouse monitoring with Prometheus​","type":1,"pageTitle":"Clickhouse on Kubernetes","url":"docs/Integrations/clickhouse_kubernetes#clickhouse-monitoring-with-prometheus","content":"Refer to Prometheus Exporter overview to understand how SnappyFlow monitors using Prometheus exporters. "},{"title":"Pre-requisites​","type":1,"pageTitle":"Clickhouse on Kubernetes","url":"docs/Integrations/clickhouse_kubernetes#pre-requisites","content":"Prometheus exporter is deployed as a side-car in the application container and the exporter port is accessible to sfPod "},{"title":"Metrics list​","type":1,"pageTitle":"Clickhouse on Kubernetes","url":"docs/Integrations/clickhouse_kubernetes#metrics-list","content":"Cluster Details​ Name\tDescriptionDNSError\tTotal count of errors in DNS resolution DelayedInserts\tNumber of times the INSERT of a block to a MergeTree table was throttled due to high number of active data parts for partition. ContextLocks\tNumber of times the lock of Context was acquired or tried to acquire. This is global lock. MergedUncompressedBytes\tUncompressed bytes (for columns as they stored in memory) that was read for background merges. This is the number before merge. MergesTimeMilliseconds\tTotal time spent for background merges. DiskReadElapsedMicroseconds\tTotal time spent waiting for read syscall. This include reads from page cache. DiskWriteElapsedMicroseconds\tTotal time spent waiting for write syscall. This include writes to page cache. MergeTreeDataWriterCompressedBytes\tBytes written to filesystem for data INSERTed to MergeTree tables. MergeTreeDataWriterRows\tNumber of rows INSERTed to MergeTree tables. NumberOfTables\tNumber of tables InsertedBytes\tNumber of bytes (uncompressed; for columns as they stored in memory) INSERTed to all tables. InsertedRows\tNumber of rows INSERTed to all tables. Merge\tNumber of launched background merges. Query\tNumber of queries to be interpreted and potentially executed. Does not include queries that failed to parse or were rejected due to AST size limits, quota limits or limits on the number of simultaneously running queries. May include internal queries initiated by ClickHouse itself. Does not count subqueries. FailedQuery\tNumber of failed queries. SelectQuery\tSame as Query, but only for SELECT queries. FailedSelectQuery\tSame as FailedQuery, but only for SELECT queries. fileopen\tNumber of files opened. NumberOfDatabases\tNumber of databases ReadonlyReplica\tNumber of Replicated tables that are currently in readonly state due to re-initialization after ZooKeeper session loss or due to startup without ZooKeeper configured. OSCPUWaitMicroseconds\tTotal time a thread was ready for execution but waiting to be scheduled by OS, from the OS point of view. OSIOWaitMicroseconds\tTotal time a thread spent waiting for a result of IO operation, from the OS point of view. This is real IO that doesn't include page cache. UserTimeMicroseconds\tTotal time spent in processing (queries and other tasks) threads executing CPU instructions in user space. This include time CPU pipeline was stalled due to cache misses, branch mispredictions, hyper-threading, etc. OSWriteBytes\tNumber of bytes written to disks or block devices. Doesn't include bytes that are in page cache dirty pages. May not include data that was written by OS asynchronously. QueryDuration (Prom Metric: chi_clickhouse_event_RealTimeMicroseconds)\tTotal (wall clock) time spent in processing (queries and other tasks) threads (not that this is a sum). MergedRows\tRows read for background merges. This is the number of rows before merge. LongestRunningQuery\tLongest running query time HardPageFaults\tAn exception that the memory management unit (MMU) raises when a process accesses a memory page without proper preparations Host Details​ Name\tDescriptionDNSError\tTotal count of errors in DNS resolution Httpconnection\tThe number of connections to HTTP server CompressedReadBufferBlocks\tNumber of compressed blocks (the blocks of data that are compressed independent of each other) read from compressed sources (files, network) CompressedReadBufferBytes\tNumber of uncompressed bytes (the number of bytes after decompression) read from compressed sources (files, network). DiskReadElapsedMicroseconds\tTotal time spent waiting for read syscall. This include reads from page cache. DiskWriteElapsedMicroseconds\tTotal time spent waiting for write syscall. This include writes to page cache. MergeTreeDataWriterCompressedBytes\tBytes written to filesystem for data INSERTed to MergeTree tables. MergeTreeDataWriterRows\tNumber of rows INSERTed to MergeTree tables. DistributedConnectionFailAtAll\tTotal count when distributed connection fails after all retries finished InsertQuery\tSame as Query, but only for INSERT queries. NumberOfTables\tNumber of tables Query Threads\tNumber of query processing threads ZooKeeperUserExceptions\tZooKeeper User Exceptions BackgroundDistributedSchedulePoolTask\tNumber of active tasks in BackgroundDistributedSchedulePool. This pool is used for distributed sends that is done in background. BackgroundMovePoolTask\tNumber of active tasks in BackgroundProcessingPool for moves BackgroundSchedulePoolTask\tNumber of active tasks in BackgroundSchedulePool. This pool is used for periodic ReplicatedMergeTree tasks, like cleaning old data parts, altering data parts, replica re-initialization, etc ContextLocks\tNumber of times the lock of Context was acquired or tried to acquire. This is global lock. ContextLockWait\tNumber of threads waiting for lock in Context. This is global lock. GlobalThreadActive\tNumber of threads in global thread pool running a task. GlobalThreadTotal\tNumber of threads in global thread pool. LocalThreadActive\tNumber of threads in local thread pools running a task. LocalThreadTotal\tNumber of threads in local thread pool. PartMutation\tNumber of mutations (ALTER DELETE/UPDATE) FailedQuery\tNumber of failed queries. SelectQuery\tSame as Query, but only for SELECT queries. FailedSelectQuery\tSame as FailedQuery, but only for SELECT queries. Fileopen\tNumber of files opened. MergedRows\tRows read for background merges. This is the number of rows before merge. Merge\tNumber of launched background merges. Query\tNumber of queries to be interpreted and potentially executed. Does not include queries that failed to parse or were rejected due to AST size limits, quota limits or limits on the number of simultaneously running queries. May include internal queries initiated by ClickHouse itself. Does not count subqueries. InsertedBytes\tNumber of bytes (uncompressed; for columns as they stored in memory) INSERTed to all tables. InsertedRows\tNumber of rows INSERTed to all tables. MergedUncompressedBytes\tUncompressed bytes (for columns as they stored in memory) that was read for background merges. This is the number before merge. MergesTimeMilliseconds\tTotal time spent for background merges. ReplicasMaxInsertsInQueue ReplicasMaxMergesInQueue ReplicasSumInsertsInQueue ReplicasSumMergesInQueue jemalloc_background_thread_num_runs MaxPartCountForPartition MemoryTrackingForMerges\tTotal amount of memory (bytes) allocated for background merges. Included in MemoryTrackingInBackgroundProcessingPool. Note that this value may include a drift when the memory was allocated in a context of background processing pool and freed in other context or vice-versa. This happens naturally due to caches for tables indexes and doesn't indicate memory leaks. ZooKeeperWaitMicroseconds ArenaAllocBytes\t TableStats​ Name\tDescriptionTable\tName of the table Database\tName of the database NumPartitions\tNumber of partitions of the table NumTableParts\tNumber of parts of the table TableSize\tTable size in bytes NumRow\tNumber of rows in the table "},{"title":"Configuration ​","type":1,"pageTitle":"Clickhouse on Kubernetes","url":"docs/Integrations/clickhouse_kubernetes#configuration","content":"Clickhouse Deployment.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: clickhouse-operator spec: selector: matchLabels: app: clickhouse-operator template: metadata: annotations: prometheus.io/port: &quot;8888&quot; prometheus.io/scrape: &quot;true&quot; labels: app: clickhouse-operator spec: containers: - env: - name: OPERATOR_POD_NODE_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: spec.nodeName image: altinity/clickhouse-operator:0.10.0 imagePullPolicy: Always name: clickhouse-operator volumeMounts: - mountPath: /etc/clickhouse-operator name: etc-clickhouse-operator-folder - mountPath: /etc/clickhouse-operator/conf.d name: etc-clickhouse-operator-confd-folder - mountPath: /etc/clickhouse-operator/config.d name: etc-clickhouse-operator-configd-folder - mountPath: /etc/clickhouse-operator/templates.d name: etc-clickhouse-operator-templatesd-folder - mountPath: /etc/clickhouse-operator/users.d name: etc-clickhouse-operator-usersd-folder - image: altinity/metrics-exporter:0.10.0 imagePullPolicy: Always name: metrics-exporter ports: - containerPort: 8888 name: metrics protocol: TCP volumeMounts: - mountPath: /etc/clickhouse-operator name: etc-clickhouse-operator-folder - mountPath: /etc/clickhouse-operator/conf.d name: etc-clickhouse-operator-confd-folder - mountPath: /etc/clickhouse-operator/config.d name: etc-clickhouse-operator-configd-folder - mountPath: /etc/clickhouse-operator/templates.d name: etc-clickhouse-operator-templatesd-folder - mountPath: /etc/clickhouse-operator/users.d name: etc-clickhouse-operator-usersd-folder volumes: - configMap: defaultMode: 420 name: etc-clickhouse-operator-files name: etc-clickhouse-operator-folder - configMap: defaultMode: 420 name: etc-clickhouse-operator-confd-files name: etc-clickhouse-operator-confd-folder - configMap: defaultMode: 420 name: etc-clickhouse-operator-configd-files name: etc-clickhouse-operator-configd-folder - configMap: defaultMode: 420 name: etc-clickhouse-operator-templatesd-files name: etc-clickhouse-operator-templatesd-folder - configMap: defaultMode: 420 name: etc-clickhouse-operator-usersd-files name: etc-clickhouse-operator-usersd-folder Copy "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"Clickhouse on Kubernetes","url":"docs/Integrations/clickhouse_kubernetes#viewing-data-and-dashboards","content":"Data collected by plugins can be viewed in SnappyFlow’s browse data section  Plugin = kube-prom-clickhouse documentType= clusterDetails, hostDetails, tableStats Dashboard template: Clickhouse_Kube_Prom "},{"title":"Monitoring JAVA applications running on Instances","type":0,"sectionRef":"#","url":"docs/Integrations/java/java_instances","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Monitoring JAVA applications running on Instances","url":"docs/Integrations/java/java_instances#overview","content":"JVM on instances is monitored using sfAgent configured with jvm plugin. The plugin monitors JVM metrics, jvm arguments used to start Java process and deadlock metrics. JVM plugin internally uses the following utilities to collect metrics: Jstats for JVM metrics Jolokia will be started by plugin to collect deadlock metrics if monitor Deadlocks parameter is set in configuration file  "},{"title":"Pre-requisites​","type":1,"pageTitle":"Monitoring JAVA applications running on Instances","url":"docs/Integrations/java/java_instances#pre-requisites","content":"Jcmd has to be installed in the instance "},{"title":"Configuration​","type":1,"pageTitle":"Monitoring JAVA applications running on Instances","url":"docs/Integrations/java/java_instances#configuration","content":"sfAgent section provides steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: &lt;profile_key&gt; generate_name: true tags: Name: &lt;instance name&gt; appName: &lt;application name&gt; projectName: &lt;project name&gt; metrics: plugins: - name: jvm enabled: true interval: 60 config: process: * heapinterval: 3600 monitorDeadlocks: false deadlockMonitoringInterval: 300 Copy "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"Monitoring JAVA applications running on Instances","url":"docs/Integrations/java/java_instances#viewing-data-and-dashboards","content":"Data generated by this plugin can be viewed in browse data page inside the respective application under plugin=jvm and documentType=jvm Dashboard for this data can be instantiated by importing dashboard template JVM to the application dashboard. "},{"title":"Overview","type":0,"sectionRef":"#","url":"docs/Integrations/java/overview","content":"Overview JAVA monitoring on SnappyFlow is available for the following platforms Instances Kubernetes","keywords":""},{"title":"Heap Analysis For K8s Applications","type":0,"sectionRef":"#","url":"docs/Integrations/java/java_profiling_kubernetes","content":"","keywords":""},{"title":"Pre-requisites​","type":1,"pageTitle":"Heap Analysis For K8s Applications","url":"docs/Integrations/java/java_profiling_kubernetes#pre-requisites","content":"The SF-KUBE-AGENT docker image used for Heap Analysis in K8s Applications is arunsebastinraj/sf-apm-agent from the version v4.0 The example illustrates the use of docker image used in values.yaml file for Heap Analysis in K8s Applications. sfagent: enabled: true image: arunsebastinraj/sf-apm-agent imageTag: v4.0 imagePullPolicy: Always Mount the path /opt/sfagent as shared memory location in the sfagent container and the java application running container of the java application pod The example illustrates enabling shared path for java application pod i.e (application-deployment.yaml) file in volumeMounts section of respective containers ​ containers: ​ - name: sfagent ​ volumeMounts: ​ - name: mat ​ mountPath: /opt/sfagent ​ - name: (Java Application Container) ​ volumeMounts: ​ - name: mat ​ mountPath: /opt/sfagent Add the volume mat in volumes section of the application deployment file. volumes: ​ - name: mat ​ emptyDir: {} "},{"title":"Heap Analysis​","type":1,"pageTitle":"Heap Analysis For K8s Applications","url":"docs/Integrations/java/java_profiling_kubernetes#heap-analysis","content":"Memory profiling provides the heap dump analyses.Profiling helps us to identify Memory leaks. "},{"title":"Trigger Heap Dump​","type":1,"pageTitle":"Heap Analysis For K8s Applications","url":"docs/Integrations/java/java_profiling_kubernetes#trigger-heap-dump","content":"Select the project &gt;&gt; application where the instance is monitored Go to Dashboards &gt;&gt; Click on Profiling pane &gt;&gt; Click on Configure tab &gt;&gt; Generate Heap Dump button Fill the form with following details : Heap Dump NameAdd tags : Provide the tags keys and tags values used to identify the target instance on the which heap dump needs to be triggered.To drill down on the process on which heap dump has to be triggered provide the process name in the process name filter. If nothing is provided heap dump will be triggered on all java process identified by JCMD command.In Configure Schedule section : #Heap Dumps : indicate the number of heap dumps to be triggeredInterval between Heap Dumps : indicates the interval between individual heap dumpsMax wait time : indicates the time duration after which if no response is returned from target instances the command is marked invalid The command status can be viewed on expanding the command. In the details views, click on the Report icon to view the heap dump report. "},{"title":"Viewing Reports​","type":1,"pageTitle":"Heap Analysis For K8s Applications","url":"docs/Integrations/java/java_profiling_kubernetes#viewing-reports","content":"Click on Reports tab in Profiling PaneSelect the Language as JavaSelect Profiling as Heap analysesSelect Instance Name from drop downSelect process Name from drop downSelect the heapdump from the timeline chartDetails of the heapdump can be found in the down panel "},{"title":"Java Heap Analysis Troubleshoot Documentation​","type":1,"pageTitle":"Heap Analysis For K8s Applications","url":"docs/Integrations/java/java_profiling_kubernetes#java-heap-analysis-troubleshoot-documentation","content":"Error Message / Status\tWhat does it mean? When and why this occurs?\tWhat action user can take next?The listed process is not running\tWhile generating dump if the mentioned java process is not running in the VM this error will occur.\tRestart the process after investigation. Unable to get dump exit status 13\tOccurs while processing the heap dump, if the size of the heap dump is beyond supported limit.\tRefer the supported size limits. Unknown\tIf SFAgent is failed to update the heap dump processing status with in the maximum wait time, then SnappyFlow backend treat this as stale entry and mark it as Unknown.\tUser may increase the “Maximum wait time” while configuring heapdump. stat /opt/sfagent/dump_files/ Dump_file_141295_1638767771354\tIf SFAgent fails to locate the heap dump file, after triggering the heap dump this may occur. Possible reason could be process is down.\tRestart the process after investigation. Here are the details of plugin, indexes and document types involved in heap analysis feature.User can view these documents through browse data feature in SF UI. Index\tPlugin Type\tDocument Type(s)Heartbeat\tHeapDump\tCommand_Events Profiling\tHeapDump\tDominator_Class_stats, Shortest_Paths_To_the_Accumulation_Point_stats, class_usage_stats, heap_dump_overview_stats, leak_suspects_stats, All_Accumulated_Objects_by_Class_stats "},{"title":"Screenshots​","type":1,"pageTitle":"Heap Analysis For K8s Applications","url":"docs/Integrations/java/java_profiling_kubernetes#screenshots","content":"List of tags which will be used to filter instances.   Dashboard &gt;&gt; Profiling &gt;&gt; Configure &gt;&gt; Generate Heap Dump         Dashboard   You can filter the process name.   Profiling &gt;&gt; Reports &gt;&gt; Select the filters from the options.   Click any heap dump in the graph to see the overview of it.       Top Memory leak suspects &gt;&gt; Details       "},{"title":"Note​","type":1,"pageTitle":"Heap Analysis For K8s Applications","url":"docs/Integrations/java/java_profiling_kubernetes#note","content":"User can tag the instances with user defined tags.Application : Grouping the instances performing a similar role/purpose (say web service) with user defined tags. config.yaml tags: Name: Custom_log_parser_Test appName: Custom_log_parser_Test projectName: Demo-QA-Apps serviceName: MyService # user defined tags Copy   "},{"title":"CPU Profiling","type":0,"sectionRef":"#","url":"docs/Integrations/java/java_profiler_cpu","content":"","keywords":""},{"title":"Pre-requisites​","type":1,"pageTitle":"CPU Profiling","url":"docs/Integrations/java/java_profiler_cpu#pre-requisites","content":"Install Java - JDK 11Install sfAgent on Linux note Add profile key and SnappyFlow tags in the config.yaml.For existing customers, update SFAgent config.yaml with latest - profile key, if SFAgent is already deployed.Reference for installing sfAgent : Click hereName and AppName should always be unique (pair).jcmd equivalent : Application Dashboard &gt;&gt; Process Name "},{"title":"CPU Profiling​","type":1,"pageTitle":"CPU Profiling","url":"docs/Integrations/java/java_profiler_cpu#cpu-profiling-1","content":"CPU profiling provides the thread wise CPU usage.Profiling helps us to identify various issues : Synchronization Issues : Deadlock Situation : when several threads running hold a synchronized block on a shared objectThread Contention : when a thread is blocked waiting for others to finish Execution Issues : Abnormally high CPU usage : when a thread is in running state for a very long period of timeProcessing Performance is low : when many threads are blocked It also provides an ability to correlate the thread stack at that time.Java CPU profiling can be collected by using Java profiling plugin. config.yaml metrics: plugins: - name: javaprofiling enabled: true interval: 10 target: profile config: process: process_name # otherwise mention '*' monitorCpuProfiling: true cpuProfilingInterval: 10 stackTraceLimit: 25 Copy CPU profiling interval : It is by default set to 10 seconds.It is recommended to keep it 10s to get better overview to the thread.Increasing this we might miss out frequent thread state changes. Stack trace limit : It is set to 25 by default.If need this can be increased or decreased. CPU Profiling tool : It itself will consume some amount of CPU.The feature can be turned off after fixing your issue using the flag monitorCpuProfiling. Process : It provides the process name for which CPU profiling needs to be enabled. Regex can also be used. CPU Profiling Dashboards : To view the CPU profiling dashboards : Dashboards &gt;&gt; Profiling pane &gt;&gt; Select Java in the language dropdown nextCPU Profiling from the dropdown &gt;&gt; Select the instance, process name from the next dropdowns.It displays the thread activity in the form of visuals.We can easily find blocked threads and reasons why they are blocked. "},{"title":"Screenshots​","type":1,"pageTitle":"CPU Profiling","url":"docs/Integrations/java/java_profiler_cpu#screenshots","content":"Click on the filter icon to add filters.   To add filters, write the thread name by prepending ~ in the Filter column : e.g. ThreadName ~ Thread-8 and then click on OK.   Click the thread from the list.     You can modify the start and end time of the thread in the Stats Breakup.   You can expand the list item to get more information.   Hover over the graphs to get quick summary about Thread States and CPU Util by Thread.       "},{"title":"Memory","type":0,"sectionRef":"#","url":"docs/Integrations/java/java_profiler_memory","content":"","keywords":""},{"title":"Heap Analysis​","type":1,"pageTitle":"Memory","url":"docs/Integrations/java/java_profiler_memory#heap-analysis","content":""},{"title":"Pre-requisites​","type":1,"pageTitle":"Memory","url":"docs/Integrations/java/java_profiler_memory#pre-requisites","content":"Install Java - JDK 11Install sfAgent on Linux note Add profile key and SnappyFlow tags in the config.yaml.For existing customers, update SFAgent config.yaml with latest - profile key, if SFAgent is already deployed.Name and AppName should always be unique (pair).jcmd equivalent : Application Dashboard &gt;&gt; Process Name "},{"title":"Heap Analysis​","type":1,"pageTitle":"Memory","url":"docs/Integrations/java/java_profiler_memory#heap-analysis-1","content":"Memory profiling provides the heap dump analyses.Profiling helps us to identify Memory leaks. "},{"title":"Trigger Heap Dump​","type":1,"pageTitle":"Memory","url":"docs/Integrations/java/java_profiler_memory#trigger-heap-dump","content":"Select the project &gt;&gt; application where the instance is monitored Go to Dashboards &gt;&gt; Click on Profiling pane &gt;&gt; Click on Configure tab &gt;&gt; Generate Heap Dump button Fill the form with following details : Heap Dump NameAdd tags : Provide the tags keys and tags values used to identify the target instance on the which heap dump needs to be triggered.To drill down on the process on which heap dump has to be triggered provide the process name in the process name filter. If nothing is provided heap dump will be triggered on all java process identified by JCMD command.In Configure Schedule section : #Heap Dumps : indicate the number of heap dumps to be triggeredInterval between Heap Dumps : indicates the interval between individual heap dumpsMax wait time : indicates the time duration after which if no response is returned from target instances the command is marked invalid The command status can be viewed on expanding the command. In the details views, click on the Report icon to view the heap dump report. "},{"title":"Remote Processing​","type":1,"pageTitle":"Memory","url":"docs/Integrations/java/java_profiler_memory#remote-processing","content":"Eventhough if the user opts for remote processing, if the heapdump hprof file size is smaller than or equal to the default maximum supported file size, parsing of reports will be done by agent and normal workflow prevails.The following are the environment variables which user can use to override the default values. MAXSUPPORTEDSIZEGB: To override the default maximum supported heapdump filesize(15 % of Machine's RAM) (eg: MAXSUPPORTEDSIZEGB=2 for 2GB and MAXSUPPORTEDSIZEGB=2.5 for 2.5GB) in /opt/sfagent/env.conf CHUNKSIZEMB: To override the default chunksize value (500MB) (eg: CHUNKSIZEMB=1000 for 1GB) in /opt/sfagent/env.conf MEMORYANALYZERMAXHEAPSIZEGB: To override the default maxheapsize of memoryanalyser.ini(50 % of machine's RAM) (eg: MEMORYANALYZERMAXHEAPSIZEGB=2 for 2GB and MEMORYANALYZERMAXHEAPSIZEGB=2.5 for 2.5GB) in /opt/sfagent/env.conf RHPSCLIENTTIMEOUTSEC: To override the default RHPS http client timeout (300 Seconds) (eg: RHPSCLIENTTIMEOUTSEC=500 for 500 seconds) in /opt/sfagent/env.conf "},{"title":"Viewing Reports​","type":1,"pageTitle":"Memory","url":"docs/Integrations/java/java_profiler_memory#viewing-reports","content":"Click on Reports tab in Profiling PaneSelect the Language as JavaSelect Profiling as Heap analysesSelect Instance Name from drop downSelect process Name from drop downSelect the heapdump from the timeline chartDetails of the heapdump can be found in the down panel "},{"title":"Heap Analysis Support Size Limits​","type":1,"pageTitle":"Memory","url":"docs/Integrations/java/java_profiler_memory#heap-analysis-support-size-limits","content":"Heap analysis is memory and CPU intensive activity.Based on the application memory usage heap dump binary size, processing overhead on the system will also increase on the SFAgent w.r.t CPU and memory. We successfully tested up to 3.3GB of heap dump size on t2.xlarge (4 vCPU, 16 GiB) instance. Here are the resources consumption details : Java Process Max Heap Size\tEclipse MAT Max Heap Size\tMax Dump Size Created\tTime Taken To Generate Dump (min.)\tTime Taken To Parse Reports (min.)\tSystem’s Max CPU Usage1 GB\t8 GB\t1 GB\t0.1\t2.34\t45% 2 GB\t8 GB\t1.7 GB\t0.3\t3.6\t65% 3 GB\t8 GB\t3.1 GB\t0.6\t9.8\t55% 4 GB\t8 GB\t3.3 GB\t0.6\t10.5\t80% "},{"title":"Java Heap Analysis Troubleshoot Documentation​","type":1,"pageTitle":"Memory","url":"docs/Integrations/java/java_profiler_memory#java-heap-analysis-troubleshoot-documentation","content":"Error Message / Status\tWhat does it mean? When and why this occurs?\tWhat action user can take next?The listed process is not running\tWhile generating dump if the mentioned java process is not running in the VM this error will occur.\tRestart the process after investigation. Unable to get dump exit status 13\tOccurs while processing the heap dump, if the size of the heap dump is beyond supported limit.\tRefer the supported size limits. Unknown\tIf SFAgent is failed to update the heap dump processing status with in the maximum wait time, then SnappyFlow backend treat this as stale entry and mark it as Unknown.\tUser may increase the “Maximum wait time” while configuring heapdump. stat /opt/sfagent/dump_files/ Dump_file_141295_1638767771354\tIf SFAgent fails to locate the heap dump file, after triggering the heap dump this may occur. Possible reason could be process is down.\tRestart the process after investigation. Here are the details of plugin, indexes and document types involved in heap analysis feature.User can view these documents through browse data feature in SF UI. Index\tPlugin Type\tDocument Type(s)Heartbeat\tHeapDump\tCommand_Events Profiling\tHeapDump\tDominator_Class_stats, Shortest_Paths_To_the_Accumulation_Point_stats, class_usage_stats, heap_dump_overview_stats, leak_suspects_stats, All_Accumulated_Objects_by_Class_stats "},{"title":"Screenshots​","type":1,"pageTitle":"Memory","url":"docs/Integrations/java/java_profiler_memory#screenshots","content":"List of tags which will be used to filter instances.   Dashboard &gt;&gt; Profiling &gt;&gt; Configure &gt;&gt; Generate Heap Dump         Dashboard   You can filter the process name.   Profiling &gt;&gt; Reports &gt;&gt; Select the filters from the options.   Click any heap dump in the graph to see the overview of it.       Top Memory leak suspects &gt;&gt; Details       "},{"title":"Note​","type":1,"pageTitle":"Memory","url":"docs/Integrations/java/java_profiler_memory#note","content":"User can tag the instances with user defined tags.Application : Grouping the instances performing a similar role/purpose (say web service) with user defined tags. config.yaml tags: Name: Custom_log_parser_Test appName: Custom_log_parser_Test projectName: Demo-QA-Apps serviceName: MyService # user defined tags Copy   "},{"title":"Jboss plugin","type":0,"sectionRef":"#","url":"docs/Integrations/jboss","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Jboss plugin","url":"docs/Integrations/jboss#overview","content":"Jboss Metric plugin monitors Jboss server by collecting multiple types of metrics like server stats, jvm stats using Jolokia "},{"title":"Prerequisites​","type":1,"pageTitle":"Jboss plugin","url":"docs/Integrations/jboss#prerequisites","content":"Jboss Metric Plugin is based on Jolokia agent which requires JMX monitoring to be enabled locally. Following property needs to be included during the start of jboss process  -Dcom.sun.management.jmxremote Copy JCMD command must be installed in the machine "},{"title":"Configuration Settings​","type":1,"pageTitle":"Jboss plugin","url":"docs/Integrations/jboss#configuration-settings","content":"Refer to sfAgent section for steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent directory   metrics: metrics: plugins: - name: jboss enabled: true interval: 30 config: username: jbossuser password: password protocol: http port: 8080 context: jolokia Copy "},{"title":"Documents​","type":1,"pageTitle":"Jboss plugin","url":"docs/Integrations/jboss#documents","content":"It consists of two document types: Jboss stats: contain metrics like jboss sever version, uptime, server name,transactions and session related details,request information like processing time, request count, data received and sent,processing time, request count, data received and sentJVM stats: contain all JVM related metrics used by tomcat server like garbage collection details, memory pools, loaded/unloaded classes etc Use Jboss dashboard for data visualization. "},{"title":"Server Info pane​","type":1,"pageTitle":"Jboss plugin","url":"docs/Integrations/jboss#server-info-pane","content":" "},{"title":"Jvm Health pane​","type":1,"pageTitle":"Jboss plugin","url":"docs/Integrations/jboss#jvm-health-pane","content":" "},{"title":"Api Load Analysis pane​","type":1,"pageTitle":"Jboss plugin","url":"docs/Integrations/jboss#api-load-analysis-pane","content":" "},{"title":"Logger​","type":1,"pageTitle":"Jboss plugin","url":"docs/Integrations/jboss#logger","content":""},{"title":"Description​","type":1,"pageTitle":"Jboss plugin","url":"docs/Integrations/jboss#description","content":"Jboss logger to capture Wildfly server access logs and error logs. "},{"title":"Prerequisites​","type":1,"pageTitle":"Jboss plugin","url":"docs/Integrations/jboss#prerequisites-1","content":"Jboss server access log format needs to be modified to capture all metrics from the access logs, which includes following steps Edit the file $JBOSS_HOME/standalone/configuration/standalone.xml Set log format in the following section &lt;host name=&quot;default-host&quot; alias=&quot;localhost&quot;&gt; by specifying the pattern value to pre-defined “combined” log format or &lt;host name=&quot;default-host&quot; alias=&quot;localhost&quot;&gt; &lt;location name=&quot;/&quot; handler=&quot;welcome-content&quot;/&gt; &lt;access-log pattern=&quot;%h %t &amp;quot;%r&amp;quot; %s &amp;quot;%{i,User-Agent}&amp;quot; %D &quot; use-server-log=&quot;false&quot;/&gt; &lt;http-invoker security-realm=&quot;ApplicationRealm&quot;/&gt; &lt;/host&gt; Copy Set the attribute record-request-start-time=&quot;true&quot; in the section &lt;subsystem xmlns=&quot;urn:jboss:domain:undertow:*&quot;&gt; for all the listeners as %D and %T access log elements will work only after record-request-start-time is turned on Set the attribute statistics-enabled=&quot;true&quot; in all the occurences of standalone.xml as the statistics are disabled by default After changing log pattern to combined or the above mentioned pattern, sample log would look like: 183.83.155.203 [07/Aug/2020:14:24:17 +0000] &quot;GET /petclinic/org.richfaces.resources/javax.faces.resource/org.richfaces/skinning.ecss?db=eAG7dPvZfwAIqAOT HTTP/1.1&quot; 500 &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36&quot; 3 Copy "},{"title":"Configuration Settings​","type":1,"pageTitle":"Jboss plugin","url":"docs/Integrations/jboss#configuration-settings-1","content":"Mention the access log file and server log file path in plugin configuration. Wildcard character supported logging: plugins: - name: jboss-access enabled: true ## OPTIONAL config: log_path: &quot;/opt/wildfly/standalone/log/access*.log&quot; geo_info: true ua_parser: true url_normalizer: false #Not recommended for k8s deployment - name: jboss-error enabled: true ## OPTIONAL config: log_level: - error - warn - warning log_path: &quot;/opt/wildfly/standalone/log/server*.log&quot; Copy Jboss access log plugin also supports: Geo-IP: Useful to find geographical location of the client using the IP address. To enable, set the option &quot;geo_info&quot; to true in the above configurationUser-Agent Analysis: To get the host machine details like browser, Operating system and device by analysis the user-agent. To enable, set the option &quot;ua_parser&quot; to true in the above configuration. If enabled, by default it runs on port 8586URL Normalizer (not supported in container deployment): Normalize incoming URL paths. To enable, set the option &quot;url_normalizer&quot; to true in the above configuration. If enabled, by default it runs on port 8587 Config Field Description, interval: Normalization algorithm runtime interval enabled: Rely on normalization feature for rule generation *rules_length_limit Normalization specific configuration is available in /opt/sfagent/normalization/config.yaml which resonate the following, interval: 300 dynamic_rule_generation: enabled: true rules_length_limit: 1000 log_volume: 100000 rules: [] Copy t*: Limit over size of generated rules. set the value to -1 for specifying no limit log_volume: Limit over number of logs processed. set the value to -1 for specifying no limit rules: Rules Generated Recommended Approach is to have sfagent running with dynamic_rule_generation enabled over a period of time. Observe whether rules generated reflect all the web app requests intended to be normalized and if its a true reflection, set enabled flag to false , indicating no further rules will be generated Default ports used by user-agent analysis and URL Normalizer can be changed respectively with the inclusion of following in config.yaml agent: uaparserport: desired_port_number url_normalizer: desired_port_number Copy note Latitude and Longitude are often near the center of population. These values are not precise and should not be used to identify a particular address or householdUser-agent parsing requires high computation power. Recommended to enable only if needed and system have enough CPU resource available. Please refer the following table to know the CPU consumption range. These are approximate values, and might vary depending on multiple factors Average Request Rate (request/sec)\tApprox. CPU Utilization (%)50\t4 - 7 100\t12 - 15 200\t30 - 60 300\t40 - 60 500\t50 - 80 1000\t80 - 140 2000\t90 - 160 "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"Jboss plugin","url":"docs/Integrations/jboss#viewing-data-and-dashboards","content":"Data collected by plugins can be viewed in SnappyFlow’s browse data section under metrics section plugin: jboss, jboss-access, jboss-error documentType: jboss, jvm, jboss-access, jboss-error Dashboard template: jboss For help with plugins, please reach out to support@snappyflow.io. "},{"title":"Monitoring JAVA applications running in Kubernetes","type":0,"sectionRef":"#","url":"docs/Integrations/java/java_kubernetes","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Monitoring JAVA applications running in Kubernetes","url":"docs/Integrations/java/java_kubernetes#overview","content":"Java applications running in Kubernetes can be monitored in SnappyFlow using two approaches: sfKubeAgent as sidecar container. Prometheus exporter  "},{"title":"Java monitoring with sfKubeAgent​","type":1,"pageTitle":"Monitoring JAVA applications running in Kubernetes","url":"docs/Integrations/java/java_kubernetes#java-monitoring-with-sfkubeagent","content":"In this option, the Java application should be run with Jolokia agent and sfKubeAgent running as a sidecar container and fetches metrics via Jolokia port. Refer to sfKubeAgent Overview "},{"title":"Prerequisites​","type":1,"pageTitle":"Monitoring JAVA applications running in Kubernetes","url":"docs/Integrations/java/java_kubernetes#prerequisites","content":"Copy Jolokia JAR into docker image Run the java application with Jolokia JAR in docker image: -javaagent:/&lt;path_jolokia_jar&gt;/jolokia-jvm-&lt;version&gt;-agent.jar Copy "},{"title":"Configurations​","type":1,"pageTitle":"Monitoring JAVA applications running in Kubernetes","url":"docs/Integrations/java/java_kubernetes#configurations","content":"Run sfKubeAgent with JVMJolokia plugin, which is specified using the config map shown below: apiVersion: v1 kind: ConfigMap metadata: name: jvm-configmap data: config.yaml: |- key: &lt;profile_key&gt; metrics: plugins: name: jvmjolokia enabled: true interval: 300 config: ip: 127.0.0.1 protocol: http port: &lt;userDefinedJolokiaPort&gt; context: jolokia monitorDeadlocks: false deadLockMonitoringInterval: 300 Copy The example illustrates instantiating sfKubeAgent with jvm-configmap. sfAKubeAgent talks to the Java application via userDefinedJolokiaPort (this example used 8778) kind: Pod apiVersion: v1 metadata: name: my-first-pod-1 labels: snappyflow/appname: &lt;app_name&gt; snappyflow/projectname: &lt;project_name&gt; spec: containers: name: java-container image: &lt;docker_id&gt;/&lt;docker_image&gt;:&lt;tag&gt; ports: name: jolokiaport containerPort: &lt;userDefinedJolokiaPort&gt; Snappyflow's sfkubeagent container name: java-sfagent image: snappyflowml/sfagent:latest imagePullPolicy: Always command: /app/sfagent -enable-console-log env: name: APP_NAME value: &lt;app_name&gt; name: PROJECT_NAME value: &lt;project_name&gt; volumeMounts: name: configmap-jvm mountPath: /opt/sfagent/config.yaml subPath: config.yaml volumes: name: configmap-jvm configMap: name: jvm-configmap Copy "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"Monitoring JAVA applications running in Kubernetes","url":"docs/Integrations/java/java_kubernetes#viewing-data-and-dashboards","content":"Data generated by plugin can be viewed in browse data page inside the respective application under plugin=jvm_jolokia and documentType=jvm Dashboard for this data can be instantiated by Importing dashboard template JVM to the application dashboard  "},{"title":"Troubleshooting​","type":1,"pageTitle":"Monitoring JAVA applications running in Kubernetes","url":"docs/Integrations/java/java_kubernetes#troubleshooting","content":"Check if the Jolokia port is accessible From inside the application container, run a curl command to the userDefinedJolokiaPort. curl http://localhost:&lt;userDefinedJolokiaPort&gt; Copy Check the logs in sfKubeAgent container for any errors "},{"title":"JVM Monitoring with Prometheus exporter​","type":1,"pageTitle":"Monitoring JAVA applications running in Kubernetes","url":"docs/Integrations/java/java_kubernetes#jvm-monitoring-with-prometheus-exporter","content":"Refer to Prometheus Exporter Overview. Prometheus exporter is deployed as a sidecar container in the application pod and connects to the JMX target exposed by the application to scrape the metrics. sfPod polls Prometheus exporter to scrape the metrics. "},{"title":"Pre-requisites​","type":1,"pageTitle":"Monitoring JAVA applications running in Kubernetes","url":"docs/Integrations/java/java_kubernetes#pre-requisites","content":"sfPod can access Prometheus exporter at Service IP: userDefinedPrometheusPort "},{"title":"Configurations​","type":1,"pageTitle":"Monitoring JAVA applications running in Kubernetes","url":"docs/Integrations/java/java_kubernetes#configurations-1","content":"Run Java application with JMX options: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port= &lt;userDefinedJMXPort&gt; -Dcom.sun.management.jmxremote.authenticate=false - Dcom.sun.management.jmxremote.ssl=false Copy Start the Prometheus exporter with java -jar jmx_prometheus_httpserver.jar &lt;userDefinedPrometheusPort&gt; &lt;exporterConfigFile&gt; Copy Configurations are passed using config map: apiVersion: v1 kind: ConfigMap metadata: labels: snappyflow/appname: &lt;app_name&gt; snappyflow/projectname: &lt;project_name&gt; data: jmx-config.yaml: | --- jmxUrl: service:jmx:rmi:///jndi/rmi://127.0.0.1:&lt;userDefinedJMXPort&gt;/jmxrmi ssl: false rules: - pattern: '.*' Copy Prometheus exporter interfaces to JMX via userDefinedJMXPort. Example below uses 555S as the port. Prometheus exporter exposes userDefinedPrometheusPort for scraping. Example uses 5556 as the port Pod definition YAML that illustrates the configuration for Java application and exporter kind: Pod apiVersion: v1 metadata: name: my-first-pod labels: snappyflow/appname: &lt;app_name&gt; snappyflow/projectname: &lt;project_name&gt; spec: containers: name: app-container image: &lt;docker_id&gt;/&lt;docker_image&gt;:&lt;tag&gt; command: sh -c -x java -jar -Dcom.sun.management.jmxremote - Dcom.sun.management.jmxremote.port=&lt;userDefinedJMXPort&gt; - Dcom.sun.management.jmxremote.authenticate=false - Dcom.sun.management.jmxremote.ssl=false &lt;application_jar&gt; name: &quot;exporter-container&quot; image: &quot;bitnami/jmx-exporter:latest&quot; imagePullPolicy: command: sh -c -x java -jar jmx_prometheus_httpserver.jar &lt;userDefinedPrometheusPort&gt; /tmp/jmx-config.yaml ports: name: exporter-port containerPort: &lt;userDefinedPrometheusPort&gt; volumeMounts: name: configmap-jmx mountPath: /tmp volumes: name: configmap-jmx configMap: name: jmx-configmap Copy "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"Monitoring JAVA applications running in Kubernetes","url":"docs/Integrations/java/java_kubernetes#viewing-data-and-dashboards-1","content":"Data generated by plugin can be viewed in “browse data” page inside the respective application under plugin=‘kube-prom-jmx‘ and documentType=‘jmxStats’Dashboard for this data can be instantiated by Importing dashboard template “JVM” to the application dashboard.  "},{"title":"Troubleshooting​","type":1,"pageTitle":"Monitoring JAVA applications running in Kubernetes","url":"docs/Integrations/java/java_kubernetes#troubleshooting-1","content":"Check if the JMX port is accessible .From inside the application container, run a curl command to the userDefinedJMXPort. curl http://localhost:&lt;userDefinedJMXPort&gt; Copy Check if metrics are getting scraped. From inside the exporter container, run a curl command to the userDefinedPrometheusPort curl http://localhost:&lt;userDefinedPrometheusPort&gt;/metrics Copy "},{"title":"overview","type":0,"sectionRef":"#","url":"docs/Integrations/kafka/overview","content":"overview Kafka monitoring on SnappyFlow is available for the following platforms Instances​ Kubernetes​","keywords":""},{"title":"Monitoring Kafka Clusters running on Instances","type":0,"sectionRef":"#","url":"docs/Integrations/kafka","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Monitoring Kafka Clusters running on Instances","url":"docs/Integrations/kafka#overview","content":"Monitoring Kafka cluster requires running sfAgent on all the nodes in the clusters. sfAgent collects metrics and logs from each of the nodes and the aggregate view is presented in SnappyFlow. Following aspects of Kafka node is monitored by sfAgent Kafka Metrics jmxStats: JVM health which includes heap utilization, GC Time, Loaded/Unloaded Classes, Memory pools and thread analysiskafkaStats: Aggregate metrics for the cluster which include - data in/out rate, message in-rate, Fetch follower request rate, Processing times for producer, consumer and follower requests etc.partitionStats: Size of each partition by topictopicStats: Topic metrics which includes data in/out rate, message in rate, producer request rate, failed producer request rate, failed fetch request rate etc..consumerStats: Offset and Lag by topic by partition  Kafka Logs kafkaGeneralLogs "},{"title":"Prerequisites​","type":1,"pageTitle":"Monitoring Kafka Clusters running on Instances","url":"docs/Integrations/kafka#prerequisites","content":"Kafka Metric Plugin is based on Jolokia agent which requires JMX monitoring to be enable locally. Following property needs to be included during the start of kafka process -Dcom.sun.management.jmxremote Copy Jcmd has to be installed on the instance Kafka ships with log4j support . Log4j property file (log4j.properties) which is present in root folder of kafka. has to be set as follows Enable root logger and file appender where file appender can be of any type based on rolling strategy log4j.rootLogger=INFO, logfile log4j.appender.logFile=org.apache.log4j.DailyRollingFileAppender Copy Specify custom log file name along with its path, layout properties and data pattern log4j.appender.logFile.DatePattern='.'yyyy-MM-dd-HH log4j.appender.logFile.File=&lt;..logpath..&gt; log4j.appender.logFile.layout=org.apache.log4j.PatternLayout log4j.appender.logFile.layout.ConversionPattern=[%d] %p %m (%c)%n Copy After configuring log4j properties, emitted log would look like [2020-07-09 11:15:23,376] INFO [GroupCoordinator 1]: Group consgrp-tst with generation 1588 is now empty (__consumer_offsets-5) (kafka.coordinator.group.GroupCoordinator) Copy "},{"title":"Configuration​","type":1,"pageTitle":"Monitoring Kafka Clusters running on Instances","url":"docs/Integrations/kafka#configuration","content":"sfAgent section provides steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: &lt;Profile_key&gt; tags: Name: &lt;instance_name&gt; appName: &lt;app_name&gt; projectName: &lt;project_name&gt; metrics: plugins: - name: kafkajmx enabled: true interval: 60 config: process: kafka.Kafka - name: kafkatopic enabled: true interval: 60 config: documentsTypes: - kafkaStats - partitionStats - topicStats - consumerStats port: 9092 process: kafka.Kafka logging: plugins: - name: kafka-general enabled: true config: log_level: - error - info - notice - debug log_path: &lt;log_path&gt; Copy Kafka metrics monitoring on instances requires two separate plugins – KafkaJMX to collect JVM health metrics and kafkaTopic plugin to collect broker and topic metrics. kafkaTopic plugin requires the following parameters: process: Kafka process name (It should be part of java main class)port: Broker Port "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"Monitoring Kafka Clusters running on Instances","url":"docs/Integrations/kafka#viewing-data-and-dashboards","content":"Data collected by plugins can be viewed in SnappyFlow’s browse data section Metrics plugin: kafka documentType: jmxStats, kafkaStats, partitionStats, topicStats, consumerStats Dashboard template: Kafka Logs Plugin: kafka documentType: kafkaGeneralLogs "},{"title":"Test Matrix​","type":1,"pageTitle":"Monitoring Kafka Clusters running on Instances","url":"docs/Integrations/kafka#test-matrix","content":"OS\tJDK versionubuntu 18.04 JDK 11 openjdk version &quot;11.0.11&quot; 2021-04-20 OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04) OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing) ubuntu 18.04 JDK 8 openjdk version &quot;1.8.0_292&quot; OpenJDK Runtime Environment (build 1.8.0_292-8u292-b10-0ubuntu1~18.04-b10) OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode) Centos 7 JDK 11 openjdk version &quot;11.0.12&quot; 2021-07-20 LTS OpenJDK Runtime Environment 18.9 (build 11.0.12+7-LTS) OpenJDK 64-Bit Server VM 18.9 (build 11.0.12+7-LTS, mixed mode, sharing) Centos 7 JDK 8 openjdk version &quot;1.8.0_302&quot; OpenJDK Runtime Environment (build 1.8.0_302-b08) OpenJDK 64-Bit Server VM (build 25.302-b08, mixed mode) "},{"title":"See Also​","type":1,"pageTitle":"Monitoring Kafka Clusters running on Instances","url":"docs/Integrations/kafka#see-also","content":"Zookeeper Elasticsearch Kafka-REST Kafka-Connect "},{"title":"Monitoring Kafka Clusters running on Instances","type":0,"sectionRef":"#","url":"docs/Integrations/kafka/kafka_instances","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Monitoring Kafka Clusters running on Instances","url":"docs/Integrations/kafka/kafka_instances#overview","content":"Monitoring Kafka cluster requires running sfAgent on all the nodes in the clusters. sfAgent collects metrics and logs from each of the nodes and the aggregate view is presented in SnappyFlow. Following aspects of Kafka node is monitored by sfAgent Kafka Metrics jmxStats: JVM health which includes heap utilization, GC Time, Loaded/Unloaded Classes, Memory pools and thread analysiskafkaStats: Aggregate metrics for the cluster which include - data in/out rate, message in-rate, Fetch follower request rate, Processing times for producer, consumer and follower requests etc.partitionStats: Size of each partition by topictopicStats: Topic metrics which includes data in/out rate, message in rate, producer request rate, failed producer request rate, failed fetch request rate etc..consumerStats: Offset and Lag by topic by partition  Kafka Logs kafkaGeneralLogs "},{"title":"Prerequisites​","type":1,"pageTitle":"Monitoring Kafka Clusters running on Instances","url":"docs/Integrations/kafka/kafka_instances#prerequisites","content":"Kafka Metric Plugin is based on Jolokia agent which requires JMX monitoring to be enable locally. Following property needs to be included during the start of kafka process -Dcom.sun.management.jmxremote Copy Jcmd has to be installed on the instance Kafka ships with log4j support . Log4j property file (log4j.properties) which is present in root folder of kafka. has to be set as follows Enable root logger and file appender where file appender can be of any type based on rolling strategy log4j.rootLogger=INFO, logfile log4j.appender.logFile=org.apache.log4j.DailyRollingFileAppender Copy Specify custom log file name along with its path, layout properties and data pattern log4j.appender.logFile.DatePattern='.'yyyy-MM-dd-HH log4j.appender.logFile.File=&lt;..logpath..&gt; log4j.appender.logFile.layout=org.apache.log4j.PatternLayout log4j.appender.logFile.layout.ConversionPattern=[%d] %p %m (%c)%n Copy After configuring log4j properties, emitted log would look like [2020-07-09 11:15:23,376] INFO [GroupCoordinator 1]: Group consgrp-tst with generation 1588 is now empty (__consumer_offsets-5) (kafka.coordinator.group.GroupCoordinator) Copy "},{"title":"Configuration​","type":1,"pageTitle":"Monitoring Kafka Clusters running on Instances","url":"docs/Integrations/kafka/kafka_instances#configuration","content":"sfAgent section provides steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: &lt;Profile_key&gt; tags: Name: &lt;instance_name&gt; appName: &lt;app_name&gt; projectName: &lt;project_name&gt; metrics: plugins: - name: kafkajmx enabled: true interval: 60 config: process: kafka.Kafka - name: kafkatopic enabled: true interval: 60 config: documentsTypes: - kafkaStats - partitionStats - topicStats - consumerStats port: 9092 process: kafka.Kafka logging: plugins: - name: kafka-general enabled: true config: log_level: - error - info - notice - debug log_path: &lt;log_path&gt; Copy Kafka metrics monitoring on instances requires two separate plugins – KafkaJMX to collect JVM health metrics and kafkaTopic plugin to collect broker and topic metrics. kafkaTopic plugin requires the following parameters: process: Kafka process name (It should be part of java main class)port: Broker Port "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"Monitoring Kafka Clusters running on Instances","url":"docs/Integrations/kafka/kafka_instances#viewing-data-and-dashboards","content":"Data collected by plugins can be viewed in SnappyFlow’s browse data section Metrics plugin: kafka documentType: jmxStats, kafkaStats, partitionStats, topicStats, consumerStats Dashboard template: Kafka Logs Plugin: kafka documentType: kafkaGeneralLogs "},{"title":"Test Matrix​","type":1,"pageTitle":"Monitoring Kafka Clusters running on Instances","url":"docs/Integrations/kafka/kafka_instances#test-matrix","content":"OS\tJDK versionubuntu 18.04 JDK 11 openjdk version &quot;11.0.11&quot; 2021-04-20 OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04) OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing) ubuntu 18.04 JDK 8 openjdk version &quot;1.8.0_292&quot; OpenJDK Runtime Environment (build 1.8.0_292-8u292-b10-0ubuntu1~18.04-b10) OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode) Centos 7 JDK 11 openjdk version &quot;11.0.12&quot; 2021-07-20 LTS OpenJDK Runtime Environment 18.9 (build 11.0.12+7-LTS) OpenJDK 64-Bit Server VM 18.9 (build 11.0.12+7-LTS, mixed mode, sharing) Centos 7 JDK 8 openjdk version &quot;1.8.0_302&quot; OpenJDK Runtime Environment (build 1.8.0_302-b08) OpenJDK 64-Bit Server VM (build 25.302-b08, mixed mode) "},{"title":"See Also​","type":1,"pageTitle":"Monitoring Kafka Clusters running on Instances","url":"docs/Integrations/kafka/kafka_instances#see-also","content":"Zookeeper Elasticsearch Kafka-REST Kafka-Connect "},{"title":"Kubernetes Monitoring with sfPod","type":0,"sectionRef":"#","url":"docs/Integrations/kubernetes/kubernetes_monitoring_with_sfPod","content":"","keywords":""},{"title":"sfPod overview​","type":1,"pageTitle":"Kubernetes Monitoring with sfPod","url":"docs/Integrations/kubernetes/kubernetes_monitoring_with_sfPod#sfpod-overview","content":"sfPod is a collector that is installed on Kubernetes and runs as a DaemonSet on each worker node. It monitors the following elements of a Kubernetes environment: Kubernetes metrics Cluster metrics Host metrics Pod metrics Container metrics Events Kubernetes services health– Kubelet, Kube-Proxy, API Server, Controller Manager, Core DNS Kubernetes cluster logs Poll Prometheus Exporters running on application pods Pod Application Logs "},{"title":"sfPod installation​","type":1,"pageTitle":"Kubernetes Monitoring with sfPod","url":"docs/Integrations/kubernetes/kubernetes_monitoring_with_sfPod#sfpod-installation","content":"Below is short video explaining the sfPOD installation steps  "},{"title":"Step 1: Create a Cloud profile​","type":1,"pageTitle":"Kubernetes Monitoring with sfPod","url":"docs/Integrations/kubernetes/kubernetes_monitoring_with_sfPod#step-1-create-a-cloud-profile","content":"Create a cloud profile in SnappyFlow (or use an existing profile) and copy the profile key to use it while installing the sfPod in your cluster. "},{"title":"Step 2: Add Snappyflow helm chart​","type":1,"pageTitle":"Kubernetes Monitoring with sfPod","url":"docs/Integrations/kubernetes/kubernetes_monitoring_with_sfPod#step-2-add-snappyflow-helm-chart","content":"Login to any node that has network connectivity to Kubernetes master node and run the following commands helm repo add snappyflow https://snappyflow.github.io/helm-charts helm repo list helm repo update Copy "},{"title":"Step 3: Installing helm chart​","type":1,"pageTitle":"Kubernetes Monitoring with sfPod","url":"docs/Integrations/kubernetes/kubernetes_monitoring_with_sfPod#step-3-installing-helm-chart","content":"kubectl create namespace sfagent helm install &lt;name&gt; snappyflow/sfagent --set config.project_name=&lt;my-project-name&gt; --set config.app_name=&lt;my-cluster-name&gt; --set config.key=&lt;profile key&gt; --namespace sfagent Copy note &lt;my-cluster-name&gt; Replace with any name, Cluster is discovered by this name on the Snappyflow. &lt;profile key&gt; Replace with key copied from SnappyFlow. "},{"title":"Restricted sfPod Configuration​","type":1,"pageTitle":"Kubernetes Monitoring with sfPod","url":"docs/Integrations/kubernetes/kubernetes_monitoring_with_sfPod#restricted-sfpod-configuration","content":"By default, sfPod is installed in a full configuration mode where it monitors all the elements. For a restricted configuration i.e. monitor only cluster logs or cluster metrics, user can use the flags outlined below: --set config.cluster_monitoring=true/false If true monitoring of cluster metrics and cluster logs is enabled. If this field is made false, cluster monitoring is switched off and only Prometheus polling and Centralized Application Log Monitoring are enabled --set config.node_agent.drop_cluster_logs=true =&gt; If true, monitoring of Kubernetes cluster logs is disabled. --set config.&lt;doc_type&gt;= false sfPod organizes data collected by plugin/documentType. Example of some of the document types that are collected by sfPod include – pod, node, container, cluster_stats etc. User can disable collection of a documentType using this configuration. The detailed list of document types can be reviewed in Browse Data page of a Kubernetes cluster --set config.app_view By default sfPod sends pod and container metrics of tagged pods (I,e pods that have projectName and appName tags) to both Cluster Index and Project Index leading to duplication of metric data. This feature is enabled to enhance correlation of Application and Infra data. This feature can be switched-off if the flag= true. "},{"title":"Centralized Logging of Application Pod Logs","type":0,"sectionRef":"#","url":"docs/Integrations/kubernetes/centralized_logging_of_application_pod_logs","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Centralized Logging of Application Pod Logs","url":"docs/Integrations/kubernetes/centralized_logging_of_application_pod_logs#overview","content":"SnappyFlow can collect &amp; parse application logs from pods in 2 ways: Collect logs locally by running sfKubeAgent as a sidecar container inside application pod Collect logs centrally through sfPod, which is explained in this page  "},{"title":"Procedure for Centralized Logging​","type":1,"pageTitle":"Centralized Logging of Application Pod Logs","url":"docs/Integrations/kubernetes/centralized_logging_of_application_pod_logs#procedure-for-centralized-logging","content":"User runs a busybox sidecar container in the application pod with log files mounted to the container. Busybox tails &amp; streams the application logs to stdout Add SnappyFlow labels: snappyflow/projectName and snappyflow/appName: These are mandatory labels for SnappyFlow monitoring. sfPod collects logs only from pods that have these labels and collected logs are organized under projectName/appName hierarchy in SnappyFlow snappyflow/component: This label is used to signal to sfPod on which parser to apply to parse the logs. List of standard parsers packaged with sfPod. If no label is present, sfPod will apply SnappyFlow’s generic parser which collects the whole log line as a message. sfPod runs as daemon-set in all the Kubernetes data nodes. It picks up logs from stdout of tagged pods, parses the logs based on component tag and ships parsed logs to SnappyFlow under projectName/appName hierarchy. "},{"title":"How to tag application Pods with project and application name labels​","type":1,"pageTitle":"Centralized Logging of Application Pod Logs","url":"docs/Integrations/kubernetes/centralized_logging_of_application_pod_logs#how-to-tag-application-pods-with-project-and-application-name-labels","content":""},{"title":"Running Pods​","type":1,"pageTitle":"Centralized Logging of Application Pod Logs","url":"docs/Integrations/kubernetes/centralized_logging_of_application_pod_logs#running-pods","content":"Use the following kubectl commands to tag your application pods with the appropriate tags: kubectl label pods &lt;pod_name&gt; snappyflow/projectname=&lt;project_name&gt; --namespace &lt;appnamespace&gt; kubectl label pods &lt;pod_name&gt; snappyflow/appname=&lt;app_name&gt; --namespace &lt;appnamespace&gt; Copy "},{"title":"Automatically apply labels to new Pods​","type":1,"pageTitle":"Centralized Logging of Application Pod Logs","url":"docs/Integrations/kubernetes/centralized_logging_of_application_pod_logs#automatically-apply-labels-to-new-pods","content":"To automatically apply right labels for new pods which get created due to various reasons such as upgrade, restarts etc. apply labels to pod templates. If you are using helm chart, best practice is to define labels in values.yaml and use these labels parameter in pod template section of Deployment, StatefulSet, Daemonset or other Kubernetes controller. Below is one example values.yaml apiVersion: apps/v1 kind: Deployment metadata: name: sfapm-ui labels: app: sfapm role: ui spec: replicas: 1 selector: matchLabels: app: sfapm role: ui template: metadata: labels: app: sfapm role: ui snappyflow/appname: demo-application snappyflow/projectname: demo-project Copy "},{"title":"Example​","type":1,"pageTitle":"Centralized Logging of Application Pod Logs","url":"docs/Integrations/kubernetes/centralized_logging_of_application_pod_logs#example","content":""},{"title":"Centralized logging for nginx-access logs​","type":1,"pageTitle":"Centralized Logging of Application Pod Logs","url":"docs/Integrations/kubernetes/centralized_logging_of_application_pod_logs#centralized-logging-for-nginx-access-logs","content":"Configure Nginx to drop logs in the required format in /var/log/nginx folder using config map Add busy box container to tail logs from access logs and stream to stdout Signal to sfPod to use “nginx” parser using label “component” Nginx Pod YAML kind: Pod apiVersion: v1 metadata: name: my-first-pod labels: snappyflow/appname: &lt;app_name&gt; snappyflow/projectname: &lt;project_name&gt; snappyflow/component: nginx spec: containers: - name: nginx-container image: nginx:latest imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP volumeMounts: - name: varlog mountPath: /var/log/nginx - name: nginx-config mountPath: /etc/nginx/nginx.conf subPath: nginx.conf - name: busybox-container image: busybox command: [&quot;/bin/sh&quot;, &quot;-c&quot;] args: [&quot;tail -n+1 -f /var/log/nginx/access1.log&quot;] volumeMounts: - name: varlog mountPath: /var/log/nginx volumes: - name: nginx-config configMap: name: nginx-config - name: varlog emptyDir: {} Copy Config map for Nginx configuration apiVersion: v1 kind: ConfigMap metadata: name: nginx-configmap labels: snappyflow/appname: &lt;app_name&gt; snappyflow/projectname: &lt;project_name&gt; data: nginx.conf: | worker_processes 5; events { worker_connections 4096; } http { default_type application/octet-stream; log_format upstream_time '$remote_addr:$remote_port $remote_user [$time_local] ' '&quot;$request&quot; $status $body_bytes_sent ' '&quot;$http_referer&quot; &quot;$http_user_agent&quot; &quot;$http_referer&quot; ' 'rt=$request_time uct=$upstream_connect_time uht=$upstream_header_time urt=$upstream_response_time'; server { listen 80; error_log /var/log/nginx/error1.log; access_log /var/log/nginx/access1.log upstream_time; location /nginx_status { stub_status; } } } Copy "},{"title":"Overview","type":0,"sectionRef":"#","url":"docs/Integrations/kubernetes/overview","content":"Overview SnappyFlow provides various approaches to monitor Kubernetes applications. Choose one to continue sfPod​ sfPod is a collector that is installed on Kubernetes and runs as a DaemonSet on each worker node.It monitors the following elements of a Kubernetes environment: Host, Pod &amp; Container metricsResources such as deployments, Daemon Sets etc.Kubernetes core services metricsCluster logsMonitor Prometheus exporters running on any of the application pods sfKubeAgent​ sfKubeAgent is sfAgent packaged as a container and run as a sidecar within a Kubernetes application pod. It can be configured to collect both application metrics and logs similar to the way sfAgent does. Prometheus Integration​ Centralized Logging​","keywords":""},{"title":"Prometheus Exporter","type":0,"sectionRef":"#","url":"docs/Integrations/kubernetes/prometheus_exporter","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Prometheus Exporter","url":"docs/Integrations/kubernetes/prometheus_exporter#overview","content":"sfPod scans all pods in the namespaces that it has access to for specific labels snappyflow/projectname and snappyflow/appname If a pod is tagged with SnappyFlow labels, sfPod then looks for standard Prometheus annotations Label\tValueprometheus.io/scrape\tIf true, the pod is considered for Prometheus scraping, else it is excluded. prometheus.io/port\tThis label defines a list of ports that sfPod will scan. sfPod will also apply the appropriate parser. If this label is empty, sfPod scans all exposed container ports. Default value is empty. prometheus.io/path\tDefine the path as /metrics. Empty by default. If sfPod finds data on these ports, the data is scanned, parsed and sent to SnappyFlow  "},{"title":"Monitoring pods using Prometheus exporter​","type":1,"pageTitle":"Prometheus Exporter","url":"docs/Integrations/kubernetes/prometheus_exporter#monitoring-pods-using-prometheus-exporter","content":""},{"title":"Pre-requisites​","type":1,"pageTitle":"Prometheus Exporter","url":"docs/Integrations/kubernetes/prometheus_exporter#pre-requisites","content":"Ensure sfPod is running and has access privileges to namespace in which application pod is running Ensure sfPod has access to ports exposing Prometheus exporters  "},{"title":"Enable access to Prometheus exporter​","type":1,"pageTitle":"Prometheus Exporter","url":"docs/Integrations/kubernetes/prometheus_exporter#enable-access-to-prometheus-exporter","content":"Add Prometheus exporter container as a sidecar in the application pod. Pls see example below for Prometheus exporter pod. sfPod needs to access the Prometheus exporter on the exported port, which should be exposed in pod’s service PostgreSQL Statefulset YAML PostgreSQL Service YAML After setup of Prometheus exporter container, please verify connectivity using: curl service_ip: 9187 curl service_ip: 5432 Copy "},{"title":"Tag applications with Labels​","type":1,"pageTitle":"Prometheus Exporter","url":"docs/Integrations/kubernetes/prometheus_exporter#tag-applications-with-labels","content":"note Applying labels are key to monitoring in SnappyFLow. Endpoints are organized in a hierarchy as per the labels defined. Add labels snappyflow/projectName and snappyflow/appName if the application pods are already running, use the following kubectl commands to tag your application pods with the appropriate tags: kubectl label pods &lt;pod_name&gt; snappyflow/projectname=&lt;project_name&gt; --namespace&lt;appnamespace&gt; kubectl label pods &lt;pod_name&gt; snappyflow/appname=&lt;app_name&gt; --namespace&lt;appnamespace&gt; Copy To automatically apply the right labels for the new pods which get created due to various reasons such as upgrades, restarts etc, apply labels to pod templates. If you are using helm chart, a best practice is to define labels in values.yaml and use the label parameters in pod template section of Deployment, StatefulSet, DaemonSet or other Kubernetes controller. "},{"title":"List of Prometheus exporters supported by sfPod​","type":1,"pageTitle":"Prometheus Exporter","url":"docs/Integrations/kubernetes/prometheus_exporter#list-of-prometheus-exporters-supported-by-sfpod","content":"Plugins\tExporter Links\tservice_discovery_regex\tDocker imageapache\tExporter Link\t[&quot;apache_accesses_total&quot;,&quot;apache_+&quot;]\tDocker image elasticsearch\tExporter Link\t[&quot;elasticsearch_+&quot;]\tDocker image haproxy\tExporter Link\t[&quot;haproxy_+&quot;]\tDocker image jmx\tExporter Link\t[&quot;jmx_exporter_build_info&quot;,&quot;jmx_+&quot;,&quot;java_lang_+&quot;]\tDocker image kafka-connect-j9\tExporter Link\t[&quot;kafka_connect+&quot;,&quot;java_lang_+&quot;,&quot;java_lang_memorymanager_valid_j9+&quot;]\tDocker image kafka-connect\tExporter Link\t[&quot;kafka_connect+&quot;,&quot;java_lang_+&quot;,&quot;java_lang_garbagecollector_collectiontime_g1_young_generation&quot;]\tDocker image kafka-jmx\tExporter Link\t[&quot;kafka_server_+&quot;,&quot;kafka_network_+&quot;,&quot;java_lang_+&quot;]\tDocker image kafka-rest-j9\tExporter Link\t[&quot;kafka_rest+&quot;,&quot;java_lang_+&quot;,&quot;java_lang_memorymanager_valid_j9+&quot;]\tDocker image kafka-rest\tExporter Link\t[&quot;kafka_rest+&quot;,&quot;java_lang_+&quot;,&quot;java_lang_garbagecollector_collectiontime_g1_young_generation&quot;]\tDocker image kafka\tExporter Link\t[&quot;kafka_topic_+&quot;]\tDocker image linux\tExporter Link\t[&quot;node_cpu_+&quot;,&quot;node_disk_+&quot;,&quot;node_procs_+&quot;]\tDocker image mongod\tExporter Link\t[&quot;mongodb_+&quot;]\tDocker image mysql\tExporter Link\t[&quot;mysql_global_+&quot;,&quot;mysql_version_+&quot;]\tDocker image nginx\tExporter Link\t[&quot;nginx_+&quot;]\tDocker image nodejs\tNo exporter. Using code instrumentation\t[&quot;nodejs_+&quot;] postgres\tExporter Link\t&quot;pg_stat_+&quot;\tDocker image zookeeper-jmx\tExporter Link\t[&quot;zookeeper_+&quot;,&quot;java_lang_&quot;]\tDocker image "},{"title":"Monitoring Application Pods with sfKubeAgent","type":0,"sectionRef":"#","url":"docs/Integrations/kubernetes/sfkubeagent_installation","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Monitoring Application Pods with sfKubeAgent","url":"docs/Integrations/kubernetes/sfkubeagent_installation#overview","content":"sfKubeAgent is sfAgent packaged as a container and run as a sidecar within a Kubernetes application pod. It can be configured to collect both application metrics and logs similar to the way sfAgent does. "},{"title":"Integrating sfKubeAgent to application pods​","type":1,"pageTitle":"Monitoring Application Pods with sfKubeAgent","url":"docs/Integrations/kubernetes/sfkubeagent_installation#integrating-sfkubeagent-to-application-pods","content":"Instantiate sfKubeAgent docker image in the pod Mount sfKubeAgent config map to the container. Config.yaml file used here is similar to the one used for sfAgent. Configurations for specific applications or log types can be found in Integrations section Pass parameters projectName and appName through container’s yaml file. These are mandatory tags and SnappyFlow uses these tags to organize the end-points in a project/ application hierarchy Mount log paths that need to be monitored to sfKubeAgent container in the correct path "},{"title":"Example:​","type":1,"pageTitle":"Monitoring Application Pods with sfKubeAgent","url":"docs/Integrations/kubernetes/sfkubeagent_installation#example","content":"Below is an example of sfKubeAgent yaml that monitors JVM and Syslog in an application pod. "},{"title":"Pod description YAML​","type":1,"pageTitle":"Monitoring Application Pods with sfKubeAgent","url":"docs/Integrations/kubernetes/sfkubeagent_installation#pod-description-yaml","content":"kind: Pod apiVersion: v1 metadata: name: jvm-pod labels: snappyflow/appname: test snappyflow/projectname: test-new-1 spec: containers: - name: java-container image: ruchira27/jolokia:latest ports: - name: jolokiaport containerPort: 8778 # Snappyflow's sfkubeagent container - name: java-sfagent image: snappyflowml/sfagent:latest imagePullPolicy: Always command: - /app/sfagent - -enable-console-log env: - name: APP_NAME value: test - name: PROJECT_NAME value: test-new-1 volumeMounts: - name: configmap-jmx mountPath: /opt/sfagent/config.yaml subPath: config.yaml - name: varlog mountPath: /var/log volumes: - name: configmap-jmx configMap: name: jmx-configmap - name: varlog hostPath: path: /var/log Copy "},{"title":"Config Map​","type":1,"pageTitle":"Monitoring Application Pods with sfKubeAgent","url":"docs/Integrations/kubernetes/sfkubeagent_installation#config-map","content":"apiVersion: v1 kind: ConfigMap metadata: name: jmx-configmap data: config.yaml: |- key: Hc0cioeml0Sv7b7MbC+N56DKjygUlcvtP3wLtoUQitk3hw3/SevFv5loicDL9cCJDz3fImeLCuR1MrM/un4z+G2gELVeapNVCh96RhqSDvrV4MV9jMiuGi8RCa8MEj6KzAsvxnBPotbYKiM+11cm0xWOZ7K5G0C6J6T+SLX2/xk9us3BN2MhnBCH1N3xGhlDrNAy7j+KLSKsroiZcDw87iFjSaUzt0ADhCEwEJV3JBLZc2xpSM+n1hm3e4HHnVhaXcOi3Fcb9qD280Ya15t7eTsJywHyhKPcNKXpqF0OGVolLEUDc2vwklHGHIZXHF9hY/+/anS9+VSfhVpBNKVsDb+hDCLJbB8uBivJ9idRcnMvGkhir4kAUcsryCgvpay0ghqKZkjQ7zuhzKYW4/szHoXv+8g/Gn+nnxu3yFAa4aTOq6/AMNCA49S9EmU9Tn2yr+dUhiheWhKWFCTc8jd7vowehcPstNW1t8+SMfERkTqSKo1I/PSG0MGm3vrAa2yfU2GwnsyJnROSF/ylSY5JjTBlmfp7ZozKO8XPc7q+vaMwKEQzcDSqpSE26gOVMxrkYD2ksE/BQPbO2X1YTwlOqHSbr9Z0E5XOJXBSmgT7it7BgBCNro0/YcpALdoyEsJr4FBzM0K4ZwZNpnbDrbs0UIKLISaSGkYGAGBtuEXrusQ= metrics: plugins: - name: jvmjolokia enabled: true interval: 300 config: ip: 127.0.0.1 protocol: http port: 8778 context: jolokia monitorDeadlocks: false deadLockMonitoringInterval: 300 logging: plugins: - name: linux-syslog enabled: true config: log_level: - error - warning - info log_path: /var/log/auth.log,/var/log/messages,/var/log/secure Copy "},{"title":"Untitled","type":0,"sectionRef":"#","url":"docs/Integrations/kubernetes/Untitled","content":"Untitled","keywords":""},{"title":"Monitoring MS SQL databases","type":0,"sectionRef":"#","url":"docs/Integrations/mssql_windows","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Monitoring MS SQL databases","url":"docs/Integrations/mssql_windows#overview","content":"The MSSQL Metric plugin collects metrics for number of user connections, rate of SQL compilations, and more. "},{"title":"Prerequisites​","type":1,"pageTitle":"Monitoring MS SQL databases","url":"docs/Integrations/mssql_windows#prerequisites","content":"Create a read-only login to connect to your server. Follow the below given steps: CREATE LOGIN &lt;USERNAME&gt; WITH PASSWORD = ‘&lt;PASSWORD&gt;’; CREATE USER &lt;USERNAME&gt; FOR LOGIN &lt;USERNAME&gt;; GRANT SELECT on sys.dm_os_performance_counters to &lt;USERNAME&gt;; GRANT VIEW SERVER STATE to &lt;USERNAME&gt;; GRANT CONNECT ANY DATABASE to &lt;USERNAME&gt;; GRANT VIEW ANY DEFINITION to &lt;USERNAME&gt;; Copy "},{"title":"Configuration Settings​","type":1,"pageTitle":"Monitoring MS SQL databases","url":"docs/Integrations/mssql_windows#configuration-settings","content":"Add the plugin configuration in config.yaml file under C:\\Program Files (x86)\\Sfagent directory as follows to enable this plugin - name: mssql enabled: true interval: 300 config: username: xxxx password: xxxx port: 1433 documentsTypes: - serverDetails - databaseDetails - tableDetails - queryDetails Copy For help with plugins, please reach out to support@snappyflow.io "},{"title":"MySQL on Instances","type":0,"sectionRef":"#","url":"docs/Integrations/mysql/mysql_instances","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"MySQL on Instances","url":"docs/Integrations/mysql/mysql_instances#overview","content":"MySQL on instances is monitored using sfAgent configured with MySQL plugin . MySQL plugin has been tested on ubuntu (16.04 and 18.04) and centos 7 with MySQL versions 5.7 and 8. "},{"title":"Metrics plugin​","type":1,"pageTitle":"MySQL on Instances","url":"docs/Integrations/mysql/mysql_instances#metrics-plugin","content":"Collects metric data organized in following documentType under metrics index:  serverDetailsdatabaseDetailstableDetailsmasterReplicationDetailsslaveReplicationDetails "},{"title":"Logger plugin​","type":1,"pageTitle":"MySQL on Instances","url":"docs/Integrations/mysql/mysql_instances#logger-plugin","content":"collects general logs and slow query logs. General logs are sent to log index whereas slow queries are sent to metrics index under documentType:mysqlSlowQueryLogs  "},{"title":"Pre-requisites ​","type":1,"pageTitle":"MySQL on Instances","url":"docs/Integrations/mysql/mysql_instances#pre-requisites","content":""},{"title":"Enable MySQL configurations​","type":1,"pageTitle":"MySQL on Instances","url":"docs/Integrations/mysql/mysql_instances#enablemysqlconfigurations","content":"Logging needs to be configured in the mysql.conf.d/mysqld.cnf file. In the configuration file uncomment and configure the variables shown below:  show_compatibility_56 = On #neeeded for metrics log-error=/var/log/mysqld.log pid-file=/var/run/mysqld/mysqld.pid general_log_file=/var/log/mysql/mysql.log general_log=1 Copy This file can be located by executing the command as shown below:  mysqld --verbose --help | grep -A 1 &quot;Default options&quot; Copy E.g. output is /etc/my.cnf /etc/mysql/my.cnf ~/my.cnf. User needs to check each of the files for the configuration Alternatively, login to mysql with root user and execute below commands  SET GLOBAL general_log = 'ON'; SET GLOBAL general_log_file= '/path/filename'; Copy "},{"title":"Enable Slow Query Logs  ​","type":1,"pageTitle":"MySQL on Instances","url":"docs/Integrations/mysql/mysql_instances#enable-slow-query-logs","content":"In mysqld.cnf file, uncomment and configure the variables shown below:  slow_query_log= 1 slow_query_log_file=/var/log/mysql/mysql-slow.log Copy Or, login to mysql with root user and execute below commands  SET GLOBAL slow_query_log = 'ON'; SET GLOBAL long_query_time = 100; SET GLOBAL slow_query_log_file = '/path/filename'; Copy note By Default /var/log/mysql directory is not present in centos, so we must create and provide ownership of that directory as mysql chown -R mysql:mysql /var/log/mysql Copy "},{"title":"Set access permissions​","type":1,"pageTitle":"MySQL on Instances","url":"docs/Integrations/mysql/mysql_instances#set-access-permissions","content":"Username used for DB access should have appropriate permissions  grant select on information_schema.* to 'username' identified by 'password'; grant select on performance_schema.* to 'username' identified by 'password'; Copy note Root user has these permissions by default  "},{"title":"Enable Replication(optional)​","type":1,"pageTitle":"MySQL on Instances","url":"docs/Integrations/mysql/mysql_instances#enable-replicationoptional","content":"To collect the replication details replication has to be enabled. Execute the following queries on the slave using the login of the user provided in the config.yaml file: 1)&quot;show slave status&quot; 2)&quot;select * from replication_connection_status&quot; If the user is able to execute these queries then the replication details can be collected. Commands to create a replication user if you want to enable replication: the commands to be executed on the source or master, CREATE USER 'replica_user'@'slave_server_ip' IDENTIFIED WITH mysql_native_password BY 'password'; Copy GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'replica_user'@'slave_server_ip'; Copy "},{"title":"Configuration ​","type":1,"pageTitle":"MySQL on Instances","url":"docs/Integrations/mysql/mysql_instances#configuration","content":"Refer to sfAgent section for steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory  metrics: plugins: - name: mysql enabled: true interval: 60 config: documentsTypes: - databaseDetails - serverDetails - tableDetails - masterReplicationDetails #optional to be enabled when replication is setup - slaveReplicationDetails #optional to be enabled when replication is setup host: 127.0.0.1 password: USERad@123$ port: 3306 user: root logging: plugins: - name: mysql-error enabled: true config: log_level: - error - warning - note log_path: /var/log/mysql/error.log, /var/log/mysql/mysql-error.log, /var/log/mysqld.err, /var/log/mysqld.log - name: mysql-general enabled: true config: log_path: /var/log/mysql/mysql.log , /var/log/mysql.log, /var/log/mysqld.log, /var/lib/mysql/ip-*.log - name: mysql-slowquery enabled: true config: log_path: /var/lib/mysql/ip-*slow.log, /var/log/mysql/mysql-slow.log Copy Viewing data and dashboards  Data generated by plugin can be viewed inbrowse data page inside the respective application underplugin=mysql and documentType= serverDetailsDashboard for this data can be instantiated by Importing dashboard templateMySQL to the application dashboard "},{"title":"overview","type":0,"sectionRef":"#","url":"docs/Integrations/mysql/overview","content":"overview MySQL monitoring on SnappyFlow is available for the following platforms Instances​ Kubernetes​ Cloud services (with sfpoller plugin)​ Windows​","keywords":""},{"title":"Monitoring Kafka Clusters running in Kubernetes","type":0,"sectionRef":"#","url":"docs/Integrations/kafka/kafka_kubernetes","content":"","keywords":""},{"title":"Kafka monitoring with sfKubeAgent​","type":1,"pageTitle":"Monitoring Kafka Clusters running in Kubernetes","url":"docs/Integrations/kafka/kafka_kubernetes#kafka-monitoring-with-sfkubeagent","content":""},{"title":"Prerequisites​","type":1,"pageTitle":"Monitoring Kafka Clusters running in Kubernetes","url":"docs/Integrations/kafka/kafka_kubernetes#prerequisites","content":"sfKubeAgent container is run as a side-car n Kafka Pod Init-Container: sfKubeAgent plugin requires Jolokia to be run with the Kafka process. Jolokia exposes mbeans over default port 8778. Jolokia can be downloaded using init-container from the below url: https://repo1.maven.org/maven2/org/jolokia/jolokia-jvm/1.6.2/jolokia-jvm-1.6.2-agent.jar Copy Both Confluent and Apache distributions allow Java agents to be started along with Kafka process. To start Jolokia along with the Kafka broker , set environment variable KAFKA_JMX_OPTS along with Configured agent mount path in broker’s container. The value of Kafka-JMX-OPTS must be as below: - name: KAFKA_JMX_OPTS value: &quot;-javaagent:/agent/jolokia.jar=port=8778,host=127.0.0.1 -Djava.rmi.server.hostname=127.0.0.1 -Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false&quot; Copy Sample Init Container Configuration for including Jolokia initContainers: - name: get-jolokia image: alpine command: - sh - -c - -x - apk add --no-cache curl &amp;&amp; curl ${JOLOKIA_JAR_URL} -o /agent/jolokia.jar env: - name: JOLOKIA_JAR_URL value: https://repo1.maven.org/maven2/org/jolokia/jolokia-jvm/1.6.2/jolokia-jvm-1.6.2-agent.jar volumeMounts: - name: javaagentdir mountPath: /agent Copy Kafka Pod yaml: Example of the Kafka Pod.yaml is below: apiVersion: v1 kind: Pod metadata: labels: snappyflow/appname: &lt;application_name&gt; snappyflow/projectname: &lt;project_name&gt; name: kafka-pod spec: containers: - command: - sh - -exc - | export KAFKA_BROKER_ID=${HOSTNAME##*-} &amp;&amp; \\ export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka-pod.kafka-sfagent-kafka-sfagent-headless.default:9092,EXTERNAL://${HOST_IP}:$((31090 + ${KAFKA_BROKER_ID})) &amp;&amp; \\ exec /etc/confluent/docker/run env: - name: KAFKA_JMX_OPTS value: -javaagent:/agent/jolokia.jar=port=8778,host=127.0.0.1 -Djava.rmi.server.hostname=127.0.0.1 -Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false image: mahendra0939/kafka-auditable:latest imagePullPolicy: IfNotPresent name: kafka-broker ports: - containerPort: 9092 name: kafka protocol: TCP - containerPort: 8778 name: jolokia protocol: TCP volumeMounts: - mountPath: /opt/kafka/data name: datadir - mountPath: /etc/customlog4j name: connect-log4j-properties - mountPath: /var/log/kafka name: varlog - mountPath: /agent name: javaagentdir readOnly: true - command: - /app/sfagent - -enable-console-log env: - name: APP_NAME value: &lt;application_name&gt; - name: PROJECT_NAME value: &lt;project_name&gt; image: snappyflowml/sfagent:latest imagePullPolicy: Always name: sfagent volumeMounts: - mountPath: /var/log/kafka name: varlog - mountPath: /opt/sfagent/config.yaml name: sfagent-config-kafka subPath: config.yaml readOnly: true initContainers: - command: - sh - -c - -x - apk add --no-cache curl &amp;&amp; curl ${JOLOKIA_JAR_URL} -o /agent/jolokia.jar env: - name: JOLOKIA_JAR_URL value: https://repo1.maven.org/maven2/org/jolokia/jolokia-jvm/1.6.2/jolokia-jvm-1.6.2-agent.jar image: alpine imagePullPolicy: Always name: get-jolokia volumeMounts: - mountPath: /agent name: javaagentdir readOnly: true volumes: - emptyDir: {} name: datadir - emptyDir: {} name: javaagentdir - emptyDir: {} name: varlog - configMap: defaultMode: 420 name: kafka-sfkubeagent-configmap name: sfagent-config-kafka - configMap: defaultMode: 420 name: kafka-configmap name: connect-log4j-properties Copy In addition to the pod.yaml below config-map for sfkubeAgent container and Kafka container. Example for the config-map are below: Kafka config map: apiVersion: v1 kind: ConfigMap metadata: name: kafka-configmap data: connect-log4j.properties: &quot;log4j.rootLogger=INFO, kafkaAppender\\nlog4j.appender.kafkaAppender.File=/var/log/kafka/server.log\\nlog4j.appender.kafkaAppender.layout=org.apache.log4j.PatternLayout\\nlog4j.appender.kafkaAppender.layout.ConversionPattern=[%d] %p %m (%c)%n\\nlog4j.appender.kafkaAppender=org.apache.log4j.RollingFileAppender\\nlog4j.appender.kafkaAppender.MaxFileSize=5MB\\nlog4j.appender.kafkaAppender.MaxBackupIndex=5\\nlog4j.appender.stateChangeAppender.File=/var/log/kafka/state-change.log\\nlog4j.appender.stateChangeAppender.layout=org.apache.log4j.PatternLayout \\ \\nlog4j.appender.stateChangeAppender.layout.ConversionPattern=[%d] %p %m (%c)%n\\nlog4j.appender.stateChangeAppender=org.apache.log4j.RollingFileAppender\\nlog4j.appender.stateChangeAppender.MaxFileSize=5MB\\nlog4j.appender.stateChangeAppender.MaxBackupIndex=5\\ng4j.appender.requestAppender=org.apache.log4j.DailyRollingFileAppender\\nlog4j.appender.requestAppender.File=/var/log/kafka/kafka-request.log\\nlog4j.appender.requestAppender.layout=org.apache.log4j.PatternLayout\\nlog4j.appender.requestAppender.layout.ConversionPattern=[%d] %p %m (%c)%n\\nlog4j.appender.requestAppender=org.apache.log4j.RollingFileAppender\\nlog4j.appender.requestAppender.MaxFileSize=5MB\\nlog4j.appender.requestAppender.MaxBackupIndex=5\\nlog4j.appender.cleanerAppender.File=/logs/log-cleaner.log\\nlog4j.appender.cleanerAppender.layout=org.apache.log4j.PatternLayout\\nlog4j.appender.cleanerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n\\nlog4j.appender.cleanerAppender=org.apache.log4j.RollingFileAppender\\nlog4j.appender.cleanerAppender.MaxFileSize=5MB\\nlog4j.appender.cleanerAppender.MaxBackupIndex=5\\nlog4j.appender.controllerAppender.File=/var/log/kafka/controller.log\\nlog4j.appender.controllerAppender.layout=org.apache.log4j.PatternLayout\\nlog4j.appender.controllerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n\\nlog4j.appender.controllerAppender=org.apache.log4j.RollingFileAppender\\nlog4j.appender.controllerAppender.MaxFileSize=5MB\\nlog4j.appender.controllerAppender.MaxBackupIndex=5\\nlog4j.appender.authorizerAppender.File=/var/log/kafka/kafka-authorizer.log\\nlog4j.appender.authorizerAppender.layout=org.apache.log4j.PatternLayout\\nlog4j.appender.authorizerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n\\nlog4j.appender.authorizerAppender=org.apache.log4j.RollingFileAppender\\nlog4j.appender.authorizerAppender.MaxFileSize=5MB\\nlog4j.appender.authorizerAppender.MaxBackupIndex=5\\n log4j.logger.kafka=INFO\\nlog4j.logger.org.apache.kafka=INFO\\n log4j.logger.kafka.request.logger=WARN, requestAppender\\nlog4j.additivity.kafka.request.logger=false\\n log4j.logger.kafka.network.RequestChannel$=WARN, requestAppender\\nlog4j.additivity.kafka.network.RequestChannel$=false\\nlog4j.logger.kafka.controller=TRACE, controllerAppender\\nlog4j.additivity.kafka.controller=false\\nlog4j.logger.kafka.log.LogCleaner=INFO, cleanerAppender\\nlog4j.additivity.kafka.log.LogCleaner=false\\nlog4j.logger.state.change.logger=TRACE, stateChangeAppender\\nlog4j.additivity.state.change.logger=false\\n \\nlog4j.logger.kafka.authorizer.logger=DEBUG, authorizerAppender\\nlog4j.additivity.kafka.authorizer.logger=false\\n&quot; Copy Config map for Kafka sfKubeAgent: apiVersion: v1 kind: ConfigMap metadata: labels: app.kubernetes.io/managed-by: Helm snappyflow/appname: &lt;application_name&gt; snappyflow/projectname: &lt;project_name&gt; name: kafka-sfkubeagent-configmap data: config.yaml: | --- key: &quot;&lt;Profile_key&gt;” logging: plugins: - name: kafka-general enabled: true config: log_level: - error - info - debug - warn - notice log_path: /var/log/kafka/server.log,/var/log/kafka/state-change.log,/var/log/kafka/kafka-request.log,/var/log/kafka/controller.log,/var/log/kafka/kafka-authorizer.log metrics: plugins: - name: kube-sfagent-kafkatopic enabled: true interval: 300 config: documentsTypes: - kafkaStats - partitionStats - topicStats - consumerStats jolokiaPort: 8778 RMIPort: 8778 jolokiaContext: jolokia jolokiaProtocol: http jolokiaProxy: false port: 9092 process: kafka.Kafka,io.confluent.support.metrics.SupportedKafka - name: kube-sfagent-kafkajmx enabled: true interval: 300 config: process: kafka.Kafka,io.confluent.support.metrics.SupportedKafka jolokiaPort: 8778 RMIPort: 8778 jolokiaContext: jolokia jolokiaProtocol: http jolokiaProxy: false Copy Plugin Type kafkatopic consists of documents organized into following types Kafka stats: contain transactional data and metrics related to broker state Topic stats: provide metrics for analyzing internal transactions associated with each topic Partition stats: provide log size and segment information for each partition Consumer stats: provide consumer lag and offset information Plugin Type kafkajmx contains single type of document JVM stats: contain all JVM related metrics like garbage collection details, memory pools, loaded/unloaded classes etc. "},{"title":"Configuration Parameters Breakdown​","type":1,"pageTitle":"Monitoring Kafka Clusters running in Kubernetes","url":"docs/Integrations/kafka/kafka_kubernetes#configuration-parameters-breakdown","content":"jolokiaPort: port on which jolokia based requests are served jolokiaContext: context makes jolokia endpoint. Default value is string jolokiaProtocol: request/response protocol jolokiaProxy: jolokia proxy mode, if enabled for Jolokia agent RMIPort: RMI port. if enabled "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"Monitoring Kafka Clusters running in Kubernetes","url":"docs/Integrations/kafka/kafka_kubernetes#viewing-data-and-dashboards","content":"Metric data generated by plugin can be viewed in browse data page inside the respective application under metrics section with plugin= kube-sfagent-kafkajmx and documentType= jmxStats. Data from topics can be found in metrics section with plugin= kube-sfagent-kafkatopic and documentType= consumerStats , kafkaStats, partitionStats , topicStats. Dashboard for this data can be instantiated by Importing dashboard template Kafka_Kube_SF to the application dashboard For help with plugins, please reach out to support@snappyflow.io. "},{"title":"Kafka monitoring with Prometheus:​","type":1,"pageTitle":"Monitoring Kafka Clusters running in Kubernetes","url":"docs/Integrations/kafka/kafka_kubernetes#kafka-monitoring-with-prometheus","content":"Refer to Prometheus Exporter Overview to understand how SnappyFlow monitors using Prometheus exporters. "},{"title":"Prerequisites​","type":1,"pageTitle":"Monitoring Kafka Clusters running in Kubernetes","url":"docs/Integrations/kafka/kafka_kubernetes#prerequisites-1","content":"Enabling JMX Monitoring JMX monitoring needs to be enabled for the kafka process. Following properties need to be set for the process. -Dcom.sun.management.jmxremote -Djava.rmi.server.hostname=0.0.0.0 -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.port= userDefinedJMXPort -Dcom.sun.management.jmxremote.rmi.port= userDefinedJMXPort -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false Copy Prometheus JMX exporter Integration Docker Image: solsson/kafka-prometheus-jmx-exporter@sha256 Image Tag: a23062396cd5af1acdf76512632c20ea6be76885dfc20cd9ff40fb23846557e8 Copy Configuration: Exporter needs to be deployed as one of the containers in broker pods and following command needs to be executed as the actual container process java -jar jmx_prometheus_httpserver.jar userDefinedPrometheusPort configurationFile Copy Instrumentation: Prometheus instrumentation is based on the exporter rules specified in exporterConfigurationFile. Config File needs to be mounted with following contents jmxUrl: service:jmx:rmi:///jndi/rmi://127.0.0.1:userDefinedJMXPort/jmxrmi lowercaseOutputName: true lowercaseOutputLabelNames: true ssl: false whitelistObjectNames: [&quot;kafka.controller:*&quot;,&quot;kafka.server:*&quot;,&quot;java.lang:*&quot;,&quot;kafka.network:*&quot;,&quot;kafka.log:*&quot;] rules: - pattern: kafka.controller&lt;type=(ControllerChannelManager), name=(QueueSize), broker-id=(\\d+)&gt;&lt;&gt;(Value) name: kafka_controller_$1_$2_$4 labels: broker_id: &quot;$3&quot; - pattern: kafka.controller&lt;type=(ControllerChannelManager), name=(TotalQueueSize)&gt;&lt;&gt;(Value) name: kafka_controller_$1_$2_$3 - pattern: kafka.controller&lt;type=(KafkaController), name=(.+)&gt;&lt;&gt;(Value) name: kafka_controller_$1_$2_$3 - pattern: kafka.controller&lt;type=(ControllerStats), name=(.+)&gt;&lt;&gt;(Count|OneMinuteRate) name: kafka_controller_$1_$2_$3 - pattern: kafka.server&lt;type=(ReplicaFetcherManager), name=(.+), clientId=(.+)&gt;&lt;&gt;(Value) name: kafka_server_$1_$2_$4 labels: client_id: &quot;$3&quot; - pattern : kafka.network&lt;type=(Processor), name=(IdlePercent), networkProcessor=(.+)&gt;&lt;&gt;(Value) name: kafka_network_$1_$2_$4 labels: network_processor: $3 - pattern : kafka.network&lt;type=(RequestMetrics), name=(.+), request=([^,]+).*&gt;&lt;&gt;(Count|OneMinuteRate|Mean) name: kafka_network_$1_$2_$4 labels: request: $3 - pattern: kafka.server&lt;type=(.+), name=(.+), topic=(.+)&gt;&lt;&gt;(Count|OneMinuteRate) name: kafka_server_$1_$2_$4 labels: topic: $3 - pattern: kafka.server&lt;type=(DelayedOperationPurgatory), name=(.+), delayedOperation=(.+)&gt;&lt;&gt;(Value) name: kafka_server_$1_$2_$3_$4 - pattern: kafka.server&lt;type=(.+), name=(.+)&gt;&lt;&gt;(Count|Value|OneMinuteRate) name: kafka_server_$1_total_$2_$3 - pattern: kafka.server&lt;type=(.+)&gt;&lt;&gt;(queue-size) name: kafka_server_$1_$2 - pattern: java.lang&lt;type=(.+), name=(.+)&gt;&lt;(.+)&gt;(\\w+) name: java_lang_$1_$4_$3_$2 - pattern: java.lang&lt;type=(.+), name=(.+)&gt;&lt;&gt;(\\w+) name: java_lang_$1_$3_$2 - pattern : java.lang&lt;type=(.*)&gt; - pattern: kafka.log&lt;type=(.+), name=(.+), topic=(.+), partition=(.+)&gt;&lt;&gt;Value name: kafka_log_$1_$2 labels: topic: $3 partition: $4 Copy note By default, confluent and apache kafka helm charts ships with the JMX exporter. In these charts , JMX port will be set to 5555 by default .Exporter rules and whitelisted objects only needs to be updated with the above specified Instrumentation in the configuration file. Kubernetes service needs to be created for exposing userDefinedPrometheusPort Reference: Prometheus JMX exporter Danielqs Exporter Docker Image: danielqsj/kafka-exporter Image Tag: v1.0.1 Copy Configuration: Exporter is installed as a container in separate pod. Kafka broker details and prometheus listener port needs to be configured in following way - args: - --kafka.server= brokerhost:brokerPort - --web.listen-address=:userDefinedPrometheusListenerPort Copy Kubernetes service needs to be created for exposing userDefinedPrometheusListenerPort Reference: Danielqs Exporter "},{"title":"Documents​","type":1,"pageTitle":"Monitoring Kafka Clusters running in Kubernetes","url":"docs/Integrations/kafka/kafka_kubernetes#documents","content":"Plugin Type kube-prom-kafka-jmx contains the metrics organized into following document types Kafka stats: contain transactional data and metrics related to broker stateTopic stats: provide metrics for analyzing internal transactions associated with each topicPartition stats: provide log size information for each partitionjmx stats: contain all JVM related metrics like garbage collection details, memory pools, loaded/unloaded classes etc. Plugin Type kube-prom-kafka contains the metrics organized into following document types Consumer stats: Contains consumers related information and the total partiton count. Configmap for kafka Prometheus apiVersion: v1 kind: ConfigMap metadata: name: kafka-prom-configmap data: jmx-kafka-prometheus.yml: | jmxUrl: service:jmx:rmi:///jndi/rmi://127.0.0.1:5555/jmxrmi lowercaseOutputName: true lowercaseOutputLabelNames: true ssl: false whitelistObjectNames: [&quot;kafka.controller:*&quot;,&quot;kafka.server:*&quot;,&quot;java.lang:*&quot;,&quot;kafka.network:*&quot;,&quot;kafka.log:*&quot;] rules: - pattern: kafka.controller&lt;type=(ControllerChannelManager), name=(QueueSize), broker-id=(\\d+)&gt;&lt;&gt;(Value) name: kafka_controller_$1_$2_$4 labels: broker_id: &quot;$3&quot; - pattern: kafka.controller&lt;type=(ControllerChannelManager), name=(TotalQueueSize)&gt;&lt;&gt;(Value) name: kafka_controller_$1_$2_$3 - pattern: kafka.controller&lt;type=(KafkaController), name=(.+)&gt;&lt;&gt;(Value) name: kafka_controller_$1_$2_$3 - pattern: kafka.controller&lt;type=(ControllerStats), name=(.+)&gt;&lt;&gt;(Count|OneMinuteRate) name: kafka_controller_$1_$2_$3 - pattern: kafka.server&lt;type=(ReplicaFetcherManager), name=(.+), clientId=(.+)&gt;&lt;&gt;(Value) name: kafka_server_$1_$2_$4 labels: client_id: &quot;$3&quot; - pattern : kafka.network&lt;type=(Processor), name=(IdlePercent), networkProcessor=(.+)&gt;&lt;&gt;(Value) name: kafka_network_$1_$2_$4 labels: network_processor: $3 - pattern : kafka.network&lt;type=(RequestMetrics), name=(.+), request=([^,]+).*&gt;&lt;&gt;(Count|OneMinuteRate|Mean) name: kafka_network_$1_$2_$4 labels: request: $3 - pattern: kafka.server&lt;type=(.+), name=(.+), topic=(.+)&gt;&lt;&gt;(Count|OneMinuteRate) name: kafka_server_$1_$2_$4 labels: topic: $3 - pattern: kafka.server&lt;type=(DelayedOperationPurgatory), name=(.+), delayedOperation=(.+)&gt;&lt;&gt;(Value) name: kafka_server_$1_$2_$3_$4 - pattern: kafka.server&lt;type=(.+), name=(.+)&gt;&lt;&gt;(Count|Value|OneMinuteRate) name: kafka_server_$1_total_$2_$3 - pattern: kafka.server&lt;type=(.+)&gt;&lt;&gt;(queue-size) name: kafka_server_$1_$2 - pattern: java.lang&lt;type=(.+), name=(.+)&gt;&lt;(.+)&gt;(\\w+) name: java_lang_$1_$4_$3_$2 - pattern: java.lang&lt;type=(.+), name=(.+)&gt;&lt;&gt;(\\w+) name: java_lang_$1_$3_$2 - pattern : java.lang&lt;type=(.*)&gt; - pattern: kafka.log&lt;type=(.+), name=(.+), topic=(.+), partition=(.+)&gt;&lt;&gt;Value name: kafka_log_$1_$2 labels: topic: $3 partition: $4 Copy Kafka Service YAML apiVersion: v1 kind: Service metadata: labels: snappyflow/appname: &lt;application_name&gt; snappyflow/plugin: kafka-jmx snappyflow/projectname: &lt;project_name&gt; name: kafka-prom-service ports: - name: broker port: 9092 protocol: TCP targetPort: 9092 - name: jmx-exporter port: 5555 protocol: TCP targetPort: 5556 - name: kafka-exporter port: 9308 protocol: TCP targetPort: 9308 type: ClusterIP Copy Kafka Pod YAML apiVersion: v1 kind: Pod metadata: labels: snappyflow/appname: &lt;application_name&gt; snappyflow/component: kafka snappyflow/projectname: &lt;project_name&gt; name: kafka-prom-pod spec: containers: - command: - java - -XX:+UnlockExperimentalVMOptions - -XX:+UseCGroupMemoryLimitForHeap - -XX:MaxRAMFraction=1 - -XshowSettings:vm - -jar - jmx_prometheus_httpserver.jar - &quot;5556&quot; - /etc/jmx-kafka/jmx-kafka-prometheus.yml image: solsson/kafka-prometheus-jmx-exporter@sha256:a23062396cd5af1acdf76512632c20ea6be76885dfc20cd9ff40fb23846557e8 imagePullPolicy: IfNotPresent name: metrics ports: - containerPort: 5556 protocol: TCP volumeMounts: - mountPath: /etc/jmx-kafka name: jmx-config readOnly: true - command: - sh - -exc - | export KAFKA_BROKER_ID=${HOSTNAME##*-} &amp;&amp; \\ export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_NAME}.kafka-prom-kafka-headless.${POD_NAMESPACE}:9092,EXTERNAL://${HOST_IP}:$((31090 + ${KAFKA_BROKER_ID})) &amp;&amp; \\ exec /etc/confluent/docker/run env: - name: JMX_PORT value: &quot;5555&quot; - name: KAFKA_JMX_OPTS value: -Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false image: mahendra0939/kafka-auditable:latest imagePullPolicy: IfNotPresent livenessProbe: exec: command: - sh - -ec - /usr/bin/jps | /bin/grep -q SupportedKafka name: kafka-broker ports: - containerPort: 9092 name: kafka protocol: TCP - containerPort: 8778 name: jolokia protocol: TCP - containerPort: 5555 name: jmx protocol: TCP volumeMounts: - mountPath: /opt/kafka/data name: datadir - mountPath: /var/log/kafka name: varlog - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: kafka-prom-token-s8fmp readOnly: true - args: - --kafka.server=kafka-prom-service:9092 - --web.listen-address=:9308 image: danielqsj/kafka-exporter:v1.0.1 imagePullPolicy: IfNotPresent name: kafka-exporter ports: - containerPort: 9308 protocol: TCP volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: default-token-lv29f readOnly: true volumes: - name: datadir persistentVolumeClaim: claimName: datadir-kafka-prom-kafka-0 - emptyDir: {} name: varlog - configMap: defaultMode: 420 name: kafka-prom-configmap name: jmx-config - name: default-token-lv29f secret: defaultMode: 420 secretName: default-token-lv29f Copy "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"Monitoring Kafka Clusters running in Kubernetes","url":"docs/Integrations/kafka/kafka_kubernetes#viewing-data-and-dashboards-1","content":"Data generated by plugin can be viewed in browse data page inside the respective application under plugin= kube-prom-kafka and documentType= consumerStats . JMX Data can be found in metrics section with plugin= kube-prom-kafka-jmx and documentType= jmxStats , kafkaStats, partitionStats , topicStats.Dashboard for this data can be instantiated by Importing dashboard template Kafka_Kube_Prom to the application dashboard For help with plugins, please reach out to support@snappyflow.io. "},{"title":"Monitoring MySQL databases running on cloud services (Amazon RDS) using sfPoller","type":0,"sectionRef":"#","url":"docs/Integrations/mysql/mysql_sfpoller","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Monitoring MySQL databases running on cloud services (Amazon RDS) using sfPoller","url":"docs/Integrations/mysql/mysql_sfpoller#overview","content":"sfPoller includes all necessary plugins to connect to Public cloud APIs, Cloudwatch and Azure Monitor and enables easy monitoring of databases running on cloud services such as Amazon RDS and Azure. The video below explains the steps involved in setting up sfPoller to monitor a MySQL database running on AWS.  "},{"title":"Monitoring MySQL databases running on Windows","type":0,"sectionRef":"#","url":"docs/Integrations/mysql/mysql_windows","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Monitoring MySQL databases running on Windows","url":"docs/Integrations/mysql/mysql_windows#overview","content":"The MYSQL Metric plugin collects metrics for number of user connections and more. "},{"title":"Metrics plugin​","type":1,"pageTitle":"Monitoring MySQL databases running on Windows","url":"docs/Integrations/mysql/mysql_windows#metrics-plugin","content":"Collects metric data organized in following documentType under metrics index:  serverDetailsdatabaseDetailstableDetailsmasterReplicationDetailsslaveReplicationDetails "},{"title":"Prerequisites​","type":1,"pageTitle":"Monitoring MySQL databases running on Windows","url":"docs/Integrations/mysql/mysql_windows#prerequisites","content":"MySQL 5.6 or above "},{"title":"Logger plugin​","type":1,"pageTitle":"Monitoring MySQL databases running on Windows","url":"docs/Integrations/mysql/mysql_windows#logger-plugin","content":"collects general logs and slow query logs. General logs are sent to log index whereas slow queries are sent to metrics index under documentType:mysqlSlowQueryLogs  "},{"title":"Set access permissions​","type":1,"pageTitle":"Monitoring MySQL databases running on Windows","url":"docs/Integrations/mysql/mysql_windows#set-access-permissions","content":"Username used for DB access should have appropriate permissions  grant select on information_schema.* to 'username' identified by 'password'; grant select on performance_schema.* to 'username' identified by 'password'; Copy note Root user has these permissions by default  "},{"title":"Enable Replication(optional)​","type":1,"pageTitle":"Monitoring MySQL databases running on Windows","url":"docs/Integrations/mysql/mysql_windows#enable-replicationoptional","content":"To collect the replication details replication has to be enabled. Execute the following queries on the slave using the login of the user provided in the config.yaml file: 1)&quot;show slave status&quot; 2)&quot;select * from replication_connection_status&quot; If the user is able to execute these queries then the replication details can be collected. Commands to create a replication user if you want to enable replication: the commands to be executed on the source or master, CREATE USER 'replica_user'@'slave_server_ip' IDENTIFIED WITH mysql_native_password BY 'password'; Copy GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'replica_user'@'slave_server_ip'; Copy "},{"title":"Configuration Settings​","type":1,"pageTitle":"Monitoring MySQL databases running on Windows","url":"docs/Integrations/mysql/mysql_windows#configuration-settings","content":"Add the plugin configuration in config.yaml file under C:\\Program Files (x86)\\Sfagent\\ directory as follows to enable this plugin - name: mysql enabled: true interval: 300 config: port: 3306 host: 127.0.0.1 user: xxxx password: xxxx documentsTypes: - databaseDetails - serverDetails - tableDetails - masterReplicationDetails #optional to be enabled when replication is setup - slaveReplicationDetails #optional to be enabled when replication is setup Copy For help with plugins, please reach out to support@snappyflow.io Viewing data and dashboards  Data generated by plugin can be viewed inbrowse data page inside the respective application underplugin=mysql and documentType= serverDetailsDashboard for this data can be instantiated by Importing dashboard templateMySQL to the application dashboard "},{"title":"Monitoring MongoDB","type":0,"sectionRef":"#","url":"docs/Integrations/mongodb","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Monitoring MongoDB","url":"docs/Integrations/mongodb#overview","content":"MongoDB plugin is an sfAgent plugin used to collect mongodb metrics. It uses serverStatus, dbstats command to retrieve the statistics. If replication is enabled and in cluster mode(replicaset) then collects the replication information. Support monitoring on: Standalone modeCluster (replicaset) mode MongoDB Monitoring in Standalone mode "},{"title":"Prerequisites​","type":1,"pageTitle":"Monitoring MongoDB","url":"docs/Integrations/mongodb#prerequisites","content":"sfAgent requires access to MongoDB system tables to collect metrics. This will require adding user credentials to sfAgent plugin configuration. User role and credentials can be created with the procedure mentioned below Open /etc/mongod.conf file and comment security config if it exists. This is needed to make changes to config file and will be uncommented in subsequent step #security: #authorization: enabled Copy Start mongodb service with command service mongod start Check mongodb status is active using command service mongod status Create mongostatRole and listDatabases roles Use admin Execute the command below db.createRole( { role: &quot;mongostatRole&quot;, privileges: [ { resource: { cluster: true }, actions: [ &quot;serverStatus&quot; ] }], roles: [] }) db.createRole( { role: &quot;listDatabases&quot;, privileges: [ { resource: { cluster: true }, actions: [ &quot;listDatabases&quot; ] }], roles: [] }) Copy Create new user with clusterMonitor , mongostatRole and listDatabases roles db.createUser( { user:&quot;Newuser&quot;, pwd: &quot;pass&quot;, roles : [ { role : &quot;mongostatRole&quot;, db : &quot;admin&quot; }, { role : &quot;listDatabases&quot;, db :&quot;admin&quot; }, { role : &quot;clusterMonitor&quot;, db : &quot;admin&quot; } ] } ) Copy Exit mongo using exit command Open /etc/mongod.conf file and uncomment security config if exists else add the config security: authorization: enabled Copy Restart mongodb using command service mongod restart Use the created mongo user credentials in sfagent config and start sfagent "},{"title":"Configuration​","type":1,"pageTitle":"Monitoring MongoDB","url":"docs/Integrations/mongodb#configuration","content":"sfAgent section provides steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory. key: &lt;Profile_key&gt; tags: Name: &lt;instance_name&gt; appName: &lt;app_name&gt; projectName: &lt;project_name&gt; metrics: plugins: - name: mongodb enabled: true interval: 300 config: port: 27017 host: localhost username: Newuser password: pass Copy "},{"title":"Metrics list​","type":1,"pageTitle":"Monitoring MongoDB","url":"docs/Integrations/mongodb#metrics-list","content":"documentType : dbStats​ Name\tDescriptionCollections\tContains a count of the number of collections in that database DataSize\tTotal size of the data held in this database including the padding factor.Shown as byte Db\tdatabase name IndexSize\tTotal size of all indexes created on this database.Shown as byte StorageSize\tTotal amount of space allocated to collections in this database for document storage.Shown as byte TotalSize\tTotal size of the data held in this database including the padding factor documentType : systemInfo​ Name\tDescriptionVersion\tversion of mongodb UptimeMillis\tNumber of seconds that the mongos or mongod process has been active availabeConnection\tNumber of unused available incoming connections the database can provide currentConnection\tNumber of connections to the database server from clients activeConnections\tNumber of active connections to the database server from clients aggDataSize\tTotal size of the data held in this database including the padding factor FsTotalSize\tTotal size of all disk capacity on the filesystem where MongoDB stores data aggFsUsedSize\tTotal size of all disk space in use on the filesystem where MongoDB stores data aggStorageSize\tSum of the space allocated to all collections in the database for document storage, including free space aggTotalSize\tSum of the space allocated for both documents and indexes in all collections in the database. Includes used and free storage space. This is the sum of storageSize and indexSize TotalDb\ttotal number of databases documentType : operationalMetrics​ Name\tDescriptioninsertQuery\tNumber of times insert executed updateQuery\tNumber of times update executed deleteQuery\tNumber of times delete executed dirtyBytesInCache\tSize of the dirty data in the cache CacheSize\tMaximum cache size CurrentQueueTotal\tTotal number of operations queued waiting for the lock CurrentQueueReaders\tNumber of operations that are currently queued and waiting for the read lock CurrentQueueWriters\tNumber of operations that are currently queued and waiting for the write lock ActiveClientsTotal\tTotal number of active client connections to the database ActiveClientsReaders\tCount of the active client connections performing read operations ActiveClientswriters\tCount of active client connections performing write operations readLatency\tTotal latency statistics for read requests per second writeLatency\tTotal latency statistics for write operations per second PhysicalBytesIn\tThe number of bytes that reflects the amount of network traffic received by this database PhysicalBytesOut\tThe number of bytes that reflects the amount of network traffic sent from this database CurrentCache\tSize of the data currently in cache Viewing data and dashboards Data collected by the plugin can be viewed in SnappyFlow’s browse data section. plugin: mongodbdocumentType: dbStats, systemInfo, operationalMetricsDashboard template: MongoDBAlerts template: MongoDB  MongoDB Monitoring in cluster(replicaset) mode MongoDB cluster is a set of nodes that are primary and secondary nodes. The nodes are synced and maintain same data in all the nodes in the replicaset type. The synchronization and the node information are collected about the cluster. Install the agent on all the nodes in the cluster, both the primary nodes and the secondary nodes. Supported on: MongoDB version 5.0 There are four documentTypes related to the replication: nodeDetails replicationMemberDetails replicationSyncDetails oplogDetails Pre-requisites​ Note: The replication should be the replicaset based replication. sfAgent requires access to MongoDB system tables to collect metrics. This will require adding user credentials to sfAgent plugin configuration. User role and credentials can be created with the procedure mentioned below Open /etc/mongod.conf file and comment security config if it exists. This is needed to make changes to config file and will be uncommented in subsequent step #security: #authorization: enabled Copy Start mongodb service with command service mongod start Check mongodb status is active using command service mongod status Create mongostatRole and listDatabases roles Use admin Execute the command below db.createRole( { role: &quot;mongostatRole&quot;, privileges: [ { resource: { cluster: true }, actions: [ &quot;serverStatus&quot; ] }], roles: [] }) db.createRole( { role: &quot;listDatabases&quot;, privileges: [ { resource: { cluster: true }, actions: [ &quot;listDatabases&quot; ] }], roles: [] }) Copy Create new user with clusterMonitor , mongostatRole and listDatabases roles db.createUser( { user:&quot;Newuser&quot;, pwd: &quot;pass&quot;, roles : [ { role : &quot;mongostatRole&quot;, db : &quot;admin&quot; }, { role : &quot;listDatabases&quot;, db :&quot;admin&quot; }, { role : &quot;clusterMonitor&quot;, db : &quot;admin&quot; } ] } ) Copy Exit mongo using exit command Open /etc/mongod.conf file and uncomment security config if exists else add the config security: authorization: enabled Copy Restart mongodb using command service mongod restart Use the created mongo user credentials in sfagent config and start sfagent "},{"title":"Configuration​","type":1,"pageTitle":"Monitoring MongoDB","url":"docs/Integrations/mongodb#configuration-1","content":"sfAgent section provides steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory. key: &lt;Profile_key&gt; tags: Name: &lt;instance_name&gt; appName: &lt;app_name&gt; projectName: &lt;project_name&gt; metrics: plugins: - name: mongodb enabled: true interval: 300 config: port: 27017 host: localhost username: Newuser password: pass Copy "},{"title":"Metrics list​","type":1,"pageTitle":"Monitoring MongoDB","url":"docs/Integrations/mongodb#metrics-list-1","content":"documentType : ReplicationSyncDetails​ Name\tDescriptionsource\tsecondary node name(ip) syncedTo\tsyncronised time repllag\tDelay between a write operation on the primary and its copy to a secondary. Computed on each node and tagged by 'host', but may not be representative of cluster health. Negative values do not indicate that the secondary is ahead of the primary. SyncState\twhen instance down it shows the info of the state ErrMessage\tit shows the error message when the instance is down documentType : ReplicationMemberdetails​ Name\tDescriptionname\tThe name of the member Id\tid of the member SyncSourceID\tThe syncSourceId field holds the [replSetGetStatus.membersn]._id of the member from which this instance syncs health\tA number that indicates if the member is up (i.e. 1) or down (i.e. 0) state\tAn integer between 0 and 10 that represents the replica state of the member stateStr\tA string that describes state uptime\tThe uptime field holds a value that reflects the number of seconds that this member has been online lastHeartbeat\tAn ISODate formatted date and time that reflects the last time the server that processed the replSetGetStatus command received a response from a heartbeat that it sent to this member (members[n]) lastHeartbeatRecv\tAn ISODate formatted date and time that reflects the last time the server that processed the replSetGetStatus command received a heartbeat request from this member (members[n]) syncSourceHost\tThe syncSourceHost field holds the hostname of the member from which this instance syncs lastHeartbeatMessage\tWhen the last heartbeat included an extra message, the lastHeartbeatMessage contains a string representation of that message lastCommittedOpTime\tInformation, from the viewpoint of this member, regarding the most recent operation that has been written to a majority of replica set members. appliedOpTime\tInformation, from the viewpoint of this member, regarding the most recent operation that has been applied to this member of the replica set durableOpTime\tInformation, from the viewpoint of this member, regarding the most recent operation that has been written to the journal of this member of the replica set InfoMessage\tgives information on the member state If the instance is down documentType : nodeDetails​ Name\tDescriptionisWritablePrimary\tis the node writable name\tthe node name(ip) primary\tis a primary member of a replica set readOnly\tis node running in read-only mode secondary\tis a secondary member of a replica set setName\treplicaset name documentType : oplogDetails​ Name\tDescriptionlogSizeMB\tTotal size of the oplog usedMB\tTotal amount of space used by the oplog timeDiff(in secs)\tdifference between the first and last operation in the oplog tFirst\tReturns a time stamp for the first (i.e. earliest) operation in the oplog. Compare this value to the last write operation issued against the server.Only present if there are entries in the oplog tLast\tReturns a time stamp for the last (i.e. latest) operation in the oplog. Compare this value to the last write operation issued against the server.Only present if there are entries in the oplog now\tReturns a timestamp that reflects the current time. The shell process generates this value, and the datum may differ slightly from the server time if you're connecting from a remote host as a result. Equivalent to Date().Only present if there are entries in the oplog "},{"title":" Viewing data and dashboards​","type":1,"pageTitle":"Monitoring MongoDB","url":"docs/Integrations/mongodb#viewing-data-and-dashboards","content":"Data collected by the plugin can be viewed in SnappyFlow’s browse data section  Plugin = mongodb documentType(in primary)= dbStats, operationalMetrics, systemInfo, nodeDetails, replicationMemberDetails, replicationSyncDetails, oplogDetails documentType(in secondary)= dbStats, operationalMetrics, systemInfo, nodeDetails, oplogDetails Dashboard template: MongoDBReplicationAlerts template: MongoDBReplication "},{"title":"See Also​","type":1,"pageTitle":"Monitoring MongoDB","url":"docs/Integrations/mongodb#see-also","content":"MySQL PostgresDB "},{"title":"Monitoring Nginx on Instances","type":0,"sectionRef":"#","url":"docs/Integrations/nginx/nginx_instance","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Monitoring Nginx on Instances","url":"docs/Integrations/nginx/nginx_instance#overview","content":"Nginx monitoring involves monitoring of the following elements: Nginx Access Logs Nginx Error Logs Nginx Server Health "},{"title":"Pre-requisites​","type":1,"pageTitle":"Monitoring Nginx on Instances","url":"docs/Integrations/nginx/nginx_instance#pre-requisites","content":""},{"title":"Access Log Format​","type":1,"pageTitle":"Monitoring Nginx on Instances","url":"docs/Integrations/nginx/nginx_instance#access-log-format","content":"Ensure Nginx access logs are in format expected by sfAgent parser. Edit nginx conf file /etc/nginx/nginx.conf and set log format as follows: '$remote_addr $remote_user [$time_local] ' '&quot;$request&quot; $status $body_bytes_sent ' '&quot;$http_referer&quot; &quot;$http_user_agent&quot; ua=&quot;$upstream_addr&quot; ' 'rt=$request_time uct=$upstream_connect_time uht=$upstream_header_time urt=$upstream_response_time'; Copy Sample: log_format snappyflow '$remote_addr $remote_user [$time_local] ' '&quot;$request&quot; $status $body_bytes_sent ' '&quot;$http_referer&quot; &quot;$http_user_agent&quot; ua=&quot;$upstream_addr&quot; ' 'rt=$request_time uct=$upstream_connect_time uht=$upstream_header_time urt=$upstream_response_time'; access_log /var/log/nginx/access.log snappyflow buffer=16k flush=5s; Copy After configuring log format, the expected log entry would be: 172.31.72.81 - [01/Jul/2020:03:36:04 +0000] &quot;POST /owners/6/edit HTTP/1.1&quot; 504 167 &quot;-&quot; &quot;Apache-HttpClient/4.5.7 (Java/1.8.0_252)&quot; ua=&quot;-&quot; rt=60.004 uct=- uht=- urt=60.004 Copy Description of log fields is as follows: remote_addr: Client address.remote_user: User name supplied with the Basic authentication.time_local: Time when the log entry is written.request: Full original request line.status: Response status code.body_bytes_sent: Number of bytes sent to a client (not counting the response header).http_referer: Client request header field 'Referer'.http_user_agent: Client request header field 'User-agent'. Useful to get the client host details like OS, browser, Device.upstream_addr: Keeps the IP address and port, or the path to the UNIX-domain socket of the upstream server.request_time: Request processing time in seconds with a milliseconds resolution; time elapsed between the first bytes were read from the client and the log write after the last bytes were sent to the client.upstream_connect_time: Keeps time spent on establishing a connection with the upstream server, in seconds with millisecond resolution.upstream_header_time: Keeps time spent on receiving the response header from the upstream server, in seconds with millisecond resolution. upstream_response_time: Keeps time spent on receiving the response from the upstream server, in seconds with millisecond resolution. Optional fields supported: remote_port: Client port. Add after the remote_addr field as follows: log_format snappyflow '$remote_addr:$remote_port $remote_user [$time_local] ' '&quot;$request&quot; $status $body_bytes_sent ' '&quot;$http_referer&quot; &quot;$http_user_agent&quot; ua=&quot;$upstream_addr&quot; ' 'rt=$request_time uct=$upstream_connect_time uht=$upstream_header_time urt=$upstream_response_time'; access_log /var/log/nginx/access.log snappyflow buffer=16k flush=5s; Copy request_length: Request length including request line, header, and request body. Add it in the end after $upstream_response_time as follows: log_format snappyflow '$remote_addr $remote_user [$time_local] ' '&quot;$request&quot; $status $body_bytes_sent ' '&quot;$http_referer&quot; &quot;$http_user_agent&quot; ua=&quot;$upstream_addr&quot; ' 'rt=$request_time uct=$upstream_connect_time uht=$upstream_header_time urt=$upstream_response_time rs=$request_length'; access_log /var/log/nginx/access.log snappyflow buffer=16k flush=5s; Copy "},{"title":"Nginx Status Module​","type":1,"pageTitle":"Monitoring Nginx on Instances","url":"docs/Integrations/nginx/nginx_instance#nginx-status-module","content":"Enable Nginx status module: This is required to monitor Nginx server health Open source Nginx exposes several basic metrics about server activity on a simple status page, provided that you have HTTP Stub Status Module enabled. To check if the module is already enabled, run: nginx -V 2&gt;&amp;1 | grep -o with-http_stub_status_module Copy The status module is enabled if you see with-http_stub_status_module as output in the terminal. In order to enable mod_status , you will need to enable the status module. You can use the --with-http_stub_status_module configuration parameter when building Nginx from source:  ./configure \\ … \\ --with-http_stub_status_module make sudo make install Copy After verifying the module is enabled, you will also need to modify your Nginx configuration to set up a locally accessible URL (e.g., /stats) for the status page:  server { location /stats { stub_status; access_log off; allow 127.0.0.1; deny all; } } Copy note The server blocks of Nginx config are usually found not in the master configuration file (e.g., /etc/nginx/nginx.conf) but in supplemental configuration files that are referenced by the master config. To find the relevant configuration files, first locate the master config by running: nginx -t Open the master configuration file listed, and look for lines that begin with “include” near the end of the http block, e.g.: include /etc/nginx/conf.d/*.conf; In one of the referenced config files you should find the main server block, which you can modify as above to configure Nginx metrics reporting. After changing any configurations, reload the configs by executing: nginx -s reload Now you can view the status page to see your metrics:http://127.0.0.1/stats "},{"title":"Configuration​","type":1,"pageTitle":"Monitoring Nginx on Instances","url":"docs/Integrations/nginx/nginx_instance#configuration","content":"sfAgent section provides steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: &lt;profile key&gt; generate_name: true tags: Name: &lt;unique instance name or will be generated from IP&gt; appName: &lt;add application name&gt; projectName: &lt;add project name&gt; metrics: plugins: - name: nginx enabled: true interval: 300 config: port: 80 secure: false location: stats logging: plugins: - name: nginx-access enabled: true config: geo_info: true log_path: /var/log/nginx/access.log, /var/log/nginx/access_log ua_parser: true - name: nginx-error enabled: true config: log_level: - emerg - alert - error log_path: /var/log/nginx/error.log, /var/log/nginx/error_log Copy "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"Monitoring Nginx on Instances","url":"docs/Integrations/nginx/nginx_instance#viewing-data-and-dashboards","content":"Data generated by plugin can be viewed in “browse data” page inside the respective application under plugin=nginx with documentType= serverDetails, serverStats and plugin=nginx-access with documentType=nginxAccessLogs.Dashboard for this data can be instantiated by Importing dashboard template Nginx_Server, Nginx_Access to the application dashboard. "},{"title":"Monitoring File Stats using fileStats Plugin","type":0,"sectionRef":"#","url":"docs/Integrations/os/linux/filestats_plugin","content":"","keywords":""},{"title":"Tested on​​","type":1,"pageTitle":"Monitoring File Stats using fileStats Plugin","url":"docs/Integrations/os/linux/filestats_plugin#tested-ondirect-link-to-heading","content":"Centos: 7.x RHEL: 7.x Ubuntu: 14.x, 16.x "},{"title":"Prerequisites - Install sysstat command​","type":1,"pageTitle":"Monitoring File Stats using fileStats Plugin","url":"docs/Integrations/os/linux/filestats_plugin#prerequisites---install-sysstat-command","content":"sfAgent must be installed The cpuLoadStats plugin requires sysstat to be installed To install Sysstat in CentOS/RHEL:  sudo yum install sysstat Copy To install Sysstat in Ubuntu OS:  sudo apt install sysstat Copy Agent Configuration Add the configuration shown below to config.yaml under /opt/sfagent/ directory Default config  key: &lt;profile_key&gt; tags: Name: &lt;name&gt; appName: &lt;app_name&gt; projectName: &lt;project_name&gt; metrics: plugins: - name: fileStats group_name: linux enabled: true Copy "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"Monitoring File Stats using fileStats Plugin","url":"docs/Integrations/os/linux/filestats_plugin#viewing-data-and-dashboards","content":"Data collected by plugin can be viewed in SnappyFlow’s browse data section under metrics plugin=fileStatsdocumentType=fileStats Dashboard of fileStats data can be rendered using Template= LinuxCpuLoad ​ "},{"title":"Overview","type":0,"sectionRef":"#","url":"docs/Integrations/nginx/overview","content":"Overview NGINX monitoring on SnappyFlow is available for the following platforms Instances​ Kubernetes​","keywords":""},{"title":"Monitoring CPU load using cpuLoadStats Plugin","type":0,"sectionRef":"#","url":"docs/Integrations/os/linux/cpuloadstats_plugin","content":"","keywords":""},{"title":"Tested on​","type":1,"pageTitle":"Monitoring CPU load using cpuLoadStats Plugin","url":"docs/Integrations/os/linux/cpuloadstats_plugin#tested-on","content":"Centos: 7.x RHEL: 7.x Ubuntu: 14.x, 16.x "},{"title":"Prerequisites​","type":1,"pageTitle":"Monitoring CPU load using cpuLoadStats Plugin","url":"docs/Integrations/os/linux/cpuloadstats_plugin#prerequisites","content":"sfAgent must be installed The cpuLoadStats plugin requires sysstat to be installed To install Sysstat in CentOS/RHEL  sudo yum install sysstat Copy To install Sysstat in Ubuntu OS  sudo apt install sysstat Copy Configuring the agent Add the configuration shown below to config.yaml under /opt/sfagent/ directory Default config  key: &lt;profile_key&gt; tags: Name: &lt;name&gt; appName: &lt;app_name&gt; projectName: &lt;project_name&gt; metrics: plugins: - name: cpuLoadStats group_name: linux enabled: true Copy "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"Monitoring CPU load using cpuLoadStats Plugin","url":"docs/Integrations/os/linux/cpuloadstats_plugin#viewing-data-and-dashboards","content":"Data collected by plugin can be viewed in SnappyFlow’s browse data section under metrics plugin=cpuLoadStatsdocumentType=cpuLoadStats Dashboard of cpuLoadStats data can be rendered using Template= LinuxCpuLoad ​ "},{"title":"","type":1,"pageTitle":"Monitoring CPU load using cpuLoadStats Plugin","url":"docs/Integrations/os/linux/cpuloadstats_plugin##","content":""},{"title":"Monitoring Nginx in Kubernetes","type":0,"sectionRef":"#","url":"docs/Integrations/nginx/nginx_kubernetes","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Monitoring Nginx in Kubernetes","url":"docs/Integrations/nginx/nginx_kubernetes#overview","content":"Nginx monitoring involves monitoring of the following elements: Nginx Access Logs Nginx Error Logs Nginx Server Health "},{"title":"Pre-reading​","type":1,"pageTitle":"Monitoring Nginx in Kubernetes","url":"docs/Integrations/nginx/nginx_kubernetes#pre-reading","content":"Refer to the links below for generic approach to monitoring application metrics and logs in Kubernetes environment sfKubeAgent Prometheus Exporter Centralized Log Monitoring  Refer to Nginx monitoring on instances for sfAgent configurations "},{"title":"Configuration​","type":1,"pageTitle":"Monitoring Nginx in Kubernetes","url":"docs/Integrations/nginx/nginx_kubernetes#configuration","content":""},{"title":"Configure Nginx server to enable monitoring​","type":1,"pageTitle":"Monitoring Nginx in Kubernetes","url":"docs/Integrations/nginx/nginx_kubernetes#configure-nginx-server-to-enable-monitoring","content":"Configure format of access logs so that it can be parsed by SnappyFlow Enable Nginx status module to monitor Nginx server health These configurations can be achieved with the below ConfigMap: apiVersion: v1 kind: ConfigMap metadata: name: nginx-configmap labels: snappyflow/appname: &lt;app_name&gt; snappyflow/projectname: &lt;project_name&gt; data: nginx.conf: | worker_processes 5; events { worker_connections 4096; } http { default_type application/octet-stream; log_format upstream_time '$remote_addr:$remote_port $remote_user [$time_local] ' '&quot;$request&quot; $status $body_bytes_sent ' '&quot;$http_referer&quot; &quot;$http_user_agent&quot; &quot;$http_referer&quot; ' 'rt=$request_time uct=$upstream_connect_time uht=$upstream_header_time urt=$upstream_response_time'; server { listen 80; error_log /var/log/nginx/error1.log; access_log /var/log/nginx/access1.log upstream_time; location /nginx_status { stub_status; } } } Copy "},{"title":"sfKubeAgent​","type":1,"pageTitle":"Monitoring Nginx in Kubernetes","url":"docs/Integrations/nginx/nginx_kubernetes#sfkubeagent","content":"sfKubeAgent is deployed as sidecar container in the NGINX pod and can be used to monitor Nginx server health as well as Access Logs &amp; Error Logs. Below YAML files provide example for setting up NGINX monitoring with sfKubeAgent. sfKubeAgent ConfigMap (sfAgent-config.yaml) apiVersion: v1 kind: ConfigMap metadata: name: sfagent-configmap labels: snappyflow/appname: &lt;app_name&gt; snappyflow/projectname: &lt;project_name&gt; data: config.yaml: |+ --- key: &quot;&lt;profile_key&gt;&quot; metrics: plugins: - name: kube-sfagent-nginx enabled: true interval: 300 config: location: nginx_status port: 80 secure: false logging: plugins: - name: nginx-access enabled: true config: log_path: &quot;/var/log/nginx/access1.log&quot; - name: nginx-error enabled: true config: log_path: &quot;/var/log/nginx/error1.log&quot; Copy Pod description YAML running NGINX and sfKubeAgent apiVersion: v1 metadata: name: my-first-pod labels: snappyflow/appname: &lt;app_name&gt; snappyflow/projectname: &lt;project_name&gt; spec: containers: - name: nginx-container image: nginx:latest imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP volumeMounts: - name: varlog mountPath: /var/log/nginx - name: nginx-config mountPath: /etc/nginx/nginx.conf subPath: nginx.conf # Snappyflow's sfkubeagent container - name: nginx-sfagent image: snappyflowml/sfagent:latest imagePullPolicy: Always command: - /app/sfagent - -enable-console-log env: - name: APP_NAME value: &lt;app_name&gt; - name: PROJECT_NAME value: &lt;project_name&gt; volumeMounts: - name: nginx-sfagent-config mountPath: /opt/sfagent/config.yaml subPath: config.yaml - name: varlog mountPath: /var/log/nginx volumes: - name: nginx-sfagent-config configMap: name: sfagent-configmap - name: nginx-config configMap: name: nginx-configmap - name: varlog emptyDir: {} Copy "},{"title":"Centralized logging​","type":1,"pageTitle":"Monitoring Nginx in Kubernetes","url":"docs/Integrations/nginx/nginx_kubernetes#centralized-logging","content":"Log monitoring (both access and error logs) can be implemented through Centralized Logging approach as well which does not require sfKubeAgent. Centralized logging however requires running a busybox container as a sidecar container to stream logs to container’s stdout. Add the label - snappyflow/component: nginx, which signals to apply Nginx to container’s stdout. "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"Monitoring Nginx in Kubernetes","url":"docs/Integrations/nginx/nginx_kubernetes#viewing-data-and-dashboards","content":"Data generated by plugin can be viewed in browse data page inside the respective application under plugin=jvm and documentType=jvm Dashboard for this data can be instantiated by Importing dashboard template JVM to the application dashboard. "},{"title":"SnappyFlow Linux Integrations","type":0,"sectionRef":"#","url":"docs/Integrations/os/linux/overview","content":"SnappyFlow Linux Integrations Setup Guides Setting up sfAgent on Linux Instances Monitor Linux OS LSOF PSUtil Netstat","keywords":""},{"title":"NetStat Monitoring","type":0,"sectionRef":"#","url":"docs/Integrations/os/linux/netstat","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"NetStat Monitoring","url":"docs/Integrations/os/linux/netstat#overview","content":"Netstat Metric plugin is an agent-based plugin that collects below data for each process running on the machine. Tcp ConnectionsUdp ConnectionsTcpEstablish ConnectionsTcpListening ConnectionsTcpOpening ConnectionsTcpClosing ConnectionsTcpWaiting ConnectionsTraffic in MBBytes AckedBytes ReceivedTcp Udp futher info table "},{"title":"Prerequisite - Install netstat command​","type":1,"pageTitle":"NetStat Monitoring","url":"docs/Integrations/os/linux/netstat#prerequisite---install-netstat-command","content":"If this package is not present, use the following commands to install it. To install net-tools package in CentOS/RHEL: sudo yum install net-tools To install net-tools package in Ubuntu OS: sudo apt-get install net-tools "},{"title":"Agent Configuration​","type":1,"pageTitle":"NetStat Monitoring","url":"docs/Integrations/os/linux/netstat#agent-configuration","content":"Refer to sfAgent section for steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: &lt;profile_key&gt; tags: Name: &lt;name&gt; appName: &lt;app_name&gt; projectName: &lt;project_name&gt; metrics: plugins: - name: netstat enabled: true interval: 60 Copy "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"NetStat Monitoring","url":"docs/Integrations/os/linux/netstat#viewing-data-and-dashboards","content":"Data collected by plugin can be viewed in SnappyFlow’s browse data section under metrics plugin=netstatdocumentType=netstatDetails. Dashboard of Netstat data can be rendered using Template= Netstat "},{"title":"Test Matrix​","type":1,"pageTitle":"NetStat Monitoring","url":"docs/Integrations/os/linux/netstat#test-matrix","content":"Centos: 7.x RHEL: 7.x Ubuntu: 14.x, 16.x "},{"title":"See Also​","type":1,"pageTitle":"NetStat Monitoring","url":"docs/Integrations/os/linux/netstat#see-also","content":"Linux monitoringLSOFPSUtilCustom plugins using StatsDPrometheus Integration "},{"title":"Monitoring Linux OS","type":0,"sectionRef":"#","url":"docs/Integrations/os/linux/linux_os","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Monitoring Linux OS","url":"docs/Integrations/os/linux/linux_os#overview","content":"OS monitoring is the most commonly needed and most important aspect of monitoring. SnappyFlow provides a comprehensive monitoring of Linux OS through multiple plugins. Linux base metric plugin provides following data: CPU Static and Dynamic Metrics Memory Metrics Disk IO Metrics Network IO Metrics TCP Metrics Syslog logging plugin  "},{"title":"Configuration​","type":1,"pageTitle":"Monitoring Linux OS","url":"docs/Integrations/os/linux/linux_os#configuration","content":"Refer to sfAgent section for steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: &lt;profile_key&gt; tags: Name: &lt;name&gt; appName: &lt;app_name&gt; projectName: &lt;project_name&gt; metrics: plugins: - name: linux enabled: true interval: 30 logging: plugins: - name: linux-syslog enabled: true config: log_level: - error - warning - info log_path: /var/log/syslog,/var/log/auth.log,/var/log/messages,/var/log/secur Copy "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"Monitoring Linux OS","url":"docs/Integrations/os/linux/linux_os#viewing-data-and-dashboards","content":"Data collected by plugins can be viewed in SnappyFlow’s browse data section under metrics or logs or trace depending on the plugin Linux metrics data plugin= linux documentType cpu_static cpu_utilram_util disk_stats nic_stats tcp_stats Syslog data plugin= linux-syslog documentType= syslog  "},{"title":"Test Matrix​","type":1,"pageTitle":"Monitoring Linux OS","url":"docs/Integrations/os/linux/linux_os#test-matrix","content":"Centos: 7.x RHEL: 7.x Ubuntu: 14.x, 16.x "},{"title":"See Also​","type":1,"pageTitle":"Monitoring Linux OS","url":"docs/Integrations/os/linux/linux_os#see-also","content":"LSOF PSUTIL NETSTAT Custom plugins using StatsD Prometheus Integration "},{"title":"MySQL in Kubernetes","type":0,"sectionRef":"#","url":"docs/Integrations/mysql/mysql_kubernetes","content":"","keywords":""},{"title":"MySQL monitoring with sfKubeAgent​","type":1,"pageTitle":"MySQL in Kubernetes","url":"docs/Integrations/mysql/mysql_kubernetes#mysql-monitoring-with-sfkubeagent","content":"In this approach, sfKubeAgent is run as a side-car inside MySQL pod. The example below shows the config-map for sfKubeAgent container, config-map for MySQL container and pod yaml. "},{"title":"Config map for MySQL​","type":1,"pageTitle":"MySQL in Kubernetes","url":"docs/Integrations/mysql/mysql_kubernetes#config-map-for-mysql","content":"apiVersion: v1 kind: ConfigMap metadata: name: mysql-configmap data: mysql.cnf: | [mysqld] show_compatibility_56 = On query_cache_type = 1 query_cache_size = 16M query_cache_limit = 1M general_log_file = /var/log/mysql/mysql.log general_log = 1 slow_query_log = 1 slow_query_log_file = /var/log/mysql/mysql-slow.log Copy "},{"title":"Config map for MySQL sfKubeAgent​","type":1,"pageTitle":"MySQL in Kubernetes","url":"docs/Integrations/mysql/mysql_kubernetes#config-map-formysqlsfkubeagent","content":"apiVersion: v1 kind: ConfigMap metadata: name: mysql-sfkubeagent-configmap data: config.yaml: |- key: &lt;enter profile key&gt; metrics: plugins: - name: mysql enabled: true interval: 30 config: host: &quot;127.0.0.1&quot; password: &quot;&lt;enter password&gt;&quot; user: &quot;root&quot; documentsTypes: - databaseDetails - serverDetails - tableDetails logging: plugins: - name: mysql-general enabled: true config: log_path: &quot;/var/log/mysql/mysql.log, /var/log/mysql.log, /var/log/mysqld.log&quot; - name: mysql-error enabled: true config: log_level: - error - warn - note log_path: &quot;/var/log/mysql/error.log, /var/log/mysql/mysql-error.log, /var/log/mysqld.err&quot; - name: mysql-slowquery enabled: true config: log_path: &quot;/var/lib/mysql/ip-*slow.log, /var/log/mysql/mysql-slow.log&quot; Copy "},{"title":"MySQL Pod YAML​","type":1,"pageTitle":"MySQL in Kubernetes","url":"docs/Integrations/mysql/mysql_kubernetes#mysql-pod-yaml","content":"kind: Pod apiVersion: v1 metadata: name: mysql-pod labels: snappyflow/appname: &lt;app_name&gt; snappyflow/projectname: &lt;project_name&gt; spec: containers: - name: mysql-container image: &quot;mysql:5.7.14&quot; imagePullPolicy: IfNotPresent ports: - name: tcp containerPort: 3306 env: - name: MYSQL_ROOT_PASSWORD value: &lt;enter password&gt; - name: MYSQL_ROOT_USER value: root volumeMounts: - name: varlog mountPath: /var/log/mysql - name: configmap-mysql mountPath: /etc/mysql/conf.d # Snappyflow's sfkubeagent container - name: sfagent-container image: snappyflowml/sfagent:latest imagePullPolicy: Always command: - /app/sfagent - -enable-console-log env: - name: APP_NAME value: &lt;app_name&gt; - name: PROJECT_NAME value: &lt;project_name&gt; volumeMounts: - name: configmap-sfkubeagent-mysql mountPath: /opt/sfagent/config.yaml subPath: config.yaml - name: varlog mountPath: /var/log/mysql volumes: - name: configmap-mysql configMap: name: mysql-configmap - name: configmap-sfkubeagent-mysql configMap: name: mysql-sfkubeagent-configmap - name: varlog emptyDir: {} Copy "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"MySQL in Kubernetes","url":"docs/Integrations/mysql/mysql_kubernetes#viewing-data-and-dashboards","content":"Metric data generated by plugin can be viewed in browse data page inside the respective application under metrics section with plugin=mysql and documentType= serverDetails, databaseDetails, tableDetails. Data from slow query logs can be found in metrics section with plugin=mysql-slowquery and documentType=SlowQueryLogsDashboard for this data can be instantiated by Importing dashboard template MySQL to the application dashboard "},{"title":"MySQL monitoring with Prometheus​","type":1,"pageTitle":"MySQL in Kubernetes","url":"docs/Integrations/mysql/mysql_kubernetes#mysql-monitoring-with-prometheus","content":"Refer to Prometheus Exporter overview to understand how SnappyFlow monitors using Prometheus exporters. "},{"title":"Pre-requisites​","type":1,"pageTitle":"MySQL in Kubernetes","url":"docs/Integrations/mysql/mysql_kubernetes#pre-requisites","content":"Prometheus exporter is deployed as a side-car in the application container and the exporter port is accessible to sfPod "},{"title":"Configurations​","type":1,"pageTitle":"MySQL in Kubernetes","url":"docs/Integrations/mysql/mysql_kubernetes#configurations","content":""},{"title":"MySQL Service YAML​","type":1,"pageTitle":"MySQL in Kubernetes","url":"docs/Integrations/mysql/mysql_kubernetes#mysql-service-yaml","content":"apiVersion: v1 kind: Service metadata: labels: snappyflow/appname: &lt;app_name&gt; snappyflow/projectname: &lt;project_name&gt; snappyflow/component: mysql name: mysql-prom-service spec: ports: - name: mysql port: 3306 protocol: TCP targetPort: mysql - name: mysql-exporter port: 9104 protocol: TCP targetPort: mysql-exporter selector: app: &lt;app_name&gt; sessionAffinity: None type: ClusterIP Copy "},{"title":"MySQL Pod YAML​","type":1,"pageTitle":"MySQL in Kubernetes","url":"docs/Integrations/mysql/mysql_kubernetes#mysql-pod-yaml-1","content":"kind: Pod apiVersion: v1 metadata: name: mysql-prometheus-pod labels: snappyflow/appname: &lt;app-name&gt; snappyflow/projectname: &lt;project-name&gt; snappyflow/component: mysql spec: containers: - name: mysql-container image: &quot;mysql:5.7.14&quot; imagePullPolicy: IfNotPresent ports: - name: tcp containerPort: 3306 env: - name: MYSQL_ROOT_PASSWORD value: &lt;password&gt; - name: MYSQL_ROOT_USER value: root volumeMounts: - name: varlog mountPath: /var/log/mysql - name: configmap-mysql mountPath: /etc/mysql/conf.d # Prometheus exporter - name: mysql-exporter image: prom/mysqld-exporter:v0.10.0 imagePullPolicy: Always ports: - name: mysql-exporter containerPort: 9104 command: - sh - -c - DATA_SOURCE_NAME=&quot;root:$MYSQL_ROOT_PASSWORD@(localhost:3306)/&quot; /bin/mysqld_exporter env: - name: MYSQL_ROOT_PASSWORD value: &lt;password&gt; volumes: - name: configmap-mysql configMap: name: mysql-configmap - name: varlog emptyDir: {} Copy "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"MySQL in Kubernetes","url":"docs/Integrations/mysql/mysql_kubernetes#viewing-data-and-dashboards-1","content":"Data generated by plugin can be viewed in browse data page inside the respective application under plugin=kube-prom-mysql and documentType= serverDetails, tabledetails . Data from slow query logs can be found in metrics section with plugin=mysql-slowquery and documentType=mysqlSlowQueryLogsDashboard for this data can be instantiated by Importing dashboard template MySQL_Kube_Prom to the application dashboard "},{"title":"PSUtil Monitoring","type":0,"sectionRef":"#","url":"docs/Integrations/os/linux/psutil","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"PSUtil Monitoring","url":"docs/Integrations/os/linux/psutil#overview","content":"PSUtil Metric plugin is an agent-based plugin that collects below data for each process running on the machine Process IDUsernameCPU (%)CPU timeMemory (%)Resident Memory (%)Virtual Memory (%)Elapsed timeProcessorState Code "},{"title":"Agent Configuration​","type":1,"pageTitle":"PSUtil Monitoring","url":"docs/Integrations/os/linux/psutil#agent-configuration","content":"Refer to sfAgent section for steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: &lt;profile_key&gt; tags: Name: &lt;name&gt; appName: &lt;app_name&gt; projectName: &lt;project_name&gt; metrics: plugins: -name: psutil enabled: true interval: 60 config: numprocess: 10 sortby: pcpu Copy "},{"title":"Configuring parameters​","type":1,"pageTitle":"PSUtil Monitoring","url":"docs/Integrations/os/linux/psutil#configuring-parameters","content":"note You can configure plugin to collect top 10 process which used High CPU (%) as in sample configuration. numprocess: Number of processes for which metrics have to be collected. Set numprocess to 0 or leave it empty to get metrics for all processes. Default is 15. sortby: Sorts the process by sortby field and selects the top N processes. Default value is pcpu. E.g. you can collect top 10 processes by CPU Util if the sortby field is pcpu. Possible values are, uname - Username pid - ProcessId psr - Processor pcpu - CPUPercent cputime - CPUTime pmem - Memory Percent rsz - Resident Memory vsz - Virtual Memory etime - Elapsed Time s - State code Copy "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"PSUtil Monitoring","url":"docs/Integrations/os/linux/psutil#viewing-data-and-dashboards","content":"Data collected by plugin can be viewed in SnappyFlow’s browse data section under metrics plugin= psutildocumentType = processStats Dashboard of psutil data can be rendered usingTemplate= PSUTIL "},{"title":"Test Matrix​","type":1,"pageTitle":"PSUtil Monitoring","url":"docs/Integrations/os/linux/psutil#test-matrix","content":"Centos: 7.x RHEL: 7.x Ubuntu: 14.x, 16.x "},{"title":"See Also​","type":1,"pageTitle":"PSUtil Monitoring","url":"docs/Integrations/os/linux/psutil#see-also","content":"Linux monitoringLSOFNETSTATCustom plugins using StatsDPrometheus Integration "},{"title":"sfAgent installation on Windows","type":0,"sectionRef":"#","url":"docs/Integrations/os/windows/sfagent_windows","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"sfAgent installation on Windows","url":"docs/Integrations/os/windows/sfagent_windows#overview","content":"Monitoring of Windows based application requires installation of a lightweight agent, sfAgent on Windows. sfAgent provides following features: Monitoring of various services based on specified configurationsLog parsing and collectionTrace Java, Python and Golang applications (check out sfTracing for details) "},{"title":"Supported Platforms​","type":1,"pageTitle":"sfAgent installation on Windows","url":"docs/Integrations/os/windows/sfagent_windows#supported-platforms","content":"Windows Server 2012Windows Server 2016Windows Server 2019 "},{"title":"Install sfAgent on Windows​","type":1,"pageTitle":"sfAgent installation on Windows","url":"docs/Integrations/os/windows/sfagent_windows#install-sfagent-on-windows","content":"Download the sfAgent executable from the link below Dowload sfAgent Run sfAgent.exe executable with Administrator privileges and complete the installation "},{"title":"Configure sfAgent on Windows​","type":1,"pageTitle":"sfAgent installation on Windows","url":"docs/Integrations/os/windows/sfagent_windows#configure-sfagent-on-windows","content":"Navigate to sfAgent installation location C:\\Program Files (x86)\\sfAgentOpen file sample.yamlAdd Key and edit configuration for metrics and loggerSave the file and rename sample.yaml to config.yaml "},{"title":"Prerequisites​","type":1,"pageTitle":"sfAgent installation on Windows","url":"docs/Integrations/os/windows/sfagent_windows#prerequisites","content":"Powershell.exe must be available in %PATH environment variableFor winjvm plugin, java should be installed and java path should be set in %PATH environment variable "},{"title":"Run sfAgent service​","type":1,"pageTitle":"sfAgent installation on Windows","url":"docs/Integrations/os/windows/sfagent_windows#run-sfagent-service","content":"Open task manager and service tab.Search for service sfAgent and right click on it and click start to start service.To stop, right click on running service and click stop. "},{"title":"Standard Plugins and Log Parsers​","type":1,"pageTitle":"sfAgent installation on Windows","url":"docs/Integrations/os/windows/sfagent_windows#standard-plugins-and-log-parsers","content":"sfAgent for Windows includes plugins and log parsers for a number of standard applications and operating system utilities. (documentation coming soon!) Category\tServicesWindows[Windows Server 2012 and above]\tCPU and RAM static and dynamic parameters, Windows WinPSUtil Web Tier\tIIS Server (Server Monitoring, Access &amp; Error Logs) App Tier\tWinJVM, Apache Tomcat Database and Dataflow Elements\tMySQL, MS-SQL "},{"title":"Plugin Configuration","type":0,"sectionRef":"#","url":"docs/Integrations/plugin_config","content":"","keywords":""},{"title":"Further reading​","type":1,"pageTitle":"Plugin Configuration","url":"docs/Integrations/plugin_config#further-reading","content":"Refer to Dashboard to learn more about creating dashboards on sfAPM. "},{"title":"LSOF (List of Open Files)","type":0,"sectionRef":"#","url":"docs/Integrations/os/linux/lsof","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"LSOF (List of Open Files)","url":"docs/Integrations/os/linux/lsof#overview","content":"LSOF (list open files) Metric plugin collects data for number of files opened by a process of different file descriptor types. "},{"title":"Prerequisite - Install lsof Command​","type":1,"pageTitle":"LSOF (List of Open Files)","url":"docs/Integrations/os/linux/lsof#prerequisite----install-lsof-command","content":"lsof command needs to be installed before running this plugin. To install lsof in Centos / RHEL sudo yum install lsof Copy To install lsof in Ubuntu sudo apt-get install lsof Copy To verify installation, run the below command. lsof -v Copy Expected output: lsof version information: revision: 4.87 latest revision: ftp://lsof.itap.purdue.edu/pub/tools/unix/lsof/ latest FAQ: ftp://lsof.itap.purdue.edu/pub/tools/unix/lsof/FAQ latest man page: ftp://lsof.itap.purdue.edu/pub/tools/unix/lsof/lsof_man constructed: Tue Oct 30 16:28:19 UTC 2018 constructed by and on: mockbuild@x86-01.bsys.centos.org compiler: cc compiler version: 4.8.5 20150623 (Red Hat 4.8.5-36) (GCC) compiler flags: -DLINUXV=310000 -DGLIBCV=217 -DHASIPv6 -DHASSELINUX -D_FILE_OFFSET_BITS=64 -D_LARGEFILE64_SOURCE -DHAS_STRFTIME -DLSOF_VSTR=&quot;3.10.0&quot; -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic loader flags: -L./lib -llsof -lselinux system info: Linux x86-01.bsys.centos.org 3.10.0-693.17.1.el7.x86_64 #1 SMP Thu Jan 25 20:13:58 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux Copy note version may vary depending upon the Linux distribution. "},{"title":"Agent Configuration​","type":1,"pageTitle":"LSOF (List of Open Files)","url":"docs/Integrations/os/linux/lsof#agent-configuration","content":"Refer to sfAgent section for steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: &lt;profile_key&gt; tags: Name: &lt;name&gt; appName: &lt;app_name&gt; projectName: &lt;project_name&gt; metrics: plugins: - name: lsof enabled: true interval: 600 config: completeStats: false numProcess: 5 sortFilter: DIR Copy "},{"title":"Configuration Details​","type":1,"pageTitle":"LSOF (List of Open Files)","url":"docs/Integrations/os/linux/lsof#configuration-details","content":"LSOF plugin runs in two different modes: summary and complete stats. In summary mode, plugin returns only the count of files open of each file descriptor type (like DIR, CHR, REG etc.) at an aggregate level. Set completeStats: false for summary mode In completeStats mode, plugin returns the entire list of open files in the machine, process wise along with the process id. Set completeStats: true for this mode. Since the list of all opened files can be huge in number, plugin is by default configured in summary mode  Other configuration parameters include: numProcess: Number of top N processes with maximum number of files opened. For example, if numProcess is set to 5, it returns top 5 process stats with maximum number of files opened. Set numProcess to 0 to get all process details. Default value is 10. sortFilter: Selection of top N processes only for a particular file descriptor type. Following are the options available: none, CHR, DIR, REG, FIFO, IP, netlink, socket, a_inode. Default value is none. For example, to get top 10 processes with maximum directories opened, numProcess should be set to 10 and sortFilter as DIR. Set sort filter to none if no sorting is required.  note All the traffic related (IPv4, IPv6) file types are combined as IPv4/6 type and unix sockets into socket type "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"LSOF (List of Open Files)","url":"docs/Integrations/os/linux/lsof#viewing-data-and-dashboards","content":"Data collected by plugins can be viewed in SnappyFlow’s browse data section under metrics section plugin: lsof documentType: lsofSummary, lsofstats Dashboard template: LSOF  "},{"title":"Test Matrix​","type":1,"pageTitle":"LSOF (List of Open Files)","url":"docs/Integrations/os/linux/lsof#test-matrix","content":"Centos: 7.x RHEL: 7.x Ubuntu: 14.x, 16.x "},{"title":"See Also​","type":1,"pageTitle":"LSOF (List of Open Files)","url":"docs/Integrations/os/linux/lsof#see-also","content":"Linux monitoringPSUTILNETSTATCustom plugins using StatsDPrometheus Integration "},{"title":"sfAgent Installation on Linux","type":0,"sectionRef":"#","url":"docs/Integrations/os/linux/sfagent_linux","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"sfAgent Installation on Linux","url":"docs/Integrations/os/linux/sfagent_linux#overview","content":"sfAgent is a lightweight agent installed on VMs to collect metrics, logs and tracing data. "},{"title":"Tested on​","type":1,"pageTitle":"sfAgent Installation on Linux","url":"docs/Integrations/os/linux/sfagent_linux#tested-on","content":"Ubuntu 16 LTS, 18 LTS, 20 LTS Centos 7, 8 RHEL 7, 8.5 "},{"title":"Pre-requisites​","type":1,"pageTitle":"sfAgent Installation on Linux","url":"docs/Integrations/os/linux/sfagent_linux#pre-requisites","content":"sfAgent requires certain pre-requisites for monitoring. Common pre-requisites are mentioned below. Further, all pre-requisites and configurations needed for monitoring a specific application are mentioned under Integrations section. For Linux OS monitoring, install iostat sudo apt-get install sysstat Copy or sudo yum install sysstat Copy For JVM monitoring, install java headless service for jcmd &amp; jmap command sudo apt-get install –y openjdk-12-jdk-headless Copy or sudo yum -y install java-1.8.0-openjdk-devel-1.8.0* Copy "},{"title":"Installation​","type":1,"pageTitle":"sfAgent Installation on Linux","url":"docs/Integrations/os/linux/sfagent_linux#installation","content":"Run the following commands to install sfAgent on VMs: wget https://raw.githubusercontent.com/snappyflow/apm-agent/master/install.sh -O install.sh chmod +x install.sh sudo ./install.sh Copy info sfAgent executes commands such as iostat or jcmd to fetch metrics. If the utilities are not included in the PATH variable or not installed in the default location, use -p or --include-paths to add PATH in sfAgent. Example: ./install.sh -p /opt/jdk1.8.0_211/bin/ Copy To install sfAgent on multiple end-points using Ansible playbook, refer the following script at https://github.com/snappyflow/ansible-role-apm-agent "},{"title":"Configuration​","type":1,"pageTitle":"sfAgent Installation on Linux","url":"docs/Integrations/os/linux/sfagent_linux#configuration","content":"sfAgent can be either configured manually or automatically. In an automatic configuration step, sfAgent discovers services running in a VM and automatically generates a default configuration for monitoring the discovered services. User can further modify the configurations as needed. Check Configuration format for more details. Detailed configuration for a specific application types are present in Integrations section. Follow the steps below for automatic discovery &amp; configuration  Run following commands to discover services and generate config: sudo su cd /opt/sfagent ./sfagent -generate-config cp config-generated.yaml config.yaml Copy Add the profile key and SnappyFlow tags in the configuration file (config.yaml). Copy profile key from SnappyFlow and update key: Set values for Name:, appName:, projectName: under tags: section. Name: is the host name and the projectName: and appName: are the project name and application name used on the Snappyflow portal. Verify configuration and restart sfAgent ./sfagent -check-config service sfagent restart Copy note sfAgent log file is present in the path /var/log/sfagent/sfagent.log. "},{"title":"Upgrade sfAgent on Linux​","type":1,"pageTitle":"sfAgent Installation on Linux","url":"docs/Integrations/os/linux/sfagent_linux#upgrade-sfagent-on-linux","content":"Run following commands to upgrade sfAgent: wget https://raw.githubusercontent.com/snappyflow/apm-agent/master/install.sh -O install.sh chmod +x install.sh sudo ./install.sh --upgrade Copy "},{"title":"sfAgent Configuration Format​","type":1,"pageTitle":"sfAgent Installation on Linux","url":"docs/Integrations/os/linux/sfagent_linux#sfagent-configuration-format","content":"sfAgent is configured through its config.yaml file. There are sections for metrics and logs where appropriate plugins with their configurations have to added to these sections. sfAgent config is expected in the following format: key: &lt;add profile key here&gt; generate_name: true tags: Name: &lt;add name tag&gt; appName: &lt;add application name tag&gt; projectName: &lt;add project name tag&gt; metrics: plugins: - name: &lt;metric-plugin&gt; enabled: true interval: &lt;time in secs&gt; logging: plugins: - name: &lt;logger-plugin&gt; enabled: true config: log_level: - list - of - log - levels log_path: &lt;comma separated log paths&gt; Copy info Uninstallation​ Run the following commands to uninstall sfAgent on VMs: wget https://raw.githubusercontent.com/snappyflow/apm-agent/master/uninstall.sh -O uninstall.sh chmod +x uninstall.sh sudo ./uninstall.sh Copy "},{"title":"SnappyFlow Integrations","type":0,"sectionRef":"#","url":"docs/Integrations/overview","content":"SnappyFlow Integrations SnappyFlow supports a wide range of built in integrations to help you get started quickly. Linux Postgres Java Go MongoDB Oracle ASH HAProxy ActiveMQ Kafka Blaze Meter AzureDB Azure Blob AWS RDS Zookeper Kubernetes Clickhouse MySQL MSSQL Nginx Tomcat Pagerduty Apache Elastic Load Balancer Redis RabbitMQ TripWire Slack Windows AWS Lambda StatsD IIS Server","keywords":""},{"title":"AWS","type":0,"sectionRef":"#","url":"docs/Integrations/plugin/aws","content":"","keywords":""},{"title":"Description​","type":1,"pageTitle":"AWS","url":"docs/Integrations/plugin/aws#description","content":"AWS Metric plugin provide information related to billing and inventory for the account and it is displayed in sfAPM dashboard. "},{"title":"Prerequisites​","type":1,"pageTitle":"AWS","url":"docs/Integrations/plugin/aws#prerequisites","content":""},{"title":"Enabling AWS billing reports​","type":1,"pageTitle":"AWS","url":"docs/Integrations/plugin/aws#enabling-aws-billing-reports","content":"AWS Billing reports can be generated following simple steps given at AWS official documentation. It involves two steps: Setting up an Amazon S3 bucket for Cost and Usage Reports . (related aws documentation)Creating Cost and Usage Reports . (related aws documentation) "},{"title":"Configuration Settings​","type":1,"pageTitle":"AWS","url":"docs/Integrations/plugin/aws#configuration-settings","content":"Billing and Inventory plugins need to be added through Plugins tab. Parameters: 1) S3 bucket name where the billing csv is placed. 2) Folder path of the bucket where the billing csv is placed. 3) Interval Note: AWS billing plugin provides billing information per day. So interval must be 86400. "},{"title":"Documents​","type":1,"pageTitle":"AWS","url":"docs/Integrations/plugin/aws#documents","content":"All Billing and Inventory metrics are collected and displayed in CloudMonitoring_AWS dashboard. "},{"title":"Further Reading​","type":1,"pageTitle":"AWS","url":"docs/Integrations/plugin/aws#further-reading","content":"S3 , RDS and ELB for other AWS service related monitoring. For help with plugins, please reach out to support@snappyflow.io. "},{"title":"AzureLog","type":0,"sectionRef":"#","url":"docs/Integrations/plugin/azurelog","content":"","keywords":""},{"title":"Description​","type":1,"pageTitle":"AzureLog","url":"docs/Integrations/plugin/azurelog#description","content":"Plugin relays the log events stored in event hub to snappyflow APM "},{"title":"Prerequisites​","type":1,"pageTitle":"AzureLog","url":"docs/Integrations/plugin/azurelog#prerequisites","content":"Plugin supports two modes of basic authorization Managed Service Identity (MSI) Register sfpoller with Managed identity enabled. Refer Enable Managed Identity Assign Azure Event Hubs Data receiver role to the sfpoller machine using following steps, In azure portal, Open Access control (IAM) pane shown in sfpoller VM inventory page.Click the Role assignments tab to view the role assignments at this scopeClick Add &gt; Add role assignment. If you don't have permissions to assign roles, the Add role assignment option will be disabled.Configure role option to Azure Event Hubs Data receiver, Assign access to option to Virtual Machine and choose respective subscription and the vm name For more detailed information please refer Assign Role to resource Service Principal Token Token based Authentication is required if MSI is not available Steps to obtain Service Principal Token, Register an Application in Azure Active Directory and create a service principal Create a secret by which application uses to prove its identity.In registered application, select API permissions &gt; Add a permission &gt; APIs my organization uses &gt; Microsoft.EventHubs &gt; Add permissions Collect ClientID, ClientSecret, TenantId, SubscriptionId from registered app For more detailed information please refer Azure Docs . EventHub Access Claim Configure any of the Shared access policies across both EventHub and its respective EventHubNamespace (available in settings section of respective resources in azure portal) should have Mange claim "},{"title":"Plugin Configuration​","type":1,"pageTitle":"AzureLog","url":"docs/Integrations/plugin/azurelog#plugin--configuration","content":"Plugin to be configured through sfpoller Select Manage &gt; Cloud Account &gt; Add &gt; Azure (AccountType). If Service Principal Token in Prerequisites section is adopted, add the client credentials generated , else configure only Subscription Id, Region, Resource Group where the eventhub belongs Create desired Project and Application. Select Add Endpoint &gt; Azure &gt; Account Name &gt; Azure-LogEvents. Configure a custom instance name, desired eventhub, and its namespace. Log Specification: If all the logs updated in eventhub needs to be collected, leave the field empty. Unique identifier for each log kind is its category. Refer Available Categories. Each log category has its own schema of metrics Schema Per Category. A log Specification includes a category and following set of defined options which applies to that category,Resources: Comma Separated azure resources those are intended to be monitoredTarget: Specify whether data to be rendered in metric or log dashboard. Available values to be set, metric and logMessage Field: The field will be identified as desired log message to be shown in snappyflow APM (applicable only if Target is set to log) For help with plugins, please reach out to support@snappyflow.io. "},{"title":"AzureDB","type":0,"sectionRef":"#","url":"docs/Integrations/plugin/azuredb","content":"","keywords":""},{"title":"Description​","type":1,"pageTitle":"AzureDB","url":"docs/Integrations/plugin/azuredb#description","content":"Azuredb Metric plugin collects metrics related to relational database services like Azure SQL, Azure database for MySQL and Azure database for PostgreSQL using insights data from Azure Monitor. "},{"title":"Prerequisites​","type":1,"pageTitle":"AzureDB","url":"docs/Integrations/plugin/azuredb#prerequisites","content":"Plugin supports two modes of authentication and authorization Managed Service Identity (MSI) Registered APM controller with Managed identity enabled will auto-authenticate and authorize the plugin client without any credentials being passed as input parameter. Service Principal Token Token based Authentication is required if MSI is not available Steps to obtain Service Principal Token, Register an Application in Azure Active Directory and create a service principal Assign a role as Application Insights Component Contributor which has read access to Insight component.Create a secret by which application uses to prove its identity. Collect ClientID, ClientSecret, TenantId, SubscriptionId from registered app For more detailed information please refer Azure Docs . "},{"title":"Configuration Settings​","type":1,"pageTitle":"AzureDB","url":"docs/Integrations/plugin/azuredb#configuration-settings","content":"Plugin can be set to monitor intended service by setting respective configuration. It needs to be configured to work along with either of the snappyflow database plugins - MySQL,PostgreSQL, SQLDB ,based on endpoint service type. Inbound traffic setting should be set to allow traffic from controller instance. General Parameters Resource GroupRegionDB Server "},{"title":"Documents​","type":1,"pageTitle":"AzureDB","url":"docs/Integrations/plugin/azuredb#documents","content":"Metrics are collected and relevant tags are added to the metric document Dashboards and alerts templated are chosen based on monitored database service type For Azure MySQL relational service, Azure MySQL dashboard and Azure MySQL alert template needs to be appliedFor Azure SQL service, Azure SQL dashboard and Azure SQL alert template needs to be appliedFor Azure PostgreSQL service, Azure PostgreSQL dashboard and Azure PostgreSQL alert template needs to be applied For help with plugins, please reach out to support@snappyflow.io. "},{"title":"Blob","type":0,"sectionRef":"#","url":"docs/Integrations/plugin/blob","content":"","keywords":""},{"title":"Description​","type":1,"pageTitle":"Blob","url":"docs/Integrations/plugin/blob#description","content":"Azure blob storage is to store large amount of unstructured data on data storage platform. Plugin collects insights data from Azure Monitor to monitor the performance. "},{"title":"Prerequisites​","type":1,"pageTitle":"Blob","url":"docs/Integrations/plugin/blob#prerequisites","content":"Plugin supports two modes of authentication and authorization Managed Service Identity (MSI) Registered APM controller with Managed identity enabled will auto-authenticate and authorize the plugin client without any credentials being passed as input parameter. Service Principal Token Token based Authentication is required if MSI is not available Steps to obtain Service Principal Token, Register an Application in Azure Active Directory and create a service principal Assign a role as Application Insights Component Contributor which has read access to Insight component.Create a secret by which application uses to prove its identity. Collect ClientID, ClientSecret, TenantId, SubscriptionId from registered app For more detailed information please refer Azure Docs . "},{"title":"Configuration Settings​","type":1,"pageTitle":"Blob","url":"docs/Integrations/plugin/blob#configuration-settings","content":"Plugin need to be added through Plugins tab. General Parameters Resource GroupRegionInstance Name ( Storage account name) "},{"title":"Documents​","type":1,"pageTitle":"Blob","url":"docs/Integrations/plugin/blob#documents","content":"All Blob Storage metrics are collected and tagged based on their document type to be displayed in Blob dashboard. Predefined alert rules can be imported from Blob template. For help with plugins, please reach out to support@snappyflow.io. "},{"title":"Configuration Update Feature (Linux & Windows)","type":0,"sectionRef":"#","url":"docs/Integrations/plugin/configUpdate","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Configuration Update Feature (Linux & Windows)","url":"docs/Integrations/plugin/configUpdate#overview","content":"Agent Configuration update feature is used to update the configuration in multiple instances at a time without login into each instance. From UI, users can update the config file. "},{"title":"Prerequisite​","type":1,"pageTitle":"Configuration Update Feature (Linux & Windows)","url":"docs/Integrations/plugin/configUpdate#prerequisite","content":"sfAgent version should be more than v.0.11.21 in LinuxsfAgent version should be more than v.0.0.170 in Windows "},{"title":"To upgrade the Linux agent to the latest version​","type":1,"pageTitle":"Configuration Update Feature (Linux & Windows)","url":"docs/Integrations/plugin/configUpdate#to-upgrade-the-linux-agent-to-the-latest-version","content":" wget https://raw.githubusercontent.com/snappyflow/apm-agent/master/install.sh -O install.sh chmod +x install.sh sudo ./install.sh --upgrade Copy "},{"title":"To upgrade the windows agent to the latest version​","type":1,"pageTitle":"Configuration Update Feature (Linux & Windows)","url":"docs/Integrations/plugin/configUpdate#to-upgrade-the-windows-agent-to-the-latest-version","content":"Refer the following linkhttps://docs.snappyflow.io/docs/Integrations/os/windows/sfagent_windows "},{"title":"Using the Configuration update feature user can update the below files​","type":1,"pageTitle":"Configuration Update Feature (Linux & Windows)","url":"docs/Integrations/plugin/configUpdate#using-the-configuration-update-feature-user-can-update-the-below-files","content":"config.yaml file custom_logging_plugins.yaml custom_scripts.lua "},{"title":"Below are the options available to update the above files​","type":1,"pageTitle":"Configuration Update Feature (Linux & Windows)","url":"docs/Integrations/plugin/configUpdate#below-are-the-options-available-to-update-the-above-files","content":"Plugin config update To add/remove or update the plugins and logs configuration Profile key update To update the profile key Tag Update To update the tags (Name, appName, projectName) In Tag update again contain three options. Replace all tags This option will replace all instance tags with applied bundle config tags (Name, appName, tagName are mandatory tags). Insert/Update tags If a tag is already present in instance config and in bundle config, the bundle config file overrides the existing tags. If a tag is present in bundle config and not present in instance config, the tag which is present in bundle config will be added as a new tag. If a tag is present in instance config and not present in bundle config, the tag is retained from instance config. If the user wants to update only appName and projectName, the user must remove the Name tag from the config file and upload it. The name field will be retained from the existing config file. Users can also add custom tags. Delete Tags: This will remove the custom tags from the configuration file, but the tags that which user wants to delete should be present in both bundle config and instance config. "},{"title":"Steps to apply agent configuration bundle​","type":1,"pageTitle":"Configuration Update Feature (Linux & Windows)","url":"docs/Integrations/plugin/configUpdate#steps-to-apply-agent-configuration-bundle","content":"Collect Agent configuration bundle from UI. Navigate to the inventory page. Click on the configuration tab Click on collect agent configuration and it will show the downloaded collection. Download the configuration from UI by clicking the download icon under the action tab To edit the download bundle, follow the steps below. It is preferred to use third-party software 7zip to edit the bundle. Open the tar file using 7zip software and click on tar file. It will show the list of files available and edit the file that you want and save. After saving close the editor, it will show the prompt dialog box to update, and click on ok. To Add the downloaded configuration file, follow the below steps. Click on three dots under the action tab on the Application Page. Select agent configuration option Click on add new bundle option. Upload the bundle which you downloaded from the inventory page and provide the name and description. Click on apply. To Apply the added bundle Config bundle apply can be done in two ways a. From inventory page Using this user can apply a bundle for a single instance only at a time. Steps: Navigate to the inventory page Click on the configuration tab and select apply configuration. The user must select what changes are needed to apply to the existing config. After applying the bundle if it is a success status shows the applied, its fail status shows Error along with reason. b. From agent configuration dialogue box. Using this user can apply a bundle for multiple instances under a single application. Steps: Click on three dots under the action tab on the Application page. Click on apply. Users must select what changes need to apply to the existing config. After applying the bundle if it is a success status shows the applied, its fail status shows Error along with reason.  "},{"title":"Troubleshoot Documentation​","type":1,"pageTitle":"Configuration Update Feature (Linux & Windows)","url":"docs/Integrations/plugin/configUpdate#troubleshoot-documentation","content":"Error Message/Status\tWhat does it mean? When this occurs\tWhat user actions take nextMapping values are not allowed in this context\tMight be the wrong indentation added in the config file\tCheck the config file for proper indentation. Execution time out try again\tIt might be the command server not sending the response to APM.\tCheck whether the agent is running or not Invalid config file, expected Tag(s) are not provided: Name, appName, projectName\tOccurred because any one of the tags is missing in the tag section in the config file.\tTo update the profile key "},{"title":"Blaze Meter","type":0,"sectionRef":"#","url":"docs/Integrations/plugin/blazemeter","content":"","keywords":""},{"title":"Description​","type":1,"pageTitle":"Blaze Meter","url":"docs/Integrations/plugin/blazemeter#description","content":"Plugin collects Test Reports from the BlazeMeter Account. BlazeMeter plugin captures the Test Results of 3 Types of BlazeMeter Tests : Performance Test : It captures the summary statistics for the test, requests used in the test, time-series metrics for each of the requests used in the test workflow for the whole duration of tests and the test response metrics (in case if there are any errors).API Functional Test : It captures the summary statistics for each of the group IDs and the full report for each of the request.GUI Functional Test : It captures the scenarios summary of the test and the full report for each of the request. BlazeMeter test results can be visualized in SnappyFlow Dashboard and test anomalies can be observed. Since SnappyFlow is also set up to capture metrics, logs and traces from the application being tested, it now becomes easy to correlate the anomalies from the test observations with the application insights. "},{"title":"Pre-requisites​","type":1,"pageTitle":"Blaze Meter","url":"docs/Integrations/plugin/blazemeter#pre-requisites","content":"User should have a BlazeMeter Account. (Click here to create a BlazeMeter Account) User should have API Key and API Secret Key for the BlazeMeter Account.(Click here to know about how to generate BlazeMeter API Key) User should run Tests from the BlazeMeter Account. User can run following types of Tests in BlazeMeter : Performance Test (Click here to know about how to create and run the Performance Test in BlazeMeter)API Functional Test (Click here to know about how to create and run the API Functional Test in BlazeMeter)GUI Functional Test (Click here to know about how to create and run the GUI Functional Test in BlazeMeter) "},{"title":"Configuration Settings​","type":1,"pageTitle":"Blaze Meter","url":"docs/Integrations/plugin/blazemeter#configuration-settings","content":"Select Testing Service Endpoint Type in Add Endpoints. Select the blazemeter plugin from the dropdown under Plugins tab and config the following parameters of the BlazeMeter Account: API KeyAPI Secret KeyWorkspace IDProject IDPolling Interval "},{"title":"Documents​","type":1,"pageTitle":"Blaze Meter","url":"docs/Integrations/plugin/blazemeter#documents","content":"All Test metrics are collected and tagged based on their document type to be displayed in BlazeMeter dashboard. List of Document Types : For Performance Test : bzTestSummary : Summary of Performance Test execution.bzTestRequests : Summary stats on each of the requests used in the test workflow. Each test workflow is comprised of number of requests to the application. For each request there is a separate request statistics.bzTestRequestMetrics : During the test execution, BlazeMeter collects response statistics at regular intervals (60 seconds) for each of the requests generated by test workflow. Using BlazeMeter APIs, time series metrics are retrieved for each of the requests. Metrics for each time interval reported by the API is extracted from the response and stored as individual metrics data in SnappyFlow.bzTestResponseMetrics : During test execution, BlazeMeter collects time-line data with return codes in the response and the number of requests resulting with each type of return code. For API Functional Test : bzTestAPIFunctionalSummaryStats : Summary statistics for each of the group IDs.bzTestAPIFunctionalFullReport : Full report for each of the requests. For GUI Functional Test : bzTestGUIFunctionalScenariosSummary : Scenarios summary of the GUI Functional Test.bzTestGUIFunctionalFullReport : Full report for each of the requests. For help with plugins, please reach out to support@snappyflow.io. "},{"title":"ELB Logs","type":0,"sectionRef":"#","url":"docs/Integrations/plugin/elblogs","content":"","keywords":""},{"title":"Description :​","type":1,"pageTitle":"ELB Logs","url":"docs/Integrations/plugin/elblogs#description-","content":"This plugin captures the logs for all 3 kind of ELB's (Application, Network, Classic) based on predefined logs format. ELBLogs plugin captures logs for Application, Network, Classic: Application ELB : Refer the Description here Network ELB : Refer the Description here Classic ELB : Refer the Description here  Above all respective logs can be visualized in SnappyFlow Dashboard and can be observed. Pre-requisites :​ User should have a API Access Key and API Secret Key for AWS Account.User should have ELB with enabled access logs in some s3 bucket.  "},{"title":"Configuration Settings​","type":1,"pageTitle":"ELB Logs","url":"docs/Integrations/plugin/elblogs#configuration-settings","content":"Select the ELBLogs plugin from the dropdown under Plugins tab and find following elb logs type as document(any one): cloudwatch-networkcloudwatch-classiccloudwatch-application "},{"title":"Documents​","type":1,"pageTitle":"ELB Logs","url":"docs/Integrations/plugin/elblogs#documents","content":"All Test metrics are collected and tagged based on their document type (application, network or classic). List of Document Types : For Application ELB : Description : type(htttp/https/ws/h2/wss) time elb client:port target:port request_processing_time target_processing_time response_processing_time elb_status_code target_status_code received_bytes sent_bytes &quot;request&quot; &quot;user_agent&quot; ssl_cipher ssl_protocol target_group_arn &quot;trace_id&quot; &quot;domain_name&quot; &quot;chosen_cert_arn&quot; matched_rule_priority request_creation_time &quot;actions_executed&quot; &quot;redirect_url&quot; &quot;redirect_url&quot; &quot;target:port_list&quot; &quot;target_status_code_list&quot; &quot;classification&quot; &quot;classification_reason&quot;Example : https 2020-10-09T04:41:37.394713Z app/apm-demo/5ce576ca9784a513 207.47.39.34:62937 172.31.4.4:80 0.000 0.006 0.000 200 200 818 333 &quot;POST https://demoinput.snappyflow.io:443/sfmetrics/topics/metric-grqqwwi7 HTTP/1.1&quot; &quot;Go-http-client/1.1&quot; ECDHE-RSA-AES128-GCM-SHA256 TLSv1.2 arn:aws:elasticloadbalancing:us-west-2:159750416379:targetgroup/apm-demo/379f70084715403e &quot;Root=1-5f7fea01-6b7fdf585250ebde5b26b2ab&quot; &quot;demoinput.snappyflow.io&quot; &quot;arn:aws:iam::159750416379:server-certificate/apm-demo&quot; 0 2020-10-09T04:41:37.388000Z &quot;forward&quot; &quot;-&quot; &quot;-&quot; &quot;172.31.4.4:80&quot; &quot;200&quot; &quot;-&quot; &quot;-&quot; For Classic ELB : Description : timestamp elb client:port backend:port request_processing_time backend_processing_time response_processing_time elb_status_code backend_status_code received_bytes sent_bytes &quot;request&quot; &quot;user_agent&quot; ssl_cipher ssl_protocolExample : 2015-05-13T23:39:43.945958Z my-loadbalancer 192.168.131.39:2817 10.0.0.1:80 0.000086 0.001048 0.001337 200 200 0 57 &quot;GET https://www.example.com:443/ HTTP/1.1&quot; &quot;curl/7.38.0&quot; DHE-RSA-AES128-SHA TLSv1.2 For Network ELB : Description : type(tls) version time elb listener client:port destination:port connection_time(ms) tls_handshake_time received_bytes sent_bytes incoming_tls_alert chosen_cert_arn chosen_cert_serial tls_cipher tls_protocol_version tls_named_group domain_name alpn_fe_protocol alpn_be_protocol alpn_client_preference_list Example : tls 2.0 2018-12-20T02:59:40 net/my-network-loadbalancer/c6e77e28c25b2234 g3d4b5e8bb8464cd 72.21.218.154:51341 172.100.100.185:443 5 2 98 246 -arn:aws:acm:us-east-2:671290407336:certificate/2a108f19-aded-46b0-8493-c63eb1ef4a99 - ECDHE-RSA-AES128-SHA tlsv12 - my-network-loadbalancer-c6e77e28c25b2234.elb.us-east-2.amazonaws.com For help with plugins, please reach out to support@snappyflow.io. "},{"title":"Oracle ASH","type":0,"sectionRef":"#","url":"docs/Integrations/plugin/oracleASH","content":"","keywords":""},{"title":"Description​","type":1,"pageTitle":"Oracle ASH","url":"docs/Integrations/plugin/oracleASH#description","content":"OracleASH plugin fetch and parse the oracle ASH report from AWS-RDS. "},{"title":"Prerequisites​","type":1,"pageTitle":"Oracle ASH","url":"docs/Integrations/plugin/oracleASH#prerequisites","content":"Mention the RDS Database instance name while adding the RDS Endpoint in 'Instance Name' field.DB Host address must be specified in the 'IP' field of the RDS Endpoint configuration. "},{"title":"Configuration Settings​","type":1,"pageTitle":"Oracle ASH","url":"docs/Integrations/plugin/oracleASH#configuration-settings","content":"Add plugin through the Plugins tab under RDS Endpoint. Optional Parameters: ASH Report File Prefix: By default RDS generates the ASH report file name in the format &quot;ashrpt_beginTime_endTime&quot;. Specify the file name prefix, if changed. Default value is &quot;ashrpt&quot;.Service Name(SID): Oracle service name.Username: Database login username.Password: Database login password.Port: Database port.Report Duration in Minutes: Total time of the report in minutes. Example: If the Report Duration in Minutes is set to 5 minutes, then it will generate an ASH report for the last 5 mintues in each poll. This value should be lesser than the plugin interval. "},{"title":"Documents​","type":1,"pageTitle":"Oracle ASH","url":"docs/Integrations/plugin/oracleASH#documents","content":"Following sections from the ASH report are captured by snappyflow with respective document types: Top Service/ModuleTop SQL Command TypesTop Phases of ExecutionTop SQL with Top EventsTop SessionsTop Blocking Sessions. To add support for more sections, please reach out to snappyflow team. Use the dashboard template 'Oracle_ASH' for visualization. For help with plugins, please reach out to support@snappyflow.io. "},{"title":"ELB","type":0,"sectionRef":"#","url":"docs/Integrations/plugin/elb","content":"","keywords":""},{"title":"Description​","type":1,"pageTitle":"ELB","url":"docs/Integrations/plugin/elb#description","content":"Amazon Elastic Load Balancer used to automatically distribute incoming app traffic across AWS Instances which may be in different availability zones. Plugin will collect all metrics for Load balancer types that AWS provides which has Application, Classic and Network Load Balancer. "},{"title":"Prerequisites​","type":1,"pageTitle":"ELB","url":"docs/Integrations/plugin/elb#prerequisites","content":"CloudWatch Access for IAM Role​ Provide Read only access for CloudWatch to the dedicated IAM Role used for APM. You can use AWS managed polices that addresses many common use cases by providing standalone IAM policies that are created and administered by AWS. Attach this AWS policy CloudWatchReadOnlyAccess to IAM role to get read access for all CloudWatch else create the below custom policy and attach it to IAM.  { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Action&quot;: [ &quot;cloudwatch:Describe*&quot;, &quot;cloudwatch:Get*&quot;, &quot;cloudwatch:List*&quot;, &quot;logs:Get*&quot;, &quot;elasticloadbalancing:Describe*&quot; ], &quot;Effect&quot;: &quot;Allow&quot;, &quot;Resource&quot;: &quot;*&quot; } ] } Copy Note: Health check interval should be less than 300 Secs, since querying for data is 5mins interval it might report incorrect data from AWS. "},{"title":"Configuration Settings​","type":1,"pageTitle":"ELB","url":"docs/Integrations/plugin/elb#configuration-settings","content":"Select ELB Endpoint Type in Add Endpoints and add the following parameters: RegionInstance Name Select the plugin from the dropdown under Plugins tab and config the polling interval. Plugin configuration for ELB services this includes Classic, Network and Application plugin. You can enable/disable any of the plugin based on your needs and instance support. Cloudwatch-classic - Collects data for classic load balancer Cloudwatch-network - collects data for Network load balancersCloudwatch-application – collects data for Application load balancers. "},{"title":"Documents​","type":1,"pageTitle":"ELB","url":"docs/Integrations/plugin/elb#documents","content":"All CloudWatch metrics are collected and tagged based on their ELB type to get displayed in their respective dashboard template. Use ELB_Network, ELB_Application and ELB_Classic for data visualization. "},{"title":"Further Reading​","type":1,"pageTitle":"ELB","url":"docs/Integrations/plugin/elb#further-reading","content":"S3 and RDS for other AWS service related monitoring. For help with plugins, please reach out to support@snappyflow.io. "},{"title":"MS SQL","type":0,"sectionRef":"#","url":"docs/Integrations/plugin/mssql","content":"","keywords":""},{"title":"Description​","type":1,"pageTitle":"MS SQL","url":"docs/Integrations/plugin/mssql#description","content":"Plugin collects MSSQL stats in different category as serverDetails , databaseDetails and tableDetails . "},{"title":"Prerequisites​","type":1,"pageTitle":"MS SQL","url":"docs/Integrations/plugin/mssql#prerequisites","content":"MSSQL service should be running and it should be reachable from controller VM. To check it is reachable from controller VM run this command sqlcmd -S &lt;hostIP&gt; -U &lt;username&gt; -P &lt;password&gt; Copy "},{"title":"Configuration Settings​","type":1,"pageTitle":"MS SQL","url":"docs/Integrations/plugin/mssql#configuration-settings","content":"Select MSSQL Endpoint Type in Add Endpoints and add the following parameters: IP: DB Instance IP Select the plugin from the dropdown under Plugins tab and config the following parameters: Username: DB login usernamePassword: DB login passwordPort: Connecting portDocument Types: Select the required document types. Available options are serverDetails, databaseDetails, tableDetails, queryDetails.Interval: Polling interval "},{"title":"Documents​","type":1,"pageTitle":"MS SQL","url":"docs/Integrations/plugin/mssql#documents","content":"All MSSQL metrics are collected and tagged based on their document type to be displayed in MSSQL dashboard. "},{"title":"Further Reading​","type":1,"pageTitle":"MS SQL","url":"docs/Integrations/plugin/mssql#further-reading","content":"For help with plugins, please reach out to support@snappyflow.io. "},{"title":"Oracle AWR","type":0,"sectionRef":"#","url":"docs/Integrations/plugin/oracleAWR","content":"","keywords":""},{"title":"Description​","type":1,"pageTitle":"Oracle AWR","url":"docs/Integrations/plugin/oracleAWR#description","content":"OracleAWR plugin fetch and parse the oracle AWR report from AWS-RDS. "},{"title":"Prerequisites​","type":1,"pageTitle":"Oracle AWR","url":"docs/Integrations/plugin/oracleAWR#prerequisites","content":"Mention the RDS Database instance name while adding the RDS Endpoint in 'Instance Name' field.DB Host address must be specified in the 'IP' field of the RDS Endpoint configuration. "},{"title":"Configuration Settings​","type":1,"pageTitle":"Oracle AWR","url":"docs/Integrations/plugin/oracleAWR#configuration-settings","content":"Add plugin through the Plugins tab under RDS Endpoint. Optional Parameters: AWR Report File Prefix: By default RDS generates the ASH report file name in the format &quot;awrrpt_beginSnapId_endSnapId&quot;. Specify the file name prefix, if changed. Default value is &quot;awrrpt&quot;.Service Name(SID): Oracle service name.Username: Database login username.Password: Database login password.Port: Database port. "},{"title":"Documents​","type":1,"pageTitle":"Oracle AWR","url":"docs/Integrations/plugin/oracleAWR#documents","content":"Following sections from the AWR report are captured by Snappyflow with respective document types: SQL ordered by Elapsed TimeSQL ordered by CPU TimeSQL ordered by User I/O Wait TimeSQL ordered by Gets SQL ordered by ReadsSQL ordered by Executions To add support for more sections, please reach out to snappyflow team. Use the dashboard template 'Oracle_AWR' for visualization. For help with plugins, please reach out to support@snappyflow.io. "},{"title":"overview","type":0,"sectionRef":"#","url":"docs/Integrations/postgres/overview","content":"overview Postgres monitoring on SnappyFlow is available for the following platforms Instances​ Kubernetes​","keywords":""},{"title":"vCenter","type":0,"sectionRef":"#","url":"docs/Integrations/plugin/vcenter","content":"","keywords":""},{"title":"Description​","type":1,"pageTitle":"vCenter","url":"docs/Integrations/plugin/vcenter#description","content":"Vcenter Metric plugin collects metrics and stats for the entities in the vcenter. "},{"title":"Configuration Settings​","type":1,"pageTitle":"vCenter","url":"docs/Integrations/plugin/vcenter#configuration-settings","content":"Plugin configuration for vcenter needs vcenter's host, username/password and interval for monitoring. "},{"title":"Documents​","type":1,"pageTitle":"vCenter","url":"docs/Integrations/plugin/vcenter#documents","content":"All vcenter metrics are collected and displayed in vCenter dashboard. "},{"title":"Further Reading​","type":1,"pageTitle":"vCenter","url":"docs/Integrations/plugin/vcenter#further-reading","content":"S3 , RDS and ELB for other AWS service related monitoring. For help with plugins, please reach out to support@snappyflow.io. "},{"title":"RDS","type":0,"sectionRef":"#","url":"docs/Integrations/plugin/rds","content":"","keywords":""},{"title":"Description​","type":1,"pageTitle":"RDS","url":"docs/Integrations/plugin/rds#description","content":"Amazon Relational Database Service (Amazon RDS) is a managed SQL database service provided by Amazon Web Services (AWS). It is web service designed to simplify the setup, operation, and scaling of a relational database for use in applications. Plugin will get RDS related metrics from CloudWatch to monitor the performance. "},{"title":"Prerequisites​","type":1,"pageTitle":"RDS","url":"docs/Integrations/plugin/rds#prerequisites","content":"CloudWatch Access for IAM Role​ Provide Read only access for CloudWatch to the dedicated IAM Role used for APM. You can use AWS managed polices that addresses many common use cases by providing standalone IAM policies that are created and administered by AWS. Attach this AWS policy CloudWatchReadOnlyAccess to IAM role to get read access for all CloudWatch else create the below custom policy and attach it to IAM. { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Action&quot;: [ &quot;cloudwatch:Describe*&quot;, &quot;cloudwatch:Get*&quot;, &quot;cloudwatch:List*&quot;, &quot;logs:Get*&quot;, &quot;rds:Describe*&quot;, &quot;logs:Describe*&quot; ], &quot;Effect&quot;: &quot;Allow&quot;, &quot;Resource&quot;: &quot;*&quot; } ] } Copy Enable Performance Insights​ Enable Performance Insights for RDS (currently supported for MySQL and PostgreSQL) to collect insight metrics for RDS. We can enable while creating the RDS or click modify and edit . Please verify whether your RDS has support for Performance Insights Performance Insights is enabled when you choose Enable Performance Insights in the Performance Insights section . For more detailed information please refer AWS Documentation. Performance Insight Access for IAM Role​ To provide access to specific users, create the custom policy with access for Performance Insight and assign the policy to IAM user. { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: &quot;pi:*&quot;, &quot;Resource&quot;: &quot;arn:aws:pi:*:*:metrics/rds/*&quot; } ] } Copy For more information how to create custom policy please refer AWS Access. Enable and Publish Logs to CloudWatch​ Modify the parameter group for the general_log and slow_query_log to collect logs and publish the logs to CloudWatch you need to configure the log exporter for particular database then choose modify and select the log types as needed and continue, and then choose Modify DB Instance. For detailed Information how to enable and publish, please refer AWS Documentation "},{"title":"Configuration Settings​","type":1,"pageTitle":"RDS","url":"docs/Integrations/plugin/rds#configuration-settings","content":"Select RDS Endpoint Type in Add Endpoints and add the following parameters: IPRegionInstance Name Select the plugin from the dropdown under Plugins tab and config the polling interval. Plugin configuration for RDS instances includes CloudWatch, CloudWatch log and native MySQL plugin. You can enable/disable any of the plugin based on your needs and instance support. CloudWatch - Collects both CloudWatch and performance insights metricsCloudWatch log – collects RDS logs which have visualization under log sectionNative MySQL/PostgreSQL plugin – collects database and table related metrics Configuration for native MySQL/PostgreSQL plugin: Username: DB login usernamePassword: DB login passwordPort: Connecting portDocument Types: Select the required document types. Available options are serverDetails, databaseDetails, tableDetails.Interval: Polling interval "},{"title":"Documents​","type":1,"pageTitle":"RDS","url":"docs/Integrations/plugin/rds#documents","content":"All CloudWatch metrics are collected and displayed in RDS_MySQL/RDS_PostgreSQL dashboard. CloudWatch log for RDS are collected and tagged as RDSLogger type and slow query is captured and can be view under log section. "},{"title":"Further Reading​","type":1,"pageTitle":"RDS","url":"docs/Integrations/plugin/rds#further-reading","content":"S3 and ELB for other AWS service related monitoring. For help with plugins, please reach out to support@snappyflow.io. "},{"title":"Monitoring Postgres databases running on cloud services (Amazon RDS) using sfPoller","type":0,"sectionRef":"#","url":"docs/Integrations/postgres/postgres_sfpoller","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Monitoring Postgres databases running on cloud services (Amazon RDS) using sfPoller","url":"docs/Integrations/postgres/postgres_sfpoller#overview","content":"sfPoller includes all necessary plugins to connect to Public cloud APIs, Cloudwatch and Azure Monitor and enables easy monitoring of databases running on cloud services such as Amazon RDS and Azure. The video below explains the steps involved in setting up sfPoller to monitor a Postgres database running on AWS.  "},{"title":"Monitoring Redis on instances","type":0,"sectionRef":"#","url":"docs/Integrations/redis","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Monitoring Redis on instances","url":"docs/Integrations/redis#overview","content":"Redis on instances is monitored using sfAgent configured with Redisdb plugin  "},{"title":"Metrics plugin​","type":1,"pageTitle":"Monitoring Redis on instances","url":"docs/Integrations/redis#metrics-plugin","content":"Collects metric data organized in following documentType under metrics index:  keyspaceStat redisDetails redisStatredisPersistence "},{"title":"Logger plugin​","type":1,"pageTitle":"Monitoring Redis on instances","url":"docs/Integrations/redis#logger-plugin","content":"collects general logs and slow logs. General logs are sent to log index whereas slow queries are sent to metrics index under documentType:redisSlowLogs  "},{"title":"Pre-requisites ​","type":1,"pageTitle":"Monitoring Redis on instances","url":"docs/Integrations/redis#pre-requisites","content":""},{"title":"Enable Slow Logs  ​","type":1,"pageTitle":"Monitoring Redis on instances","url":"docs/Integrations/redis#enable-slow-logs","content":"In redis.cnf file, uncomment and configure the variables shown below:  slowlog-log-slower-than= 1 slowlog-max-len=100 Copy Or, login to redis with root user and execute below commands  config set slowlog-log-slower-than= 1; config set slowlog-max-len=100; Copy "},{"title":"Configuration ​","type":1,"pageTitle":"Monitoring Redis on instances","url":"docs/Integrations/redis#configuration","content":"Refer to sfAgent section for steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/directory  metrics: plugins: - name: redisdb enabled: true interval: 60 config: documentsTypes: - keyspaceStat - redisDetails - redisPersistence - redisStat - slowLogs password: pass port: 6379 user: admin logging: plugins: - name: redis-general enabled: true config: log_path: /var/log/redis/redis-server.log Copy "},{"title":"Viewing data and dashboards  ​","type":1,"pageTitle":"Monitoring Redis on instances","url":"docs/Integrations/redis#viewing-data-and-dashboards","content":"Data generated by plugin can be viewed in browse data page inside the respective application under plugin=redisdb and documentType=serverDetails  Dashboard for this data can be instantiated by Importing dashboard template RedisDB to the application dashboard "},{"title":"RabbitMQ on Instances","type":0,"sectionRef":"#","url":"docs/Integrations/rabbitmqinstance","content":"","keywords":""},{"title":"Monitoring RabbitMQ Message Broker running on Instances​","type":1,"pageTitle":"RabbitMQ on Instances","url":"docs/Integrations/rabbitmqinstance#monitoring-rabbitmq-message-broker-running-on-instances","content":""},{"title":"Overview​","type":1,"pageTitle":"RabbitMQ on Instances","url":"docs/Integrations/rabbitmqinstance#overview","content":"Rabbitmq sfAgent plugin provides metrics related to RabbitMQ message broker. Metrics collected by the plugin are organized across the following categories clusterDetailsnodeStatsconnectionStatschannelStatsexchangeStatsqueueStatsconsumerStats Note: node-is-quorum-critical and node-is-mirror-sync-critical is supported on 3.9 and above. Tested on: RabbitMQ version 3.9 and 3.7 "},{"title":"Configuration​","type":1,"pageTitle":"RabbitMQ on Instances","url":"docs/Integrations/rabbitmqinstance#configuration","content":"sfAgent section provides steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory "},{"title":"Metrics​","type":1,"pageTitle":"RabbitMQ on Instances","url":"docs/Integrations/rabbitmqinstance#metrics","content":""},{"title":"plugins ​","type":1,"pageTitle":"RabbitMQ on Instances","url":"docs/Integrations/rabbitmqinstance#plugins","content":" name: rabbitmq enabled: true interval: 60 config: documentsTypes: - clusterDetails - nodeStats - connectionStats - channelStats - exchangeStats - queueStats - consumerStats host: localhost password: &lt;password&gt; port: 15672 secure: false username: &lt;username&gt; Copy "},{"title":"Parameters required in metrics plugin  ​","type":1,"pageTitle":"RabbitMQ on Instances","url":"docs/Integrations/rabbitmqinstance#parameters-required-in-metrics-plugin","content":"Username: username of the RabbitMQ userpassword: password for RabbitMQport: Broker Portsecure: http(false) or https(true)documentTypes: User can either leave this empty to collect all documentTypes or mention specific documentTypes to collect. Available options for plugin type RabbitMQ are clusterDetails, nodeStats, connectionStats, exchangeStats, queueStats, consumerStats "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"RabbitMQ on Instances","url":"docs/Integrations/rabbitmqinstance#viewing-data-and-dashboards","content":"Data generated by plugin can be viewed in browse data page inside the respective application under plugin=RabbitMQ and documentType=Cluster_details.Dashboard for this data can be instantiated by Importing dashboard template RabbitMQ to the application dashboard. "},{"title":"Postgres on Instances","type":0,"sectionRef":"#","url":"docs/Integrations/postgres/postgres_instances","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Postgres on Instances","url":"docs/Integrations/postgres/postgres_instances#overview","content":"PostgreSQL on instances is monitored using sfAgent configured with postgres plugin "},{"title":"Metrics plugin​","type":1,"pageTitle":"Postgres on Instances","url":"docs/Integrations/postgres/postgres_instances#metrics-plugin","content":"Collects metric data organized in following documentTypes in metrics index: serverDetails databaseDetails tableDetails IndexDetails queryDetails  "},{"title":"Logger plugin​","type":1,"pageTitle":"Postgres on Instances","url":"docs/Integrations/postgres/postgres_instances#logger-plugin","content":"Collects general logs and slow query logs. General logs are sent to log index under documentType: postgres-general and slow queries logs are parsed and data is sent metrics index in documentType: postgres-slowquery "},{"title":"Pre-requisites​","type":1,"pageTitle":"Postgres on Instances","url":"docs/Integrations/postgres/postgres_instances#pre-requisites","content":""},{"title":"Enable PostgreSQL general logs​","type":1,"pageTitle":"Postgres on Instances","url":"docs/Integrations/postgres/postgres_instances#enable-postgresql-general-logs","content":"Logging needs to be configured in the postgresql.conf file. This file can be located by executing the command shown below: postgres=# show config_file; config_file ---------------------------------- /data/pgsql/data/postgresql.conf (1 row) Copy In postgresql.conf file, uncomment and configure the variables shown below:  log_min_messages = warning # set level as appropriate log_line_prefix = '&lt; %m &gt; ' Copy "},{"title":"Enable Slow Query Logs​","type":1,"pageTitle":"Postgres on Instances","url":"docs/Integrations/postgres/postgres_instances#enable-slow-query-logs","content":"Configuring log_min_duration_statement = 200 will log any query which takes more than 200ms to execute which. Set the value to appropriate value "},{"title":"Set access permissions​","type":1,"pageTitle":"Postgres on Instances","url":"docs/Integrations/postgres/postgres_instances#set-access-permissions","content":"Username used for DB access should have appropriate permissions grant SELECT ON pg_stat_database to &lt;username&gt;; grant pg_monitor to &lt;username&gt;; Copy note root user has these permissions by default "},{"title":"Configuration​","type":1,"pageTitle":"Postgres on Instances","url":"docs/Integrations/postgres/postgres_instances#configuration","content":"Refer to sfAgent section for steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: &lt;profile_key&gt; tags: Name: &lt;name&gt; appName: &lt;app_name&gt; projectName: &lt;project_name&gt; metrics: plugins: - name: postgres enabled: true interval: 60 config: documentsTypes: - databaseDetails - indexDetails - queryDetails - serverDetails - tableDetails host: 127.0.0.1 password: &lt;password&gt; port: 5432 user: &lt;username&gt; logging: plugins: - name: postgres-general enabled: true config: log_level: - error - warning - info - log log_path: /var/log/postgresql/postgresql-10-main.log - name: postgres-slowquery enabled: true config: log_path: /var/log/postgresql/postgresql-10-main.log Copy "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"Postgres on Instances","url":"docs/Integrations/postgres/postgres_instances#viewing-data-and-dashboards","content":"Data generated by plugin can be viewed in browse data page inside the respective application under plugin=postgres and documentType= serverDetails, databaseDetails, tableDetails, IndexDetails, queryDetails, postgres-slowquery Dashboard for this data can be instantiated by Importing dashboard template PostgreSQL to the application dashboard "},{"title":"Monitoring Redis Sentinel setup on Instances","type":0,"sectionRef":"#","url":"docs/Integrations/redis_sentinel","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Monitoring Redis Sentinel setup on Instances","url":"docs/Integrations/redis_sentinel#overview","content":"Redis Sentinel on Instances is monitored using sfAgent Configured with Redisdb Plugin. When Sentinel is Configured. Redis sentinel plugin was tested with 6.2.6 and 5.0.7 sentinel versions with ubuntu 20.04.4 LTS. "},{"title":"Metrics plugin​","type":1,"pageTitle":"Monitoring Redis Sentinel setup on Instances","url":"docs/Integrations/redis_sentinel#metrics-plugin","content":"Collects metric data organized in following documentType under metrics index:  redisDetails redisStat redisPersistence masterReplication slaveReplication  "},{"title":"Logger plugin​","type":1,"pageTitle":"Monitoring Redis Sentinel setup on Instances","url":"docs/Integrations/redis_sentinel#logger-plugin","content":"Collects general logs and slow query logs. General logs are sent to log index whereas slow queries are sent to metrics index under documentType:redisSlowLogs "},{"title":"Pre-requisites ​","type":1,"pageTitle":"Monitoring Redis Sentinel setup on Instances","url":"docs/Integrations/redis_sentinel#pre-requisites","content":""},{"title":"Enable Slow Logs  ​","type":1,"pageTitle":"Monitoring Redis Sentinel setup on Instances","url":"docs/Integrations/redis_sentinel#enable-slow-logs","content":"In redis.cnf file, uncomment and configure the variables shown below:  slowlog-log-slower-than= 1 slowlog-max-len=100 Copy Or, login to redis with root user and execute below commands  config set slowlog-log-slower-than= 1; config set slowlog-max-len=100; Copy "},{"title":"Configuration ​","type":1,"pageTitle":"Monitoring Redis Sentinel setup on Instances","url":"docs/Integrations/redis_sentinel#configuration","content":"Refer to sfAgent to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/directory  Metrics: plugins: - name: redisdb enabled: true interval: 60 config: documentsTypes: - keyspaceStat - redisDetails - redisPersistence - redisStat - slowLogs - masterReplication - slaveReplication password: &lt; password &gt; port: 6379 user: &lt; username &gt; sentinelIp: &lt; Ipaddress &gt; sentinelPort: 26379 sentinelMaster: &lt; Mastername &gt; logging: plugins: - name: redis-sentinel enabled: true config: log_path: /var/log/redis/sentinel.log Copy "},{"title":"Viewing data and dashboards     ​","type":1,"pageTitle":"Monitoring Redis Sentinel setup on Instances","url":"docs/Integrations/redis_sentinel#viewing-data-and-dashboards--","content":"Data generated by plugin can be viewed in browse data page inside the respective application under plugin=redisdb and documentType=serverDetails  Dashboard for this data can be instantiated by Importing dashboard template RedisReplication to the application dashboard. "},{"title":"RabbitMQ in Kubernetes","type":0,"sectionRef":"#","url":"docs/Integrations/rabbitmqkubernet","content":"","keywords":""},{"title":"Monitoring RabbitMQ Message Broker running on Kubernetes​","type":1,"pageTitle":"RabbitMQ in Kubernetes","url":"docs/Integrations/rabbitmqkubernet#monitoring-rabbitmq-message-broker-running-on-kubernetes","content":"RabbitMQ running in Kubernetes can be monitored in SnappyFlow using: sfKubeAgent as sidecar container. "},{"title":"MySQL monitoring with sfKubeAgent​","type":1,"pageTitle":"RabbitMQ in Kubernetes","url":"docs/Integrations/rabbitmqkubernet#mysql-monitoring-with-sfkubeagent","content":"In this approach, sfKubeAgent is run as a side-car inside RabbitMQ pod. The example below shows the config-map for sfKubeAgent container, config-map for RabbitMQ container and pod yaml. "},{"title":"ConfigMap for RabbitMQ sfKubeAgent:​","type":1,"pageTitle":"RabbitMQ in Kubernetes","url":"docs/Integrations/rabbitmqkubernet#configmap-for-rabbitmq-sfkubeagent","content":"apiVersion: v1 kind: ConfigMap metadata: name: rabbitmq-sfagent-config data: config.yaml: |- key: &quot;&lt;profile key&gt;&quot; metrics: plugins: - name: rabbitmq enabled: true interval: 60 config: documentsTypes: - clusterDetails - nodeStats - connectionStats - channelStats - exchangeStats - queueStats - consumerStats host: localhost password: &lt;password&gt; secure: false port: 15672 username: &lt;username&gt; agent: loglevel: debug Copy "},{"title":"RabbitMQ YAML(Statefulset)​","type":1,"pageTitle":"RabbitMQ in Kubernetes","url":"docs/Integrations/rabbitmqkubernet#rabbitmq-yamlstatefulset","content":"apiVersion: apps/v1 kind: StatefulSet metadata: name: rabbitmqcluster spec: replicas: 3 serviceName: rabbitmqcluster-headless template: spec: containers: - command: - /app/sfagent - -config-file - /opt/sfagent/config.yaml - -enable-console-log env: - name: APP_NAME value: rabbitmq - name: PROJECT_NAME value: rabbitmq-kube image: snappyflowml/sfagent:latest imagePullPolicy: Always name: rabbitmq-sfagent resources: limits: cpu: 500m memory: 500Mi requests: cpu: 100m memory: 128Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /opt/sfagent/config.yaml name: sfagent subPath: config.yaml securityContext: fsGroup: 1001 volumes: - configMap: defaultMode: 420 name: rabbitmq-sfagent-config name: sfagent Copy "},{"title":"Parameters required in metrics plugin​","type":1,"pageTitle":"RabbitMQ in Kubernetes","url":"docs/Integrations/rabbitmqkubernet#parameters-required-in-metrics-plugin","content":"Username: username of the RabbitMQ userpassword: password for RabbitMQport: Broker Portsecure: http(false) or https(true)documentTypes: User can either leave this empty to collect all documentTypes or mention specific documentTypes to collect. Available options for plugin type RabbitMQ are clusterDetails, nodeStats, connectionStats, exchangeStats, queueStats, consumerStats "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"RabbitMQ in Kubernetes","url":"docs/Integrations/rabbitmqkubernet#viewing-data-and-dashboards","content":"Data collected by plugins can be viewed in SnappyFlow’s browse data section plugin: rabbitmqdocumentType: clusterDetails, nodeStats, connectionStats, exchangeStats, queueStats, consumerStatsDashboard template: RabbitMQ "},{"title":"Overview","type":0,"sectionRef":"#","url":"docs/Integrations/tomcat/overview","content":"Overview Tomcat monitoring on SnappyFlow is available for the following platforms Linux​ Windows​ Tomcat monitoring in Kubernetes is also available. Documentation coming soon !!","keywords":""},{"title":"Postgres in Kubernetes","type":0,"sectionRef":"#","url":"docs/Integrations/postgres/postgres_kubernetes","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Postgres in Kubernetes","url":"docs/Integrations/postgres/postgres_kubernetes#overview","content":"PostgreSQL running in Kubernetes can be monitored in SnappyFlow using two approaches: sfKubeAgent as sidecar containerPrometheus exporter  "},{"title":"PostgreSQL monitoring with sfKubeAgent​","type":1,"pageTitle":"Postgres in Kubernetes","url":"docs/Integrations/postgres/postgres_kubernetes#postgresql-monitoring-with-sfkubeagent","content":"sfKubeAgent is run as a sidecar with the configMap shown below. The config map instantiates plugins for metrics, general logs and slow queries. apiVersion: v1 kind: ConfigMap metadata: name: postgres-configmap data: config.yaml: |- key: &lt;profile_key&gt; metrics: plugins: - name: postgres enabled: true interval: 60 config: documentsTypes: #user can enable all or only needed documents - databaseDetails - indexDetails8 - queryDetails - serverDetails - tableDetails host: 127.0.0.1 user: &lt;userName&gt; password: &lt;password&gt; port: 5432 logging: plugins: - name: postgres-general enabled: true config: log_level: - error - warning - info - log log_path: /var/log/postgres/*.log - name: postgres-slowquery enabled: true config: log_path: /var/log/postgres/*.log Copy The example of PostgreSQL pod with Postgres and sfKubeAgent containers is shown below: kind: Pod apiVersion: v1 metadata: name: postgres-pod labels: snappyflow/appname: &lt;app_name&gt; snappyflow/projectname: &lt;project_name&gt; spec: containers: - name: postgres-container securityContext: {} image: &quot;postgres:9.6&quot; args: [&quot;-c&quot;, &quot;log_statement=all&quot;, &quot;-c&quot;, &quot;log_min_messages=warning&quot;, &quot;-c&quot;, &quot;log_min_duration_statement=200&quot;, &quot;-c&quot;,&quot;log_directory=/var/log/postgres&quot;,&quot;-c&quot;,&quot;log_line_prefix=&lt; %m &gt; &quot;,&quot;-c&quot;,&quot;log_filename=postgresql-%Y-%m-%d_%H%M%S.log&quot;,&quot;-c&quot;,&quot;log_truncate_on_rotation=off&quot;,&quot;-c&quot;,&quot;log_rotation_age=1d&quot;,&quot;-c&quot;,&quot;logging_collector=on&quot;] imagePullPolicy: IfNotPresent ports: - name: tcp containerPort: 5432 protocol: TCP env: - name: POSTGRES_PASSWORD value: &lt;password&gt; - name: POSTGRES_USER value: &lt;userName&gt; volumeMounts: - name: varlog mountPath: /var/log/postgres # Snappyflow's sfkubeagent container - name: sfagent-container image: snappyflowml/sfagent:latest imagePullPolicy: Always command: - /app/sfagent - -enable-console-log env: - name: APP_NAME value: &lt;app_name&gt; - name: PROJECT_NAME value: &lt;project_name&gt; volumeMounts: - name: configmap-postgres mountPath: /opt/sfagent/config.yaml subPath: config.yaml - name: varlog mountPath: /var/log/postgres volumes: - name: configmap-postgres configMap: name: postgres-configmap - name: varlog emptyDir: {} Copy "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"Postgres in Kubernetes","url":"docs/Integrations/postgres/postgres_kubernetes#viewing-data-and-dashboards","content":"Data generated by plugin can be viewed in “browse data” page inside the respective application under plugin=postgres and documentType= serverDetails, databaseDetails, tableDetails, IndexDetails Dashboard for this data can be instantiated by Importing dashboard template PostgreSQL to the application dashboard  "},{"title":"PostgreSQL monitoring with Prometheus​","type":1,"pageTitle":"Postgres in Kubernetes","url":"docs/Integrations/postgres/postgres_kubernetes#postgresql-monitoring-with-prometheus","content":"Refer to Prometheus Exporter overview to understand how SnappyFlow monitors using Prometheus exporters. "},{"title":"Pre-requisites​","type":1,"pageTitle":"Postgres in Kubernetes","url":"docs/Integrations/postgres/postgres_kubernetes#pre-requisites","content":"Prometheus exporter is deployed as a side-car in the application container and the exporter port is accessible to sfPod  "},{"title":"Configurations​","type":1,"pageTitle":"Postgres in Kubernetes","url":"docs/Integrations/postgres/postgres_kubernetes#configurations","content":"kind: Pod apiVersion: v1 metadata: name: postgres-pod labels: snappyflow/appname: &lt;app_name&gt; snappyflow/projectname: &lt;project_name&gt; snappyflow/component: postgresql spec: containers: - name: postgres-exporter image: bitnami/postgres-exporter ports: - name: pg-exporter containerPort: 9187 command: [&quot;/bin/sh&quot;, &quot;-c&quot;] args: ['DATA_SOURCE_NAME=&quot;postgresql://&lt;user_name&gt;:&lt;password&gt;@localhost:5432/&lt;dbname&gt;?sslmode=disable&quot; /opt/bitnami/postgres-exporter/bin/postgres_exporter'] - name: postgres-container securityContext: {} image: &quot;postgres:9.6&quot; args: [&quot;-c&quot;, &quot;log_statement=all&quot;, &quot;-c&quot;, &quot;log_min_messages=warning&quot;, &quot;-c&quot;, &quot;log_min_duration_statement=200&quot;, &quot;-c&quot;,&quot;log_line_prefix=&lt; %m &gt; &quot;] imagePullPolicy: IfNotPresent ports: - name: tcp containerPort: 5432 protocol: TCP env: - name: POSTGRES_PASSWORD value: &lt;password&gt; - name: POSTGRES_USER value: &lt;user_name&gt; - name: POSTGRES_DB value: &lt;dbname&gt; Copy "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"Postgres in Kubernetes","url":"docs/Integrations/postgres/postgres_kubernetes#viewing-data-and-dashboards-1","content":"Data generated by plugin can be viewed in browse data page inside the respective application under plugin=kube-prom-postgres and documentType= psql Dashboard for this data can be instantiated by Importing dashboard template PostgreSQL_Prom to the application dashboard  "},{"title":"PostgreSQL Pod Centralized Logging​","type":1,"pageTitle":"Postgres in Kubernetes","url":"docs/Integrations/postgres/postgres_kubernetes#postgresql-pod-centralized-logging","content":"Pls refer to Centralized Logging Overview to understand how SnappyFlow implements centralized logging Centralized logging approach requires the application pod to stream logs to stdout, which is achieved by running a busy box container as shown below. kind: Pod apiVersion: v1 metadata: name: postgres-pod labels: snappyflow/appname: &lt;app_name&gt; snappyflow/projectname: &lt;project_name&gt; snappyflow/component: postgresql spec: containers: - name: postgres-exporter image: bitnami/postgres-exporter ports: - name: pg-exporter containerPort: 9187 command: - /bin/sh - '-c' args: - &gt;- DATA_SOURCE_NAME=&quot;postgresql://&lt;user_name&gt;:&lt;password&gt;@localhost:5432/&lt;dbname&gt;?sslmode=disable&quot; /opt/bitnami/postgres-exporter/bin/postgres_exporter - name: postgres-container securityContext: {} image: 'postgres:9.6' args: - '-c' - log_statement=all - '-c' - log_min_messages=warning - '-c' - log_min_duration_statement=200 - '-c' - 'log_line_prefix=&lt; %m &gt; ' - '-c' - log_directory=/var/log/postgres - '-c' - log_filename=postgresql.log - '-c' - logging_collector=on imagePullPolicy: IfNotPresent ports: - name: tcp containerPort: 5432 protocol: TCP env: - name: POSTGRES_PASSWORD value: &lt;password&gt; - name: POSTGRES_USER value: &lt;user_name&gt; - name: POSTGRES_DB value: &lt;dbname&gt; volumeMounts: - name: postgres-log mountPath: /var/log/postgres - name: postgres-general image: busybox command: - /bin/sh - '-c' args: - tail -n+1 -f /var/log/postgres/*.log volumeMounts: - name: postgres-log mountPath: /var/log/postgres volumes: - name: postgres-log emptyDir: {} Copy "},{"title":"Tomcat Linux","type":0,"sectionRef":"#","url":"docs/Integrations/tomcat/tomcat_linux","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Tomcat Linux","url":"docs/Integrations/tomcat/tomcat_linux#overview","content":"Tomcat server monitoring collect multiple types of metrics like server stats, context stats, jvm stats using Jolokia "},{"title":"Pre-requisites​","type":1,"pageTitle":"Tomcat Linux","url":"docs/Integrations/tomcat/tomcat_linux#pre-requisites","content":""},{"title":"JVM and JCMD​","type":1,"pageTitle":"Tomcat Linux","url":"docs/Integrations/tomcat/tomcat_linux#jvm-and-jcmd","content":"Tomcat Plugin is based on Jolokia agent which requires JMX monitoring to be enabled locally. JCMD command must be installed in the machineThe JVM must have HotSpot enabled and be a JVM 1.6 or larger. "},{"title":"Access Log Format​","type":1,"pageTitle":"Tomcat Linux","url":"docs/Integrations/tomcat/tomcat_linux#access-log-format","content":"Tomcat server access log format needs to be modified to capture all metrics from the access logs, which includes following steps Edit the file $TOMCAT_HOME/conf/server.xml Set log format in “org.apache.catalina.valves.AccessLogValve” class, pattern value to pre-defined “combined” log format or %h %l %u %t &amp;quot;%r&amp;quot; %s %b %D &amp;quot;%{Referer}i&amp;quot; &amp;quot;%{User-Agent}i&amp;quot; Copy After changing log pattern to combined or the above mentioned pattern, sample log would look like: 49.206.1.85 - - [30/Jun/2020:13:12:32 +0000] &quot;GET / HTTP/1.1&quot; 200 11286 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36&quot; Copy "},{"title":"Configuration​","type":1,"pageTitle":"Tomcat Linux","url":"docs/Integrations/tomcat/tomcat_linux#configuration","content":"sfAgent section provides steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: &lt;profile key&gt; generate_name: true tags: Name: &lt;unique instance name or will be generated from IP&gt; appName: &lt;add application name&gt; projectName: &lt;add project name&gt; metrics: plugins: - name: tomcat enabled: true interval: 300 config: username: xxxx password: xxxx documentTypes: - tomcatStats - requestProcessorStats - jvmStats - contextStats logging: plugins: - name: tomcat-access enabled: true config: geo_info: true log_path: /opt/apache-tomcat*/logs/localhost_access_*.txt ua_parser: false url_normalizer: false Copy Tomcat access log options: Geo-IP: Useful to find geographical location of the client using the IP address. To enable, set the option &quot;geo_info&quot; to true in the above configuration.User-Agent Analysis: To get the host machine details like browser, Operating system and device by analysis the user-agent. To enable, set the option &quot;ua_parser&quot; to true in the above configuration. If enabled, by default it runs on port 8586.URL Normalizer (not supported in container deployment): Normalize incoming URL paths. To enable, set the option &quot;url_normalizer&quot; to true in the above configuration. If enabled, by default it runs on port 8587.  Normalization specific configuration is available in /opt/sfagent/normalization/config.yaml which resonate the following, interval: 300 dynamic_rule_generation: enabled: true rules_length_limit: 1000 log_volume: 100000 rules: [] Copy Config Field Description​ interval: Normalization algorithm runtime interval. enabled: Rely on normalization feature for rule generation. rules_length_limit: Limit over size of generated rules. set the value to -1 for specifying no limit. log_volume: Limit over number of logs processed. set the value to -1 for specifying no limit. rules: Rules Generated. Recommended approach is to run sfagent with dynamic_rule_generation enabled over a period of time. Observe whether rules generated reflect all the web app requests intended to be normalized and if its a true reflection, set enabled flag to false , indicating no further rules will be generated Default ports used by user-agent analysis and URL Normalizer can be changed respectively with the inclusion of following in config.yaml agent: uaparserport: port_number url_normalizer: port_number Copy note Latitude and Longitude are often near the center of population. These values are not precise and should not be used to identify a particular address or household.User-agent parsing requires high computation power. Recommended to enable only if needed and system have enough CPU resource available. "},{"title":"View Data and Dashboard​","type":1,"pageTitle":"Tomcat Linux","url":"docs/Integrations/tomcat/tomcat_linux#view-data-and-dashboard","content":"Tomcat server metrics plugin=tomcat consists of four document types: Tomcat stats: contain metrics like tomcat sever version, uptime, thread details.Request processor stats: shows request information like processing time, request count, data received and sent.Context stats: contain tomcat context related metrics like hit count, lookup count etc.JVM stats: contain all JVM related metrics used by tomcat server like garbage collection details, memory pools, loaded/unloaded classes etc. Tomcat access logs come under metrics section on APM dashboard with plugin=tomcat-access and documentType=tomcatAccessLogs Dashboard for this data can be instantiated by Importing dashboard template Tomcat_Server, Tomcat_Access to the application dashboard. "},{"title":"Analyzing ETL Jobs with SnappyFlow","type":0,"sectionRef":"#","url":"docs/Log_management/etl_jobs","content":"","keywords":""},{"title":"Step 1: Drop logs from ETL Jobs​","type":1,"pageTitle":"Analyzing ETL Jobs with SnappyFlow","url":"docs/Log_management/etl_jobs#step-1-drop-logs-from-etl-jobs","content":"SnappyFlow allows for a job to have up to a 3-level hierarchy- Job, Stage, Task. Logs in JSON format have to be dropped whenever a job/stage/task is started, completed or terminated. This log can be parsed using SnappyFlow’s ETL parser. Log format for a Job: { &quot;jobName&quot;: &lt;Job-name&gt;, &quot;jobId&quot;: &lt;Unique JobId&gt;, &quot;time&quot;: &lt;Time in epoch milliseconds format&gt; &quot;type&quot;: &quot;job&quot;, &quot;status&quot;: &lt;status: started, success, failed, aborted&gt; } Copy Log format for a Stage:  { &quot;jobName&quot;: &lt;Job-name&gt;, &quot;jobId&quot;: &lt;Unique JobId&gt;, &quot;stageId&quot;: &lt;stageId&gt;, &quot;stageName&quot;: &lt;stageName&gt; &quot;time&quot;: &lt;Time in epoch milliseconds format&gt; &quot;type&quot;: &quot;stage&quot;, &quot;status&quot;: &lt;status can be started, success, failed, aborted&gt; } Copy Log format for a Task: { &quot;jobName&quot;: &lt;Job-name&gt;, &quot;jobId&quot;: &lt;Unique JobId&gt;, “stageId”: &lt;staged&gt;, “stageName”: &lt;stageName&gt; &quot;time&quot;: &lt;Time in epoch milliseconds format&gt; &quot;type&quot;: &quot;task&quot;, &quot;status&quot;: &lt;status can be started, success, failed, aborted&gt; } Copy "},{"title":"Step 2: Forward logs to SnappyFlow​","type":1,"pageTitle":"Analyzing ETL Jobs with SnappyFlow","url":"docs/Log_management/etl_jobs#step-2-forward-logs-to-snappyflow","content":"Add the following log parser to logging section of sfAgent’s config.yaml: logging: plugins: - name: etlRaw enabled: true config: log_path: &lt;log file path&gt; Copy Restart sfAgent with the new configuration. service sfagent restart Copy Check if documents have been received in SnappyFlow. You will find 3 documents under metrics with plugin name as “etlRaw” and documentType as “job”, “stage” and “task” depending on your hierarchy. "},{"title":"Step 3: Generate an access URL for use by summarization module​","type":1,"pageTitle":"Analyzing ETL Jobs with SnappyFlow","url":"docs/Log_management/etl_jobs#step-3-generate-an-access-url-for-use-by-summarization-module","content":"Logs shipped to SnappyFlow are in a raw form and they cannot be directly used for reporting and analysis. Therefore user has to export this raw data to a summarization script that transforms the data and sends it back to SnappyFlow into a new document. Import a ETL template into your dashboard. Go to “Scratchpad” pane Click on ‘Export API Endpoint’ option in the component and create component URL for all 3 components for interval, say Last 5 mins.  Click on the ‘API Endpoints’ option for the project to view the API List. Copy the URL’s for the 3 components and the Authentication token. These need to be provided in Step 4  "},{"title":"Step 4: Run summarization script as a cronjob​","type":1,"pageTitle":"Analyzing ETL Jobs with SnappyFlow","url":"docs/Log_management/etl_jobs#step-4-run-summarization-script-as-a-cronjob","content":"Install the pip utility from the below link. Refer to the link for Installation and Usage instructions. ​ sfapmetl · PyPI The python script takes a config file path as input Set values for key, appName, projectName, Name. Provide the component Url’s for Job, stage and Task and authKey (from Step 3) The data will be available in the dashboard under the plugin ‘etlReport’ and documentType - job, stage and task. "},{"title":"Step 5: Review ETL Dashboards​","type":1,"pageTitle":"Analyzing ETL Jobs with SnappyFlow","url":"docs/Log_management/etl_jobs#step-5-review-etl-dashboards","content":"You will now see the summarized data in dashboard under etlReport for job, stage and tasks. Select a particular job and choose a timeline to see job duration trends over the selected time period. Clicking on a particular job id provides a drilled down view of stages and tasks within that job.  "},{"title":"Tomcat Windows","type":0,"sectionRef":"#","url":"docs/Integrations/tomcat/tomcat_windows","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Tomcat Windows","url":"docs/Integrations/tomcat/tomcat_windows#overview","content":"Tomcat monitoring involves metrics like server stats, context stats, jvm stats using Jolokia, and the server access logs. "},{"title":"Pre-requisites​","type":1,"pageTitle":"Tomcat Windows","url":"docs/Integrations/tomcat/tomcat_windows#pre-requisites","content":""},{"title":"Enabling JMX Monitoring​","type":1,"pageTitle":"Tomcat Windows","url":"docs/Integrations/tomcat/tomcat_windows#enabling-jmx-monitoring","content":"Tomcat Plugin is based on Jolokia agent which requires JMX monitoring to be enabled locally. Copy the jolokia.war file from &quot;C:/Program Files (x86)/Sfagent/&quot; to ${TOMCAT_HOME}/webappsAdd jolokia as role in tomcat-users.xml (mandatory for Jolokia 1.6 or later).  &lt;role rolename=&quot;jolokia&quot;/&gt; &lt;user username=&quot;jolokia&quot; password=&quot;&lt;password&gt;&quot; roles=&quot;jolokia&quot;/&gt; Copy Restart the Tomcat serverVerify the Jolokia agent installation by accessing the URL: http://address:port/jolokia/version. The result looks similar to this: { &quot;request&quot;: { &quot;type&quot;: &quot;version&quot; }, &quot;value&quot;: { &quot;agent&quot;: &quot;1.3.7&quot;, &quot;protocol&quot;: &quot;7.2&quot;, &quot;config&quot;: { &quot;maxCollectionSize&quot;: &quot;0&quot;, &quot;agentId&quot;: &quot;10.152.24.99-29844-172f5788-servlet&quot;, &quot;debug&quot;: &quot;false&quot;, &quot;agentType&quot;: &quot;servlet&quot;, &quot;serializeException&quot;: &quot;false&quot;, &quot;detectorOptions&quot;: &quot;{}&quot;, &quot;dispatcherClasses&quot;: &quot;org.jolokia.jsr160.Jsr160RequestDispatcher&quot;, &quot;maxDepth&quot;: &quot;15&quot;, &quot;discoveryEnabled&quot;: &quot;false&quot;, &quot;canonicalNaming&quot;: &quot;true&quot;, &quot;historyMaxEntries&quot;: &quot;10&quot;, &quot;includeStackTrace&quot;: &quot;true&quot;, &quot;maxObjects&quot;: &quot;0&quot;, &quot;debugMaxEntries&quot;: &quot;100&quot; }, &quot;info&quot;: { &quot;product&quot;: &quot;tomcat&quot;, &quot;vendor&quot;: &quot;Apache&quot;, &quot;version&quot;: &quot;8.5.23&quot; } }, &quot;timestamp&quot;: 1509955465, &quot;status&quot;: 200 } Copy "},{"title":"Access Log Format​","type":1,"pageTitle":"Tomcat Windows","url":"docs/Integrations/tomcat/tomcat_windows#access-log-format","content":"Tomcat server access log format needs to be modified to capture all metrics from the access logs, which includes following steps Edit the file $TOMCAT_HOME/conf/server.xml Set suffix &quot;org.apache.catalina.valves.AccessLogValve&quot; class, pattern value to &quot;.log&quot; Set log format in &quot;org.apache.catalina.valves.AccessLogValve&quot; class, pattern value to pre-defined &quot;combined&quot; log format or %h %l %u %t &amp;quot;%r&amp;quot; %s %b %D &amp;quot;%{Referer}i&amp;quot; &amp;quot;%{User-Agent}i&amp;quot; Copy After changing log pattern to combined or the above mentioned pattern, sample log would look like: 49.206.1.85 - - [30/Jun/2020:13:12:32 +0000] &quot;GET / HTTP/1.1&quot; 200 11286 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36&quot; Copy "},{"title":"Configuration Settings​","type":1,"pageTitle":"Tomcat Windows","url":"docs/Integrations/tomcat/tomcat_windows#configuration-settings","content":"sfAgent section provides steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under &quot;C:/Program Files (x86)/Sfagent/&quot; directory key: &lt;profile key&gt; generate_name: true tags: Name: &lt;unique instance name or will be generated from IP&gt; appName: &lt;add application name&gt; projectName: &lt;add project name&gt; metrics: plugins: - name: tomcat enabled: true interval: 300 config: port: 8080 proxy: false rmiport: 9000 protocol: http username: xxxx password: xxxx documentTypes: - tomcatStats - requestProcessorStats - jvmStats - contextStats logging: plugins: - name: tomcat-access enabled: true config: log_path: &quot;C:\\\\Program Files\\\\Apache Software Foundation\\\\Tomcat*\\\\logs\\\\localhost_access*.log&quot; geo_info: true ua_parser: false url_normalizer: false Copy note Keep username and password same as jolokia role updated in tomcat-users.xml. Tomcat Access Logger Options​ Geo-IP: Useful to find geographical location of the client using the IP address. To enable, set the option &quot;geo_info&quot; to true in the above configuration.User-Agent Analysis: To get the host machine details like browser, Operating system and device by analysis the user-agent. To enable, set the option &quot;ua_parser&quot; to true in the above configuration. If enabled, by default it runs on port 8586.URL Normalizer (not supported in container deployment): Normalize incoming URL paths. To enable, set the option &quot;url_normalizer&quot; to true in the above configuration. If enabled, by default it runs on port 8587.  "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"Tomcat Windows","url":"docs/Integrations/tomcat/tomcat_windows#viewing-data-and-dashboards","content":"Tomcat plugin provides the following document types: Tomcat stats: contain metrics like tomcat sever version, uptime, thread details.Request processor stats: shows request information like processing time, request count, data received and sent.Context stats: contain tomcat context related metrics like hit count, lookup count etc.JVM stats: contain all JVM related metrics used by tomcat server like garbage collection details, memory pools, loaded/unloaded classes etc.Tomcat Access: Tomcat server access log details. Dashboard for this data can be instantiated by Importing dashboard template Tomcat_Server, Tomcat_Access to the application dashboard. "},{"title":"Monitoring Apache ZooKeeper running on Instances","type":0,"sectionRef":"#","url":"docs/Integrations/zookeeper","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Monitoring Apache ZooKeeper running on Instances","url":"docs/Integrations/zookeeper#overview","content":"Zookeeper afAgent Metric plugin helps in analyzing the efficiency of zookeeper infrastructure by providing key metrics like node count, packet count, latency, watch count etc. "},{"title":"Prerequisites​","type":1,"pageTitle":"Monitoring Apache ZooKeeper running on Instances","url":"docs/Integrations/zookeeper#prerequisites","content":"Zookeeper Plugin is based on Jolokia agent which requires JMX monitoring to be enable locally. Following property needs to be included during the start of Zookeeper process -Dcom.sun.management.jmxremote Copy JCMD command must be installed in the machine Zookeeper ships with log4j support. Log4j property file (log4j.properties) is present in root folder of Zookeeper and has to be set as follows Enabling root logger and file appender where file appender can be of any type based on rolling strategy log4j.rootLogger=INFO, logfile log4j.appender.logFile=org.apache.log4j.DailyRollingFileAppender Copy Specifying custom log file name along with its path , layout properties and data pattern log4j.appender.logFile.DatePattern='.'yyyy-MM-dd-HH log4j.appender.logFile.File= &lt;..logpath..&gt; log4j.appender.logFile.layout=org.apache.log4j.PatternLayout log4j.appender.logFile.layout.ConversionPattern=[%d] %p %m (%c)%n Copy After configuring log4j properties, emitted log would look like [2020-07-09 11:15:23,376] INFO Accepted socket connection from /10.233.115.193:34962 (org.apache.zookeeper.server.NIOServerCnxnFactory) Copy "},{"title":"Configuration​","type":1,"pageTitle":"Monitoring Apache ZooKeeper running on Instances","url":"docs/Integrations/zookeeper#configuration","content":"sfAgent section provides steps to install and automatically generate plugin configurations. User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: &lt;Profile_key&gt; tags: Name: &lt;instance_name&gt; appName: &lt;app_name&gt; projectName: &lt;project_name&gt; metrics: plugins: - name: zookeeperjmx enabled: true interval: 60 config: dataDir: /tmp/zookeeper/ documentsTypes: - jmxStats - zookeeperStats port: 2181 logging: plugins: - name: zookeeper-general enabled: true config: log_level: - error - info - debug - notice log_path: /home/kafka/kafka_2.12-2.6.2/bin/../logs/*.log Copy Viewing data and dashboards Data collected by plugins can be viewed in SnappyFlow’s browse data section Metrics plugin: zookeepr documentType: jmxStats, zookeeperStats Dashboard template: Zookeeper Logs Plugin: zookeeper documentType: zookeeper "},{"title":"Test Matrix​","type":1,"pageTitle":"Monitoring Apache ZooKeeper running on Instances","url":"docs/Integrations/zookeeper#test-matrix","content":"OS\tJDK versionubuntu 18.04 JDK 11 openjdk version &quot;11.0.11&quot; 2021-04-20 OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04) OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing) ubuntu 18.04 JDK 8 openjdk version &quot;1.8.0_292&quot; OpenJDK Runtime Environment (build 1.8.0_292-8u292-b10-0ubuntu1~18.04-b10) OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode) Centos 7 JDK 11 openjdk version &quot;11.0.12&quot; 2021-07-20 LTS OpenJDK Runtime Environment 18.9 (build 11.0.12+7-LTS) OpenJDK 64-Bit Server VM 18.9 (build 11.0.12+7-LTS, mixed mode, sharing) Centos 7 JDK 8 openjdk version &quot;1.8.0_302&quot; OpenJDK Runtime Environment (build 1.8.0_302-b08) OpenJDK 64-Bit Server VM (build 25.302-b08, mixed mode) "},{"title":"See Also​","type":1,"pageTitle":"Monitoring Apache ZooKeeper running on Instances","url":"docs/Integrations/zookeeper#see-also","content":"Kafka Elasticsearch Kafka-REST Kafka-Connect ActiveMQ "},{"title":"Custom Log Overview pane","type":0,"sectionRef":"#","url":"docs/Log_management/custom_log","content":"","keywords":""},{"title":"What is Custom Log Overview pane​","type":1,"pageTitle":"Custom Log Overview pane","url":"docs/Log_management/custom_log#what-is-custom-log-overview-pane","content":"Custom Log Overview is a built-in pane for providing a customized overview of the data present in the Logs. This pane contains 3 filters (Source, Log Type and Log Level), a line chart and a table by default. This pane is available under CustomLogOverview template. "},{"title":"Viewing Custom Log Overview pane​","type":1,"pageTitle":"Custom Log Overview pane","url":"docs/Log_management/custom_log#viewing-custom-log-overview-pane","content":"The custom Log Overview pane can be viewed in two ways Import the Custom Log Overview pane under Metrics/Logs tab from CustomLogOverview template Change the dashboard template to CustomLogOverview template note In case the CustomLogOverview template is not available, upgrade templates     "},{"title":"Features under Custom Log Overview pane​","type":1,"pageTitle":"Custom Log Overview pane","url":"docs/Log_management/custom_log#features-under-custom-log-overview-pane","content":""},{"title":"Edit (Rename) / Delete the Custom Log Pane​","type":1,"pageTitle":"Custom Log Overview pane","url":"docs/Log_management/custom_log#edit-rename--delete-the-custom-log-pane","content":"  "},{"title":"Add/ Edit/ Delete filters​","type":1,"pageTitle":"Custom Log Overview pane","url":"docs/Log_management/custom_log#add-edit-delete-filters","content":"In Custom Log Overview pane, the filters can be added/ edited/ deleted. The dependency of the filter will be automatically linked to the line chart and table component. note Maximum of 10 filters are allowed The dependency of other filter(s) should be explicitly mentioned in the filter definition    "},{"title":"Modify the Line chart/ Table queries​","type":1,"pageTitle":"Custom Log Overview pane","url":"docs/Log_management/custom_log#modify-the-line-chart-table-queries","content":"The line chart and the table queries can be modified by editing the respective components   "},{"title":"Modify the Table columns​","type":1,"pageTitle":"Custom Log Overview pane","url":"docs/Log_management/custom_log#modify-the-table-columns","content":"The select fields under the table query can be modified in order to modify the table columns   "},{"title":"Download Logs​","type":1,"pageTitle":"Custom Log Overview pane","url":"docs/Log_management/custom_log#download-logs","content":"The logs can be downloaded by specifying the number of Records and the fields required.  "},{"title":"Dropdown Filters​","type":1,"pageTitle":"Custom Log Overview pane","url":"docs/Log_management/custom_log#dropdown-filters","content":"By default, the dropdown list in any filter is limited to the top 50 records. To access specific records, use the dropdown filter to limit the results to the provided search string. To quickly access a particular record, use the search option available in the dropdown.  ​  "},{"title":"Custom Monitoring using StatsD","type":0,"sectionRef":"#","url":"docs/Integrations/statsd/custom_monitoring","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#overview","content":"StatsD is a popular standard for developing infrastructure and application plugins. A wide suite of standard plugins are available from Statsd community and can be accessed here sfAgent Statsd plugin integrates to Statsd client in the following way: Runs a daemon to listen to UDP port for data being sent by statsd client and accumulates all metrics being sent in the last N seconds (called flushinterval) Translates the data from statsd format to SnappyFlow’s format Forwards the data to SnappyFlow with necessary tags  "},{"title":"Prerequisites​","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#prerequisites","content":"Create a rules file for a statsd client or contact support@snappyflow.io to create the rules file for a specific statsd client.  "},{"title":"Configuration​","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#configuration","content":"User can also manually add the configuration shown below to config.yaml under /opt/sfagent/ directory key: &lt;profile_key&gt; tags: Name: &lt;name&gt; appName: &lt;app_name&gt; projectName: &lt;project_name&gt; metrics: plugins: - name: statsd enabled: true config: port: 8125 flushinterval: 30 ruleFile: /path/to/statsd-rules/file Copy port: The UDP port on which statsd client sends metrics. sfAgent runs a statsd server listening on this port for the UDP datagrams. Default value is 8125. flushInterval: SnappyFlow’s statsd plugin collects all the metrics received in the last N seconds and sends the data to SnappyFlow as a single document ruleFile: User generated statsd rules file path or please contact support@snappyflow.io to create a rule file for a specific statsd client. "},{"title":"Operating Instructions​","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#operating-instructions","content":"Validate the statsd configuration and the rules. It is mandatory to run this command after any change is made in the statsd rules file, followed by restarting the sfAgent service. sudo /opt/sfagent/sfagent -check-statsd Copy "},{"title":"Creating Rules File​","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#creating-rules-file","content":"statsd metrics are expected in the format shown below namespace.prefix.[type.]metric:value|metricType Copy Example ClusterA.Kafka1.Topic1.Lag:500|g Copy In this case, namespace= ClusterA, prefix= Kafka1, type= Topic1, metric= Lag, value= 500, metricType= g(gauge) Copy The field type is optional. If this field is present, it will enforce a nested json else the resulting json will be flat Example Kafka1.General.numTopic:5|g Copy In this case, namespace= Kafka1, prefix= General, metric= numTopic, value= 5, metricType= g (gauge) Copy namespace= Kafka1,prefix= General,metric= numTopic,value= 5,metricType= g (gauge) note In special cases where namespace is not present and the metrics start directly with prefix, set namespace: none. Supported datatypes are float, double, long, integer. "},{"title":"Rule to create nested json: \"NESTED\"​","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#rule-to-create-nested-json-nested","content":"Syntax &lt;json_key&gt; = NESTED(namespace: &lt;namespace&gt;, prefix: &lt;prefix_name&gt;, key: &lt;type_key&gt;, metric: [&lt;list of metrics along with datatypes&gt;]) Copy &lt;json_key&gt;: key of the final nested json. &lt;namespace&gt;: This rule is applied to all metrics having this namespace &lt;prefix&gt;: This rule is applied to all metrics having this prefix. &lt;key&gt;: adds a key:value pair in the nested json &lt;metric&gt;: Specify all the metrics to collect for this prefix. Example DB.host1.disk1.readLatency:20|g DB.host1.disk1.writeLatency:50|g Copy Rule latency = NESTED(namespace: DB, prefix: host1, key: diskName, metric:[readLatency:float, writeLatency:float]) Copy Output &quot;latency&quot;: [ { &quot;diskName&quot;: disk1, &quot;readLatency&quot;:20, &quot;writeLatency&quot;: 50 }, { &quot;diskName&quot;: disk2, &quot;readLatency&quot;:25, &quot;writeLatency&quot;: 45 } ] Copy "},{"title":"Rule to create flat json: \"FLAT\"​","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#rule-to-create-flat-json-flat","content":"Syntax &lt;json_key&gt; = FLAT(namespace: &lt;namespace&gt;, prefix: &lt;prefix_name&gt;, metric: &lt;metric_name&gt;) Copy &lt;namespace&gt;: This rule is applied to all metrics having this namespace &lt;prefix&gt;: This rule is applied to all metrics having this prefix. &lt;metric&gt;: Specify all the metrics to collect for this prefix. Example Kafka1.System.cpuutil:10|g, Kafka1.System.ramutil:20|g, Copy Rule computeMetrics = FLAT(namespace: Kafka1, prefix: System, metric: [cpuutil:float, ramutil:float]) Copy Output &quot;cpuutil&quot;: 10, “ramutil”:20 Copy "},{"title":"\"RENDER\" Rule:​","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#render-rule","content":"Extraction rules mentioned above, extract a set of metrics from statsd datagrams. These extracted metrics are grouped together in documents and shipped to SnappyFlow. Render rules describe grouping of metrics into documentType Syntax RENDER(_documentType: &lt;doctype&gt;, m1, m2,…mn) where m1..mn can be metric names or Rule names Copy Example RENDER(documentType: system, computeMetrics, latency) will create a documentType { plugin: statsd documentType: system &quot;cpuutil&quot;: 10, “ramutil”: 20 &quot;latency&quot;: [ { &quot;diskName&quot;: disk1, &quot;readLatency&quot;:20, &quot;writeLatency&quot;: 50 }, { “diskName”: disk2, “readLatency”:25, “writeLatency”: 45 } ] } Copy "},{"title":"Tagging​","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#tagging","content":"sfAgent statsD plugin is capable of parsing and forwarding the tags contained in the statsd metric datagrams. Tags are expressed in different formats based on the intended destination being Datadog, Influx or Graphite. Add TAGTYPE rule in the statsd rules file to enable the parsing. Default TAGTYPE is None i.e. no custom tags present. In each of the formats below, the tags are recognized and passed forward into SnappyFlow documents TAGTYPE = Datadog Sample metric: Cluster1.kafka1.cpuUtil:35|c|#_tag_appName:testApp1,_tag_projectName:apmProject,_documentType:cpuStats Copy TAGTYPE = Influx Sample metric: Cluster1.Kafka1.cpuUtil,_tag_appName=testApp1,_tag_projectName=apmProject,_documentType=cpuStats:35|c Copy TAGTYPE = Graphite Sample metric: Cluster1.Kafka1.cpuUtil;_tag_appName=testApp1;_tag_projectName=apmProject;_documentType=cpuStats:35|c Copy "},{"title":"Sidekiq Use-case​","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#sidekiq-use-case","content":"This section shows to monitor sidekiq using statsd with sfAgent. "},{"title":"Description​","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#description","content":"We will use a simple ruby on rails application which shows endangered sharks’ data. There are two sidekiq worker configured, one to add the data and another to remove the sharks data named as AddEndangeredWorker and RemoveEndangeredWorker respectively. Sidekiq statsd client is also configured to get the metrics. For this example, sidekiq-statsd by phstc is used as the client.  "},{"title":"Installation​","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#installation","content":"Skip this part if the statsd client is already configured. Follow this documentation to setup the ruby on rails application, if neededTo add the statsd client: Create a new file sidekiq.rb under config/initializers/ and add the configuration specified here. Install the [sidekiq-statsd gem](https://github.com/phstc/sidekiq-statsd&quot; /l &quot;installation) and run the application.  "},{"title":"Sample Metrics​","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#sample-metrics","content":"Metrics are generated upon worker activation in the application. Add endangered worker metrics production.worker.AddEndangeredWorker.processing_time:1113|ms production.worker.AddEndangeredWorker.success:1|c production.worker.enqueued:0|g production.worker.retry_set_size:0|g production.worker.processed:69|g production.worker.failed:0|g production.worker.queues.default.enqueued:0|g production.worker.queues.default.latency:0|g Copy Remove endangered worker metrics production.worker.RemoveEndangeredWorker.processing_time:1472|ms production.worker.RemoveEndangeredWorker.success:1|c production.worker.enqueued:0|g production.worker.retry_set_size:0|g production.worker.processed:107|g production.worker.failed:0|g production.worker.queues.default.enqueued:0|g production.worker.queues.default.latency:0|g Copy "},{"title":"Rules​","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#rules","content":"Follow the Rules User Guide section to understand the rules. TAGTYPE = None worker = NESTED(namespace: production, prefix: worker, key: worker_name, metric:[processing_time:double, success:float]) queues = NESTED(namespace: production, prefix: worker.queues, key: queue_name, metric:[enqueued:float, latency:float]) processedJobs = FLAT(namespace: production, prefix: worker, metric: processed:integer) RENDER(_documentType: sidekiq, worker, queues, processedJobs) Copy "},{"title":"sfAgent Configuration​","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#sfagent-configuration","content":"Content of the /opt/sfagent/config.yaml. The rules file is /opt/sfagent/statsd-rules.txt key: &lt;profile_key&gt; tags: Name: &lt;instance-name&gt; appName: &lt;app-name&gt; projectName: &lt;project-name&gt; metrics: plugins: - name: statsd enabled: true config: port: 8125 flushInterval: 10 ruleFile: '/opt/sfagent/statsd-rules.txt' Copy Output { &quot;_documentType&quot;: &quot;sidekiq&quot;, &quot;_tag_Name&quot;: &quot;vm&quot;, &quot;queues&quot;: [ { &quot;latency&quot;: 0, &quot;queue_name&quot;: &quot;default&quot;, &quot;enqueued&quot;: 0 } ], &quot;_plugin&quot;: &quot;statsD&quot;, &quot;processedJobs&quot;: 107, &quot;worker&quot;: [ { &quot;processing_time&quot;: 1472, &quot;worker_name&quot;: &quot;RemoveEndangeredWorker&quot;, &quot;success&quot;: 1 }, { &quot;processing_time&quot;: 1113, &quot;worker_name&quot;: &quot;AddEndangeredWorker&quot;, &quot;success&quot;: 1 } ], &quot;_tag_projectName&quot;: &quot;statsDProject&quot;, &quot;_tag_uuid&quot;: &quot;080027957dd8&quot;, &quot;time&quot;: 1616132931981, &quot;_tag_appName&quot;: &quot;statsDApp&quot; } Copy "},{"title":"See Also​","type":1,"pageTitle":"Custom Monitoring using StatsD","url":"docs/Integrations/statsd/custom_monitoring#see-also","content":"Linux monitoring LSOF NETSTAT Prometheus Integration "},{"title":"Archival","type":0,"sectionRef":"#","url":"docs/Log_management/archival","content":"","keywords":""},{"title":"Coming Soon!​","type":1,"pageTitle":"Archival","url":"docs/Log_management/archival#coming-soon","content":""},{"title":"TripWire","type":0,"sectionRef":"#","url":"docs/Integrations/tripwire","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"TripWire","url":"docs/Integrations/tripwire#overview","content":"Tripwire Metric Plugin parses reports generated by Tripwire Intrusion Detection System. "},{"title":"Prerequisites​","type":1,"pageTitle":"TripWire","url":"docs/Integrations/tripwire#prerequisites","content":"Tripwire plugin supported only for ubuntu and centos distributions. For other platforms, reach out to support@snappyflow.io Tripwire plugin requires Tripwire package to be installed For installation of Tripwire package Centos Distributions refer centos Ubuntu distributions refer ubuntu "},{"title":"Configuration Settings​","type":1,"pageTitle":"TripWire","url":"docs/Integrations/tripwire#configuration-settings","content":"Refer to sfAgent section for steps to install and configure sfagent. Tripwire plugin configurations can be automatically generated by sfagent or added manually. To manually add the configuration, update config.yaml under /opt/sfagent directory metrics: metrics: plugins: - name: tripwire enabled: true interval: 300 config: report_path: /var/lib/tripwire/report Copy If Tripwire reports need to be triggered automatically once in the day, you can add a cron job for it. To add a Cronjob,  crontab -e Copy add, 0 0 * * * tripwire --check Copy Once you save the file, a cron job gets scheduled automatically. if you want to run the Cronjob at a specific time everyday at say 11:30 AM, use following command  30 11 * * * tripwire --check Copy To trigger a Tripwire report at any point in time, use the following command tripwire --check Copy "},{"title":"Documents​","type":1,"pageTitle":"TripWire","url":"docs/Integrations/tripwire#documents","content":"It consists of three document types tripwireReportSummary : contains information about report creation date, user who created, policy used , command used for triggering reporttripwireRuleSummary : contains information related to list of rules added for monitoring and its severity, count of added, removed and modified filestripwireObjectSummary : contains detailed information about list of added, removed and modified files per rules Use the built-in TripWire dashboard for data visualization. "},{"title":"Tripwire summary pane​","type":1,"pageTitle":"TripWire","url":"docs/Integrations/tripwire#tripwire-summary-pane","content":" "},{"title":"Tripwire object details pane​","type":1,"pageTitle":"TripWire","url":"docs/Integrations/tripwire#tripwire-object-details-pane","content":" "},{"title":"Viewing data and dashboards​","type":1,"pageTitle":"TripWire","url":"docs/Integrations/tripwire#viewing-data-and-dashboards","content":"Data collected by plugins can be viewed in SnappyFlow’s browse data section under metrics section plugin: TripWire documentType: tripwireReportSummary, tripwireRuleSummary, tripwireObjectSummary Dashboard template: TripWire For help with plugins, please reach out to support@snappyflow.io. "},{"title":"sfPoller Setup","type":0,"sectionRef":"#","url":"docs/New_Pages/sfpoller_setup","content":"sfPoller Setup Coming Soon!","keywords":""},{"title":"Log Signatures","type":0,"sectionRef":"#","url":"docs/Log_management/log_signatures","content":"","keywords":""},{"title":"Coming Soon!​","type":1,"pageTitle":"Log Signatures","url":"docs/Log_management/log_signatures#coming-soon","content":""},{"title":"Getting Started","type":0,"sectionRef":"#","url":"docs/Quick_Start/getting_started","content":"","keywords":""},{"title":"Setup SnappyFlow Account​","type":1,"pageTitle":"Getting Started","url":"docs/Quick_Start/getting_started#setup-snappyflow-account","content":"Go to www.snappyflow.io Register for a free trial. A demo account will be created with a pre-configured sample application Request an upgrade to Full Trial by clicking on the link provided in the top bar. You will get an email stating “your trial environment is ready” once SnappyFlow team approves your trial request.  "},{"title":"Setup Self-Hosted SnappyFlow​","type":1,"pageTitle":"Getting Started","url":"docs/Quick_Start/getting_started#setup-self-hosted-snappyflow","content":"SnappyFlow can be deployed in Your on-prem data center AWS cloud Azure cloud Deployment can be automated using templates and scripts provided by SnappyFlow SnappyFlow self-hosted version is available in two flavors Ingest rates below 500 GB/Day Ingest rates above 500 GB/Day For seeting up SnappyFlow in AWS or Azure, click here For ingest rates above 500 GB/Day, please reach out to support@snappyflow.io. A support engineer will understand your data ingest rates and provide an appropriately sized solution "},{"title":"Important terminologies and concepts​","type":1,"pageTitle":"Getting Started","url":"docs/Quick_Start/getting_started#important-terminologies-and-concepts","content":"sfAgent sfPoller sfPod sfKubeAgent Profile Key Tagging Approach "},{"title":"sfAgent​","type":1,"pageTitle":"Getting Started","url":"docs/Quick_Start/getting_started#sfagent","content":"Monitoring of applications running on VM or bare-metal requires installation of a lightweight agent called sfAgent. sfAgent provides following features: Discovery of servicesAuto-recommendation of monitoring configuration based on discovered servicesMonitoring of various services based on specified configurations Log parsing and collectionOrchestration of tracing (check out sfTracing for details) Installation procedures For sfAgent on Linux For sfAgent on Windows "},{"title":"sfPoller​","type":1,"pageTitle":"Getting Started","url":"docs/Quick_Start/getting_started#sfpoller","content":"sfPoller is a poller appliance installed within user’s cloud account. It can be used to Monitor cloud services such as RDS, ELB, Lamba, ECS, Azure App Service etc. Monitor Databases Perform Synthetic Monitoring of APIs using postman like collections Stream logs from applications to sfPoller, apply parsing rules and forward logs to SnappyFlow. Procedure for sfPoller setup "},{"title":"sfPod​","type":1,"pageTitle":"Getting Started","url":"docs/Quick_Start/getting_started#sfpod","content":"Daemon set installed on Kubernetes cluster and monitors the following elements: Host, Pod &amp; Container metrics Resources such as deployments, Daemon Sets etc. Kubernetes core services metrics Cluster logs Monitor Prometheus exporters running on any of the application pods  Procedure for sfPod setup "},{"title":"sfKubeAgent​","type":1,"pageTitle":"Getting Started","url":"docs/Quick_Start/getting_started#sfkubeagent","content":"sfAgent equivalent and installed as a side-car container within a Kubernetes pod and can be configured to monitor metrics and logs of other containers running on pods. Procedure for setting up sfKubeAgent "},{"title":"Profile Key​","type":1,"pageTitle":"Getting Started","url":"docs/Quick_Start/getting_started#profile-key","content":"Every user account has a unique system generated profile key. Data sent by collectors to SnappyFlow need to have the correct profile key and tags to be allowed into SnappyFlow. This key has to be copied by the user and pasted into the configuration file of sfAgent or within sfPoller’s UI "},{"title":"Tagging Approach​","type":1,"pageTitle":"Getting Started","url":"docs/Quick_Start/getting_started#tagging-approach","content":"SnappyFlow mandates that all end-points should be assigned two tags - _tag_projectName and _tag_appName. These tags have to be added to configuration files of sfAgent or within sfPoller’s UI. Pls see the video that explains how end-points should be organized hierarchically in SnappyFlow and how tags should be assigned  "},{"title":"Let's Start Monitoring​","type":1,"pageTitle":"Getting Started","url":"docs/Quick_Start/getting_started#lets-start-monitoring","content":"Try out one of the simple exercises to familiarize yourself with the product Monitor a Linux instance​ Monitor a Kubernetes Cluster​ Monitor a Windows instance​ Trace an application​ "},{"title":"sfPoller Setup","type":0,"sectionRef":"#","url":"docs/Quick_Start/sfpoller_setup","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"sfPoller Setup","url":"docs/Quick_Start/sfpoller_setup#overview","content":"sfPoller is a poller appliance installed within user’s cloud account. It can be used to Monitor cloud services such as RDS, ELB, Lamba, ECS, Azure App Service etc.Monitor DatabasesPerform Synthetic Monitoring of APIs using postman like collectionsStream logs from applications to sfPoller, apply parsing rules and forward logs to SnappyFlow. "},{"title":"Setting up sfPoller​","type":1,"pageTitle":"sfPoller Setup","url":"docs/Quick_Start/sfpoller_setup#setting-up-sfpoller","content":"The video below explains setting up sfPoller along with two use cases Monitor a MySQL databaseMonitor a Postgres database  "},{"title":"Steps to install SnappyFlow RUM agent - Others","type":0,"sectionRef":"#","url":"docs/RUM/agent_installation/others","content":"","keywords":""},{"title":"Step 1: Install the sf-apm-rum agent​","type":1,"pageTitle":"Steps to install SnappyFlow RUM agent - Others","url":"docs/RUM/agent_installation/others#step-1-install-the-sf-apm-rum-agent","content":"cd to the project directory and run the below command $ npm install --save sf-apm-rum Copy "},{"title":"Step 2: Import the sf-apm-rum package​","type":1,"pageTitle":"Steps to install SnappyFlow RUM agent - Others","url":"docs/RUM/agent_installation/others#step-2-import-the-sf-apm-rum-package","content":"const sfApm = require('sf-apm-rum'); Copy "},{"title":"Step 3: Configure the sf-apm-rum agent​","type":1,"pageTitle":"Steps to install SnappyFlow RUM agent - Others","url":"docs/RUM/agent_installation/others#step-3-configure-the-sf-apm-rum-agent","content":"Add the following code in the applications root component.  let apmRum = new sfApm.ApmRum(); // initialize the library const apmData = { baseUrl: '&lt;add-snappyflow-server-url-here&gt;', // provide the URL of the snappyflow APM server that you are using to view the data profileKey: '&lt;add-profile-key-here&gt;', // paste the profile key copied from SF profile serviceName: '&lt;your-apm-service-name&gt;', // specify service name for RUM projectName: '&lt;add-project-name-here&gt;', // provide the snappyflow project name appName: '&lt;add-application-name-here&gt;', // provide the snappyflow application name }; apmRum.init(apmData); Copy "},{"title":"Step 4: Verify the setup​","type":1,"pageTitle":"Steps to install SnappyFlow RUM agent - Others","url":"docs/RUM/agent_installation/others#step-4-verify-the-setup","content":"Once the above mentioned steps are completed, restart the application and check for the RUM data in the Snappyflow APM server. For viewing RUM data in snappyflow server, make sure the project and application is created or discovered with project name and app name specified in the Step 3. Once application is available in the Snappyflow Server, Click on View dashboard -&gt; Click on Real User Monitoring Tab on left side bar -&gt; Go to Real Time Pane "},{"title":"Step 5: Debugging (In case of No Data in RUM Dashboard)​","type":1,"pageTitle":"Steps to install SnappyFlow RUM agent - Others","url":"docs/RUM/agent_installation/others#step-5-debugging-in-case-of-no-data-in-rum-dashboard","content":"i. Check if data is available on the Snappyflow server​ Navigate to the application dashboard -&gt; Click on Browse Data -&gt; Change the Index to &quot;Real User Monitoring&quot;. Check if the data is available. If the data is available, it will be visible on the RUM Dashboard within few seconds. ii. Check if the RUM data is sent from the configured application​ Open the Developer tools for the configured web application on the browser -&gt; Click on the Network Tab -&gt; Trigger some actions in the application. Check if there is a intake/v2/rum/events call fired from the configured application side. If this call is made, it means that the data is being sent to the snappyflow server. iii. Check if the configurations are correct​ Check if the projectName and appName provided in the Step 3 are matching the project name and application name in the snappyflow server. "},{"title":"Setting up SnappyFlow in your own environment","type":0,"sectionRef":"#","url":"docs/Quick_Start/snappyflow_self_hosted","content":"","keywords":""},{"title":"What you need to get started​","type":1,"pageTitle":"Setting up SnappyFlow in your own environment","url":"docs/Quick_Start/snappyflow_self_hosted#what-you-need-to-get-started","content":"A SnappyFlow account. An account can be created here. An approximate idea of your monthly ingest rates. The more accurate your data is, the better. This data is useful to ensure right sizing of your infrastructure "},{"title":"Size your infrastructure using the sizing tool​","type":1,"pageTitle":"Setting up SnappyFlow in your own environment","url":"docs/Quick_Start/snappyflow_self_hosted#size-your-infrastructure-using-the-sizing-tool","content":"SnappyFlow providesHead to accounts.snappyflow.io and use your SnappyFlow credentials to login. Once logged in, click on Pricing Calculator. Choose your cloud platform and select the region where you want to deploy SnappyFlow. The region can be changed at a later stage too.  "},{"title":"Total ingest rate​","type":1,"pageTitle":"Setting up SnappyFlow in your own environment","url":"docs/Quick_Start/snappyflow_self_hosted#total-ingest-rate","content":"In the configuration page, enter the total ingest rate for your stack. This ingest rate is the daily average sum of all logs, metrics and traces. The tool automatically assumes a breakup between logs, metrics and traces and you can also manually adjust this breakup to match your stack needs or leave it at its default value.  "},{"title":"Data retention​","type":1,"pageTitle":"Setting up SnappyFlow in your own environment","url":"docs/Quick_Start/snappyflow_self_hosted#data-retention","content":"The next input on data retention defines how long the ingested data is retained in a high-performance storage. A very high data retention can significantly increase storage requirements and costs.  "},{"title":"Extended backup​","type":1,"pageTitle":"Setting up SnappyFlow in your own environment","url":"docs/Quick_Start/snappyflow_self_hosted#extended-backup","content":"Using extended backup is one way to reduce infrastructure costs. All metrics and logs stored in the backup can easily be retrieved for easy visualizations. This backup is available only for metrics and logs. Any ingested log or metric that ages beyond the primary data retention period defined in the previous section is backed up in a low cost storage service.  "},{"title":"Backup for SnappyFlow account data​","type":1,"pageTitle":"Setting up SnappyFlow in your own environment","url":"docs/Quick_Start/snappyflow_self_hosted#backup-for-snappyflow-account-data","content":"It is recommended to create a backup for SnappyFlow account data. This account data is stored in a dedicated database. This helps in quick recovery in case of cloud infrastructure failures. This database stores all account and configuration information.  Click on Calculate Price button to continue. "},{"title":"Cost summary​","type":1,"pageTitle":"Setting up SnappyFlow in your own environment","url":"docs/Quick_Start/snappyflow_self_hosted#cost-summary","content":"A summary of the infrastructure required and their cost is provided on the right. By default, a 1 year reserved instance pricing is taken. The tool also provides a list of alternate servers that can be choose either for better performance or for optimizing costs.  "},{"title":"Dowload template to create SnappyFlow stack​","type":1,"pageTitle":"Setting up SnappyFlow in your own environment","url":"docs/Quick_Start/snappyflow_self_hosted#dowload-template-to-create-snappyflow-stack","content":"Click on the Download button at the end of the infrastructure summary to get a template depending on the choice of cloud platform selected. For AWS, a Cloud Formation Template is provided. For Azure, a Custom Template is provided. These templates are pre-loaded with the required stack information such as server types, quantities and help you quickly launch a SnappyFlow stack. "},{"title":"Create SnappyFlow stack on AWS​","type":1,"pageTitle":"Setting up SnappyFlow in your own environment","url":"docs/Quick_Start/snappyflow_self_hosted#create-snappyflow-stack-on-aws","content":""},{"title":"Pre-Requisites​","type":1,"pageTitle":"Setting up SnappyFlow in your own environment","url":"docs/Quick_Start/snappyflow_self_hosted#pre-requisites","content":"AWS accountPre-configured IAM roles with appropriate access levels "},{"title":"Uploading the template​","type":1,"pageTitle":"Setting up SnappyFlow in your own environment","url":"docs/Quick_Start/snappyflow_self_hosted#uploading-the-template","content":"Sign into your AWS console. Search for Cloudformation using the search bar on top and select the CloudFormation service.  Click on Create Stack and select With new resources option.  Select Template is ready and upload the template provided by SnappyFlow sizing tool and click Next.  "},{"title":"Configuration​","type":1,"pageTitle":"Setting up SnappyFlow in your own environment","url":"docs/Quick_Start/snappyflow_self_hosted#configuration","content":"In this page, provide a stack name and configure subnets and IAM Roles required for the stack.  Deployment Choose Recommended Database Type If a backup for SnappyFlow account is required, choose 'RDS'. Else select 'Local' Private Subnet APM and Opensearch instances will be created under selected Subnets. If RDS is selected as DatabaseType, add two subnets with same VPC and different availability zones, as RDS creation needs to cover all availability zones in the region. APM and Opensearch instances will be created with first subnet(first selection in checkbox) !!! note If DatabaseType is chosen as RDS, provide two Private Subnets. Without two separate subnets, stack creation will fail. Public Subnet Loadbalancer and bastion will be created under this Subnet. Select public subnet of availibility zone same as first PrivateSubnetIds (first selection in checkbox) of same VPC. For example: If a private subnet with availability zone &quot;a&quot; is selected, a public subnet of availability zone &quot;a&quot; should be added under same VPC, so that the Loadbalancer can forward request to target group of instances. Allowed IP Provide a list of IP addresses which can access SnappyFlow server KeyName Provide an existing key for SSH access to SnappyFlow instances IAMRole Provide an existing IAM Role with appropriate permissions. This is required to discover end points and create a S3 storage required for logs and metric backup. Click on Next to continue. In the Configure stack options screen, provide tags and verify all permissions and other stack creation options. Click on Next to continue. Review all configuration settings and click on `Create Stack' to create SnappyFlow stack.  Once the stack is successfully created, it should appear under the list of stacks in CloudFormation service.  "},{"title":"Accessing SnappyFlow server​","type":1,"pageTitle":"Setting up SnappyFlow in your own environment","url":"docs/Quick_Start/snappyflow_self_hosted#accessing-snappyflow-server","content":"Click on the stack and select Outputs tab  SnappyFLow portal can be accessed using the server URL provided here. The default username is admin and password is the instance id.  SnappyFlow will complete the installation automatically. This process takes upto 30 minutes.  "},{"title":"Create SnappyFlow stack on Azure​","type":1,"pageTitle":"Setting up SnappyFlow in your own environment","url":"docs/Quick_Start/snappyflow_self_hosted#create-snappyflow-stack-on-azure","content":"caution Before downloading the template for Azure, make sure your region has enough quotas for the server types provided in the sizing tool summary. If not, change server type in the sizing tool using drop down option. "},{"title":"Pre-Requisites​","type":1,"pageTitle":"Setting up SnappyFlow in your own environment","url":"docs/Quick_Start/snappyflow_self_hosted#pre-requisites-1","content":"Azure accountPre-configured subnetsAzure blob storage  "},{"title":"Uploading the template​","type":1,"pageTitle":"Setting up SnappyFlow in your own environment","url":"docs/Quick_Start/snappyflow_self_hosted#uploading-the-template-1","content":"Log into Azure portal and Search for Custom Deployment using the search bar on top and select Deploy a custom template.  Click on Build your own template in the editor  You will now see an editor and here you can Click on Load file to upload the template provided by SnappyFlow sizing tool.  Click on Save to continue. "},{"title":"Configuration​","type":1,"pageTitle":"Setting up SnappyFlow in your own environment","url":"docs/Quick_Start/snappyflow_self_hosted#configuration-1","content":"In this page, choose a subscription and resource group that this new stack will be a part of.  The region is auto populated from template. Provide the Virtual Network Name and Subnet name that you’d like this stack to be part of. If access to SnappyFlow is required via open internet, choose true in Public Ip addr field. You can also define the IP addresses that can access the stack or leave it at default for access via internet. Choose True for System Assigned Identity. To enable SSH access to the stack, you can choose from an existing Admin Username and provide the admin key. Provide a name and key for the backup storage. Click on Review + Create to proceed. Review all stack details and click on Create. Azure will validate all inputs and start the deployment process.  "},{"title":"Accessing SnappyFlow server​","type":1,"pageTitle":"Setting up SnappyFlow in your own environment","url":"docs/Quick_Start/snappyflow_self_hosted#accessing-snappyflow-server-1","content":"Once deployment is complete, you will see a deployment complete message.  To access SnappyFlow portal, select the SnappyFlow server VM. You can find this virtual machine under the resource group or subscriptions that was earlier selected.  Use the public IP address to access the portal. The default username is admin and password is admin. SnappyFlow will complete the installation automatically. This process takes upto 30 minutes.  "},{"title":"Log Onboarding","type":0,"sectionRef":"#","url":"docs/Log_management/log_overview","content":"","keywords":""},{"title":"Document format in SnappyFlow​","type":1,"pageTitle":"Log Onboarding","url":"docs/Log_management/log_overview#document-format-in-snappyflow","content":"SnappyFlow stores all information in JSON format to the datastore. A sample log document is shown below: &quot;node&quot;: &quot;ip-172-31-14-187&quot;, &quot;_plugin&quot;: &quot;linux-syslog&quot;, &quot;ident&quot;: &quot;sshd&quot;, &quot;_tag_Name&quot;: &quot;demo-presto-worker-0&quot;, &quot;level&quot;: &quot;info&quot;, &quot;@timestamp&quot;: &quot;2020-10-15T18:35:13.000000000Z&quot;, &quot;time&quot;: &quot;1602786913000&quot;, &quot;pid&quot;: &quot;14153&quot;, &quot;_documentType&quot;: &quot;syslog&quot;, &quot;host&quot;: &quot;ip-172-31-14-187&quot;, &quot;_tag_uuid&quot;: &quot;0aa46894f321&quot;, &quot;_tag_projectName&quot;: &quot;presto&quot;, &quot;file&quot;: &quot;/var/log/auth.log&quot;, &quot;signatureKey&quot;: &quot;8276318930445510094&quot;, &quot;_tag_appName&quot;: &quot;presto&quot;, &quot;message&quot;: &quot;Invalid user ofandino from 152.32.180.15 port 56712&quot; Copy "},{"title":"Types of search​","type":1,"pageTitle":"Log Onboarding","url":"docs/Log_management/log_overview#types-of-search","content":"Searching across all fields and valuesRange queries for numeric dataWildcard searchRegular expression searchLogical operations like AND, OR, NOT to build complex searches SnappyFlow datastore, processes fields which contain string values differently. The string is stored as a list of tokens. Each token is a unique word in the string. For example if the field “message” has a value “user:admin CMD=rm –rf temp PWD=/home/admin PATH=var.log.secure”. Copy This string is converted as a list of tokens as follows: “user:admin CMD rm rf temp PWD home admin PATH var.log.secure”. Copy Note that in the above tokenization, character “:” and character “.” Are treated differently. They are considered as alpha-numeric, for the purpose of tokenization and are retained, if they are preceded and succeeded by alpha-numeric characters between the “:”. A search of string user\\:admin will be successful in the above document. Note that “:” was escaped using “\\” in the search string, as “:” is considered a reserved character. See below for more on reserved characters. Also note, that a search string temp home will also match the above string, as the words temp and home are present in the above string, even though they do not appear in consecutive positions. A phrase search “temp home” (the search string is encapsulated between “), will match only if temp and home appear together in the string and are in the same order. "},{"title":"SnappyFlow Query language operator support​","type":1,"pageTitle":"Log Onboarding","url":"docs/Log_management/log_overview#snappyflow-query-language-operator-support","content":"Operator\tDescription\tExample\tExplanation:\tSearch for a value within a field\tlevel:info\tGet all documents where field “level” has value “info” &amp;&amp;\tAND operation\tinfo &amp;&amp; ident:sshd\tGet all documents where value “info” is present in any of the fields AND “ident” field has value “sshd” ||\tOR operation\tlevel:warn || level:error\tGet all documents where “level” field has value “warn” or “level” field has value “error” &quot;&quot;\tPhrase searches\tmessage: &quot;Invalid\tGet all documents where “message” field has a phrase “Invalid user”. Note: searches are case insensitive. “Invalid user” will match only if token “Invalid” and token “user” are present in the string in the same order. &gt;\tGreater than\tpid:&gt;14153\tGet all documents where field “pid” has values greater than 14153 &lt;\tLesser than\tpid:&lt;14153\tGet all documents where field “pid” has values less than 14153 &gt;=\tGreater than or equal\tpid:&gt;=14153\tGet all documents where field “pid” has values greater than or equal to 14153 &lt;=\tLesser than or equal\tpid:&lt;14153\tGet all documents where field “pid” has values less than or equal to 14153 ()\tGrouping\t(pid:(&gt;14000 &amp;&amp; &lt;=15000) || level:error) &amp;&amp; ident:sshd\tGet all documents where field “pid” is in the range 14000 – 14999 OR field “level” has value “error”. From the above search get only those documents where field “ident” has value “sshd” -\tNOT operation\tlevel:-(info || warn)\tGet all documents where field “level” does not contain value “info” or “warn” ?\tSingle character wildcard\t_plugin: sys???\tGet all documents where the field plugin has a word sys followed by 3 characters. *\tZero or more characters wildcard\tmessage: var\tGet all documents where “message” field contains a string var preceded by any characters and succeeded by any characters. For example in the message “user:admin CMD=rm –rf temp PWD=/home/admin PATH=var.log.secure”, var matches var.log.secure //\tPattern searches\tmessage: /[0-9]+.[0-9]+.[0-9]+.[0-9]+/\tGet all documents which contain an IP address pattern. In the sample log document with the message field containing &quot;Invalid user ofandino from 152.32.180.15 port 56712&quot; , the regex pattern will match 152.32.180.15. \\ Escape sequence\tmessage: sudo\\:linux\tSome of the special characters need to be escaped if they are part of a search string. Special characters to be escaped are: &amp; | &quot; = : ( ) [ ] - ? * / \\ exists:\tField name search\texists:pid\tGet all logs Note: Field names are case sensitive i.e. latency: 20 and LATENCY: 20 will give different results. Field values are case insensitive i.e. name: KEVIN and name: kevin will give the same results. Applying range queries i.e. key: &gt;=200 etc. to text fields give unpredictable results. Make sure to apply such queries on numeric fields only. Range queries cannot be used without specifying the field name i.e. &gt;=20 is not a valid query. Wildcards cannot be used in phrase searches i.e. &quot;*error&quot; or &quot;er??r&quot; is not allowed. Using a wildcard at the beginning of a word e.g. *ing is particularly heavy, because all terms in the index need to be examined, just in case they match. Regex patterns must be enclosed in forward slashes. Any string present between a pair of forward slashes will be treated as a Java regex pattern. Search Regex does not support all regex meta-characters. For details, https://www.elastic.co/guide/en/elasticsearch/reference/current/regexp-syntax.htmlPatterns are anchored by default i.e. they must match an entire Elasticsearch token. "},{"title":"Examples​","type":1,"pageTitle":"Log Onboarding","url":"docs/Log_management/log_overview#examples","content":""},{"title":"Basic Search​","type":1,"pageTitle":"Log Onboarding","url":"docs/Log_management/log_overview#basic-search","content":"Datastore has following documents { &quot;pid&quot;: 3245, &quot;upstream_response_time&quot;: 10, &quot;URL&quot;: &quot;https://www.elastic.co/guide/en/elasticsearch/reference&quot;} {&quot;pid&quot;: 2445, &quot;upstream_response_time&quot;: 4, &quot;URL&quot;: &quot;https://www.elastic.co/guide/en/machine-learning&quot; } {&quot;pid&quot;: 3246, &quot;upstream_response_time&quot;: 2, &quot;URL&quot;: &quot;https://docker-hub/pricing&quot;} {&quot;message&quot;: &quot;docker image built&quot;, &quot;pid&quot;: 1000} Copy Search Query &amp; Logic\tResult\tResults explainedpid: 3?4?\tMatches documents 1, 3.\tGet all documents with pid field value matching the pattern 3?4? (? matches any character) upstream_response_time:&gt;5 &amp;&amp; elasticsearch\tMatches document 1\tGet all documents where field upstream_response_time key has a value greater than 5 AND the string elasticsearch is present in any of the fields. elastic &amp;&amp; machine-learning\tNo documents are matched.\tGet all documents where strings elastic AND machine-learning are present in any of the fields. Though string elastic is present in documents 1, 2; it does not appear as a standalone term. This is because, special character “.” is handled differently in tokenization and is tokenized as www.elastic.co. If the search query is modified as www.elastic.co &amp;&amp; machine-learning, document 2 will match the search. Alternatively, search elasstic &amp;&amp; machine-learning, will also return the same result https docker hub pricing\tMatches documents 3 and 4.\tGet all documents which contain the words https OR docker OR hub or pricing in any order. Matches documents 3 and 4. Document 3 has all the terms and Document 4 has the term docker. If the intent is to search for a document with all the terms in the same order, then the search should be modified to “https docker hub pricing”. Note the phrase is enclosed in double quotes. This search will match only document 3. Also note the words http docker hub pricing are connected with special characters in document 3. But the search is on the tokenized version of the document and hence all special characters are removed. "},{"title":"Logical Operations and wild card usage​","type":1,"pageTitle":"Log Onboarding","url":"docs/Log_management/log_overview#logical-operations-and-wild-card-usage","content":"Datastore contains following documents {&quot;message&quot;: &quot;Disconnected from 118.24.197.243 port 35662 [preauth]&quot;} {&quot;message&quot;: &quot;Unregistered Authentication Agent for unix-session:7 (system bus name :1.89, object path /org/freedesktop/PolicyKit1/AuthenticationAgent, locale en_IN) (disconnected from bus)&quot;} {&quot;responseCode&quot;: &quot;400&quot;, &quot;responseMessage&quot;: Null} {&quot;message&quot;: &quot;request received from IP1 and redirected to IP2&quot;, &quot;responseCode&quot;: &quot;200&quot;} {&quot;message&quot;: &quot;ValueError(…)&quot;} {&quot;message&quot;: &quot;ArithmeticException(…)&quot;} Copy Examples Search Query:&quot;disconnected from&quot; Get all documents that contain the terms disconnected and from. The terms should appear together in the same order in the document. Results and explanation:Matches documents 1 and 2. Notice that in document 1, the word disconnected appears as Disconnected. Since search is always case-insensitive, document 1 is also matched. Search Query:message: (disconnected &amp;&amp; from &amp;&amp; port) Get all documents that contain the words disconnected and from and port Results and explanation:Matches document 1 Note: words need not appear together and they may appear in any order. Search Query:message: (disconnect* port) Get all documents that contain word starting with disconnect or a word port. Results and explanation:Matches documents 1 and 2. This is interpreted asmessage: (disconnect || port) disconnect matches all terms which start with the word disconnect and have zero or more characters after it i.e. disconnecting, disconnected and disconnect Search Query:message: (disconnected &amp;&amp; -port) Get all documents that has term disconnected and does not have the term port Results and explanation:Matches document 2 -(responseCode: 400 || message: (exception || error)) 2 This is a complete negation of the above search i.e. NOT operator is applied to above search Search Query:responseCode: 400 || message: (exception || error) Results and explanation:Matches 3, 5 and 6 exception matches any word that contains the string exception and similarly error. The term ArithmeticException(...) matches exception and ValueError(...) matches error Search Query:-(responseCode: 400 || message: (exception || error)) This search is a total negation of the previous search. Results and explanation:Matches document 4 "},{"title":"Regex Patterns​","type":1,"pageTitle":"Log Onboarding","url":"docs/Log_management/log_overview#regex-patterns","content":"Datastore contains following documents {&quot;message&quot;: &quot;No identification string for 118.24.197.243&quot;} {&quot;message&quot;: &quot;No identification string for 119:25.200.255&quot;} {&quot;message&quot;: &quot;Received bad request from 119:25.200.255&quot;} {&quot;message&quot;: &quot;pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=203.195.182.3&quot;} {&quot;message&quot;: &quot;Authentication failure for user admin&quot;} Copy Examples Search Query: message: /[0-9]+.[0-9]+.[0-9]+.[0-9]+/ Copy Get all documents where message field contains an IP address pattern.Results and explanation: Matches documents 1,2,3,4 Search Query: (message: /119.25.[0-9]+.[0-9]+/) Copy Get all documents where message field contains an IP Address pattern with a network address 119.25 Results and explanation: Matches document 2 and 3 auth* &amp;&amp; failure &amp;&amp;-/[0-9]+.[0-9]+.[0-9]+.[0-9]+/ 5 Any key’s value needs to consist of auth*, failure but not an IP i.e. [0-9]+.[0-9]+.[0-9]+.[0- 9]+ Copy Search Query: auth* &amp;&amp; failure &amp;&amp; -/[0-9]+.[0-9]+.[0-9]+.[0-9]+/ Copy Get all documents where an IP address pattern is NOT present in any of the fields and contains a word starting with auth in any of the fields AND contains the word failure in any of the fields. Results and explanation: Matches documents 5 "},{"title":"Steps to install SnappyFlow RUM agent - Nextjs","type":0,"sectionRef":"#","url":"docs/RUM/agent_installation/nextjs","content":"","keywords":""},{"title":"Step 1: Install the sf-apm-rum agent​","type":1,"pageTitle":"Steps to install SnappyFlow RUM agent - Nextjs","url":"docs/RUM/agent_installation/nextjs#step-1-install-the-sf-apm-rum-agent","content":"cd to the project directory and run the below command $ npm install --save sf-apm-rum Copy "},{"title":"Step 2: Import the sf-apm-rum package​","type":1,"pageTitle":"Steps to install SnappyFlow RUM agent - Nextjs","url":"docs/RUM/agent_installation/nextjs#step-2-import-the-sf-apm-rum-package","content":"note Since the library requires Web APIs to work, which are not available when Next.js pre-renders the page on the server-side, we have to use dynamic import here  useEffect(() =&gt; { const initFunction = async () =&gt; { const sfApm = await import('sf-apm-rum') // Add Step 3 code here } initFunction() }, []) Copy "},{"title":"Step 3: Configure the sf-apm-rum agent​","type":1,"pageTitle":"Steps to install SnappyFlow RUM agent - Nextjs","url":"docs/RUM/agent_installation/nextjs#step-3-configure-the-sf-apm-rum-agent","content":"Add the following code in the applications root component, usually in pages/_app.js or _app.tsx if you are using typescript let apmRum = new sfApm.ApmRum(); // initialize the library const apmData = { baseUrl: '&lt;add-snappyflow-server-url-here&gt;', // provide the URL of the snappyflow APM server that you are using to view the data profileKey: '&lt;add-profile-key-here&gt;', // paste the profile key copied from SF profile serviceName: '&lt;your-apm-service-name&gt;', // specify service name for RUM projectName: '&lt;add-project-name-here&gt;', // provide the snappyflow project name appName: '&lt;add-application-name-here&gt;', // provide the snappyflow application name }; apmRum.init(apmData); Copy "},{"title":"Step 4: Verify the setup​","type":1,"pageTitle":"Steps to install SnappyFlow RUM agent - Nextjs","url":"docs/RUM/agent_installation/nextjs#step-4-verify-the-setup","content":"Once the above mentioned steps are completed, restart the application and check for the RUM data in the Snappyflow APM server. For viewing RUM data in snappyflow server, make sure the project and application is created or discovered with project name and app name specified in the Step 3. Once application is available in the Snappyflow Server, Click on View dashboard -&gt; Click on Real User Monitoring Tab on left side bar -&gt; Go to Real Time Pane "},{"title":"Step 5: Debugging (In case of No Data in RUM Dashboard)​","type":1,"pageTitle":"Steps to install SnappyFlow RUM agent - Nextjs","url":"docs/RUM/agent_installation/nextjs#step-5-debugging-in-case-of-no-data-in-rum-dashboard","content":"i. Check if data is available on the Snappyflow server​ Navigate to the application dashboard -&gt; Click on Browse Data -&gt; Change the Index to &quot;Real User Monitoring&quot;. Check if the data is available. If the data is available, it will be visible on the RUM Dashboard within few seconds. ii. Check if the RUM data is sent from the configured application​ Open the Developer tools for the configured web application on the browser -&gt; Click on the Network Tab -&gt; Trigger some actions in the application. Check if there is a intake/v2/rum/events call fired from the configured application side. If this call is made, it means that the data is being sent to the snappyflow server. iii. Check if the configurations are correct​ Check if the projectName and appName provided in the Step 3 are matching the project name and application name in the snappyflow server. "},{"title":"Agent Installation","type":0,"sectionRef":"#","url":"docs/RUM/RUM_agent_installation","content":"","keywords":""},{"title":"The sf-apm-rum agent can be installed in the following javascript based applications​","type":1,"pageTitle":"Agent Installation","url":"docs/RUM/RUM_agent_installation#the-sf-apm-rum-agent-can-be-installed-in-the-following-javascript-based-applications","content":"Angular React Nextjs Others "},{"title":"Steps to install SnappyFlow RUM agent - Angular","type":0,"sectionRef":"#","url":"docs/RUM/agent_installation/angular","content":"","keywords":""},{"title":"Step 1: Install the sf-apm-rum agent​","type":1,"pageTitle":"Steps to install SnappyFlow RUM agent - Angular","url":"docs/RUM/agent_installation/angular#step-1-install-the-sf-apm-rum-agent","content":"cd to the project directory and run the below command $ npm install --save sf-apm-rum Copy "},{"title":"Step 2: Import the sf-apm-rum package​","type":1,"pageTitle":"Steps to install SnappyFlow RUM agent - Angular","url":"docs/RUM/agent_installation/angular#step-2-import-the-sf-apm-rum-package","content":"Add following path in angular.json under scripts 'node_modules/sf-apm-rum/dist/sf-apm-rum.js' Copy Eg:scripts: ['node_modules/sf-apm-rum/dist/sf-apm-rum.js'] "},{"title":"Step 3: Configure the error handler​","type":1,"pageTitle":"Steps to install SnappyFlow RUM agent - Angular","url":"docs/RUM/agent_installation/angular#step-3-configure-the-error-handler","content":"Create a new file apm-error.handler.ts in the add following code import { ErrorHandler } from &quot;@angular/core&quot;; declare const sfApm: any; export class ApmErrorHandler extends ErrorHandler { constructor() { super() } handleError(error) { sfApm.apm.captureError(error.originalError || error) super.handleError(error) } } Copy Then in app.module.ts add, import { ErrorHandler, NgModule } from '@angular/core'; import { ApmErrorHandler } from './apm.error-handler'; Copy under imports add, providers: [ {provide: ErrorHandler, useClass: ApmErrorHandler} ] Copy "},{"title":"Step 4: Configure the sf-apm-rum agent​","type":1,"pageTitle":"Steps to install SnappyFlow RUM agent - Angular","url":"docs/RUM/agent_installation/angular#step-4-configure-the-sf-apm-rum-agent","content":"Add the following code in the applications root component, usually in app.component.ts declare const sfApm: any; // add it outside class let apmRum = new sfApm.ApmRum(); // initialize the library const apmData = { baseUrl: '&lt;add-snappyflow-server-url-here&gt;', // provide the URL of the snappyflow APM server that you are using to view the data profileKey: '&lt;add-profile-key-here&gt;', // paste the profile key copied from SF profile serviceName: '&lt;your-apm-service-name&gt;', // specify service name for RUM projectName: '&lt;add-project-name-here&gt;', // provide the snappyflow project name appName: '&lt;add-application-name-here&gt;', // provide the snappyflow application name }; apmRum.init(apmData); Copy "},{"title":"Step 5: Verify the setup​","type":1,"pageTitle":"Steps to install SnappyFlow RUM agent - Angular","url":"docs/RUM/agent_installation/angular#step-5-verify-the-setup","content":"Once the above mentioned steps are completed, restart the application and check for the RUM data in the Snappyflow APM server. For viewing RUM data in snappyflow server, make sure the project and application is created or discovered with project name and app name specified in the Step 3. Once application is available in the Snappyflow Server, Click on View dashboard -&gt; Click on Real User Monitoring Tab on left side bar -&gt; Go to Real Time Pane "},{"title":"Step 6: Debugging (In case of No Data in RUM Dashboard)​","type":1,"pageTitle":"Steps to install SnappyFlow RUM agent - Angular","url":"docs/RUM/agent_installation/angular#step-6-debugging-in-case-of-no-data-in-rum-dashboard","content":"i. Check if data is available on the Snappyflow server​ Navigate to the application dashboard -&gt; Click on Browse Data -&gt; Change the Index to &quot;Real User Monitoring&quot;. Check if the data is available. If the data is available, it will be visible on the RUM Dashboard within few seconds. ii. Check if the RUM data is sent from the configured application​ Open the Developer tools for the configured web application on the browser -&gt; Click on the Network Tab -&gt; Trigger some actions in the application. Check if there is a intake/v2/rum/events call fired from the configured application side. If this call is made, it means that the data is being sent to the snappyflow server. iii. Check if the configurations are correct​ Check if the projectName and appName provided in the Step 4 are matching the project name and application name in the snappyflow server. "},{"title":"Steps to install SnappyFlow RUM agent - React","type":0,"sectionRef":"#","url":"docs/RUM/agent_installation/react","content":"","keywords":""},{"title":"Step 1: Install the sf-apm-rum agent​","type":1,"pageTitle":"Steps to install SnappyFlow RUM agent - React","url":"docs/RUM/agent_installation/react#step-1-install-the-sf-apm-rum-agent","content":"cd to the project directory and run the below command $ npm install --save sf-apm-rum Copy "},{"title":"Step 2: Import the sf-apm-rum package​","type":1,"pageTitle":"Steps to install SnappyFlow RUM agent - React","url":"docs/RUM/agent_installation/react#step-2-import-the-sf-apm-rum-package","content":"import * as sfApm from 'sf-apm-rum'; Copy "},{"title":"Step 3: Configure the sf-apm-rum agent​","type":1,"pageTitle":"Steps to install SnappyFlow RUM agent - React","url":"docs/RUM/agent_installation/react#step-3-configure-the-sf-apm-rum-agent","content":"Add the following code in the applications root component, usually in index.js or index.tsx if you are using typescript let apmRum = new sfApm.ApmRum(); // initialize the library const apmData = { baseUrl: '&lt;add-snappyflow-server-url-here&gt;', // provide the URL of the snappyflow APM server that you are using to view the data profileKey: '&lt;add-profile-key-here&gt;', // paste the profile key copied from SF profile serviceName: '&lt;your-apm-service-name&gt;', // specify service name for RUM projectName: '&lt;add-project-name-here&gt;', // provide the snappyflow project name appName: '&lt;add-application-name-here&gt;', // provide the snappyflow application name }; apmRum.init(apmData); Copy "},{"title":"Step 4: Verify the setup​","type":1,"pageTitle":"Steps to install SnappyFlow RUM agent - React","url":"docs/RUM/agent_installation/react#step-4-verify-the-setup","content":"Once the above mentioned steps are completed, restart the application and check for the RUM data in the Snappyflow APM server. For viewing RUM data in snappyflow server, make sure the project and application is created or discovered with project name and app name specified in the Step 3. Once application is available in the Snappyflow Server, Click on View dashboard -&gt; Click on Real User Monitoring Tab on left side bar -&gt; Go to Real Time Pane "},{"title":"Step 5: Debugging (In case of No Data in RUM Dashboard)​","type":1,"pageTitle":"Steps to install SnappyFlow RUM agent - React","url":"docs/RUM/agent_installation/react#step-5-debugging-in-case-of-no-data-in-rum-dashboard","content":"i. Check if data is available on the Snappyflow server​ Navigate to the application dashboard -&gt; Click on Browse Data -&gt; Change the Index to &quot;Real User Monitoring&quot;. Check if the data is available. If the data is available, it will be visible on the RUM Dashboard within few seconds. ii. Check if the RUM data is sent from the configured application​ Open the Developer tools for the configured web application on the browser -&gt; Click on the Network Tab -&gt; Trigger some actions in the application. Check if there is a intake/v2/rum/events call fired from the configured application side. If this call is made, it means that the data is being sent to the snappyflow server. iii. Check if the configurations are correct​ Check if the projectName and appName provided in the Step 3 are matching the project name and application name in the snappyflow server. "},{"title":"RUM Dashboard","type":0,"sectionRef":"#","url":"docs/RUM/RUM_Dashboard","content":"","keywords":""},{"title":"Summary Pane​","type":1,"pageTitle":"RUM Dashboard","url":"docs/RUM/RUM_Dashboard#summary-pane","content":"This pane provides the overview of the data captured. This view is helpful in understanding the statistics of the usage of the application. It helps the user to know the Apdex Rating, Count of the pages used, Average Response time, Number of Transactions occured, Number of Errors, Browser and event breakups which helps the user to understand the current usage of the application. It helps the user to get the information about which type of event is predominant and the most used browser etc. The Apdex Rating helps the user understand the user satisifaction for a target response time of 500 ms. Below is the snip of the Summary pane for a typical application.  "},{"title":"Pages Pane​","type":1,"pageTitle":"RUM Dashboard","url":"docs/RUM/RUM_Dashboard#pages-pane","content":"This pane provides the page wise statistics of the application. It provides the information such as top 10 slow pages, transaction wise and error wise breakup for each page. It also provides Average Response time, Number of transactions and errors occured, Transaction Rate and Error Rate for each of the pages. This information helps the user to know about the performance, errors and usage of the pages using which the performance of the pages can be improved and the errors can be fixed. Below is the snip of the Pages pane for a typical application.  "},{"title":"Real Time Pane​","type":1,"pageTitle":"RUM Dashboard","url":"docs/RUM/RUM_Dashboard#real-time-pane","content":"This pane provides the realtime usage data of the application. It provides the statistics such as event type, duration, page name, browser name and origin IP of the particular real time transaction. This pane also has a Trace view that provides the detailed view of each of the transaction. Using the Flame Graph subpane under the trace view, the user will be able to see the step wise breakup of the events/actions occuring in the transaction. This helps the user identify the rootcause of the slowness issues or the errors. Below are the snips of the Real Time pane and Trace View for a typical application.   "},{"title":"Real User Monitoring","type":0,"sectionRef":"#","url":"docs/RUM/RUM_documentation","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Real User Monitoring","url":"docs/RUM/RUM_documentation#overview","content":"Real User Monitoring (RUM) is used for performance analysis of client(Browser) side component of an application. It involves recording end-user interactions with a website or a client interacting with a server or cloud-based application. It allows website/application owners to visualize the sequence of actions and events that take place during an interaction. Metrics such as Response time, Transactions, Errors, Event information, Browser information are monitored and visualized to help identify and troubleshoot website performance issues. Below are the screenshots of RUM Dashboard.     "},{"title":"Supported Technologies​","type":1,"pageTitle":"Real User Monitoring","url":"docs/RUM/RUM_documentation#supported-technologies","content":"All the applications developed using javascript or javascript based frameworks such as Angular, React, Vue etc are supported. "},{"title":"Setting up Real User Monitoring (RUM) in SnappyFlow​","type":1,"pageTitle":"Real User Monitoring","url":"docs/RUM/RUM_documentation#setting-up-real-user-monitoring-rum-in-snappyflow","content":"Snappyflow provides a lightweight and powerful sf-apm-rum agent that can be installed in your web application to monitor user integrations in real time. Steps to install RUM agent "},{"title":"Performance Metrics​","type":1,"pageTitle":"Real User Monitoring","url":"docs/RUM/RUM_documentation#performance-metrics","content":"Below are the various performance metrics that are captured by the sf-apm-rum agent Apdex RatingPage load metricsUser interactionsUser Centric MetricsJavaScript errors "},{"title":"RUM Glossary","type":0,"sectionRef":"#","url":"docs/RUM/RUM_Glossary","content":"","keywords":""},{"title":"E​","type":1,"pageTitle":"RUM Glossary","url":"docs/RUM/RUM_Glossary#e","content":""},{"title":"Error​","type":1,"pageTitle":"RUM Glossary","url":"docs/RUM/RUM_Glossary#error","content":"An event that deviates from the normal flow of the application which results in an issue. "},{"title":"Error Rate​","type":1,"pageTitle":"RUM Glossary","url":"docs/RUM/RUM_Glossary#error-rate","content":"Number of errors that occur per minute. "},{"title":"Event​","type":1,"pageTitle":"RUM Glossary","url":"docs/RUM/RUM_Glossary#event","content":"A single unit of information, containing a timestamp plus additional data. "},{"title":"F​","type":1,"pageTitle":"RUM Glossary","url":"docs/RUM/RUM_Glossary#f","content":""},{"title":"Flame Graph​","type":1,"pageTitle":"RUM Glossary","url":"docs/RUM/RUM_Glossary#flame-graph","content":"A diagram that depicts the stepwise breakup of the event and time spent in each of those events "},{"title":"R​","type":1,"pageTitle":"RUM Glossary","url":"docs/RUM/RUM_Glossary#r","content":""},{"title":"Response Time​","type":1,"pageTitle":"RUM Glossary","url":"docs/RUM/RUM_Glossary#response-time","content":"The duration of time spent by the system between the start and end of an event. "},{"title":"S​","type":1,"pageTitle":"RUM Glossary","url":"docs/RUM/RUM_Glossary#s","content":""},{"title":"Slow Page​","type":1,"pageTitle":"RUM Glossary","url":"docs/RUM/RUM_Glossary#slow-page","content":"Any Page which has relatively higher Response time. "},{"title":"Span​","type":1,"pageTitle":"RUM Glossary","url":"docs/RUM/RUM_Glossary#span","content":"Information about the execution of a specific code path. Spans measure from the start to the end of an activity and can have a parent/child relationship with other spans. "},{"title":"T​","type":1,"pageTitle":"RUM Glossary","url":"docs/RUM/RUM_Glossary#t","content":""},{"title":"Target Response Time​","type":1,"pageTitle":"RUM Glossary","url":"docs/RUM/RUM_Glossary#target-response-time","content":"The ideal Response time within which the response is expected from a system "},{"title":"Trace​","type":1,"pageTitle":"RUM Glossary","url":"docs/RUM/RUM_Glossary#trace","content":"Defines the amount of time an application spends on a request. Traces are made up of a collection of transactions and spans that have a common root. "},{"title":"Transaction​","type":1,"pageTitle":"RUM Glossary","url":"docs/RUM/RUM_Glossary#transaction","content":"A special kind of span that has additional attributes associated with it. Transactions describe an event captured by the sf-apm-rum instrumenting a service. "},{"title":"Transaction Rate​","type":1,"pageTitle":"RUM Glossary","url":"docs/RUM/RUM_Glossary#transaction-rate","content":"Number of transactions that occur per minute. "},{"title":"Feature Extraction","type":0,"sectionRef":"#","url":"docs/Log_management/feature_extraction","content":"","keywords":""},{"title":"Examples​","type":1,"pageTitle":"Feature Extraction","url":"docs/Log_management/feature_extraction#examples","content":"Example 1 EV(message, /\\d+\\.\\d+\\.\\d+\\.\\d+/, string, ip_addr) Copy Extract IP address from field message. {&quot;message&quot;: &quot;DHCPACK from 172.31.32.1 (xid=0x381e913e)&quot;,“ip_addr”: “172.31.32.1”} {&quot;message&quot;: &quot;DHCPREQUEST on eth0 to 172.31.32.1 port 67 (xid=0x381e913e)&quot;, “ip_addr”:“172.31.32.1”} {&quot;message&quot;: &quot;Received disconnect from 167.114.226.137 port 47545:11: [preauth]&quot;, “ip_addr”:“167.114.226.137”} {&quot;message&quot;: &quot;Disconnected from 167.71.217.175 port 46180 [preauth]&quot;,“ip_addr”: “167.71.217.175”} {&quot;message&quot;: &quot;Received disconnect from 51.91.159.46 port 33914:11: [preauth]&quot;,“ip_addr”:“51.91.159.46”} Copy Example 2 EV(log_msg, /\\d+(.\\d+)*(?=ms)/, float, delay) Copy Extract delay values from the field “log_msg”. Delay value is identified by the pattern (a) one digit string, immediately followed by string “ms” OR (b) two digit strings, each separated by “.” and second digit string is immediately followed by string “ms”. Save extracted value in field named delay. {&quot; log_msg&quot;: &quot;Received request from 10.11.100.29 and re-directed to 33.229.79.17 in 10.34ms&quot;, “delay”: 10.34} {&quot;log_msg&quot;: &quot;Latency time is 5ms&quot;,“delay”: 5} {&quot;log_msg&quot;: &quot;Process 2131 completed in 50s&quot;} – *nothing is extracted from this message* Copy Example 3 (&quot;received request from&quot; &amp;&amp; &quot;re-directed&quot;) with EV(log_msg, /\\d+.\\d+.\\d+.\\d+/, string, -,rd_ip_addr) Extract IP addresses from log_msg field. Skip the first extraction and save the second extraction to field named rd_ip_addr. {&quot;log_msg&quot;: &quot;Received request from 10.11.100.29 and re-directed to 33.229.79.17 in 10.34ms&quot;,“rd_ip_addr”: &quot;33.229.79.17”} Copy "},{"title":"Extract Group​","type":1,"pageTitle":"Feature Extraction","url":"docs/Log_management/feature_extraction#extract-group","content":"Extract Group (EG) construct allow users to specify a pattern with multiple groups and extract the value of each group into a separate field. In Extract Value a single pattern contained a single group, but this single group could match multiple values in the field. This concept will be better understood through examples. EG construct uses, following parameters: Parameter\tUsefield\tfield name to which the extraction is applied, in this case message field of the log pattern\tREGEX pattern which identifies the extraction groups. Pattern is enclosed in a pair of “/”. In the REGEX pattern the group to be extracted is enclosed in parenthesis “()”. name:type\ta comma separated list of name:type tuples which specify name of the extracted group and its type. Type can be int, float or string. If no type is specified, it is assumed to be string. Example 1 EG(message, /(\\d+.\\d+.\\d+.\\d+) - \\[(.*)\\] &quot;(\\w+) ([^\\s]+) ([^\\s&quot;]+)&quot; (\\d+) (\\d+) &quot;-&quot; &quot;(.*)&quot; &quot;-&quot;/, host, httpd_timestamp, method, path, header, code:int, size:int, agent) Copy The example illustrates extracting information from an nginx access log, which contains the host sending the request, time at which the request was received, HTTP method, request Path, header version, response code, size and the agent making request. The response code, and size are integer values, whereas the other extractions are string type values. Patterns corresponding to these groups are highlighted in the above EG construct. Note: the group REGEX patterns are enclosed in “()”, strings matching each of those patterns are extracted and placed in the field name provided in the same order they appear. { &quot;message&quot;: &quot;172.31.31.45 - [06/May/2020:11:44:41] \\&quot;GET /owners/2 HTTP/1.1\\&quot; 200 4964 \\&quot;-\\&quot; \\&quot;Apache-HttpClient/4.5.7 (Java/1.8.0_242)\\&quot; \\&quot;-\\&quot; rt=14.717 uct=0.000 uht=14.716 urt=14.716&quot;, &quot;host&quot;: “172.31.31.45”, &quot;httpd_timestamp&quot;: &quot;06/May/2020:11:44:41&quot;, &quot;method&quot;: &quot;GET&quot;, &quot;path&quot;: &quot;/owners/2&quot;, &quot;header&quot;: &quot;HTTP/1.1&quot;, &quot;code&quot;: 302, &quot;size&quot;: 4964, &quot;agent&quot;: &quot;Apache-HttpClient/4.5.7 (Java/1.8.0_242)&quot; } { &quot;message&quot;: &quot;172.31.81.81 - [06/May/2020:11:44:41] \\&quot;POST /owners/new HTTP/1.1\\&quot; 201 24 \\&quot;-\\&quot; \\&quot;Mozilla/5.0 (Windows NT 10.0; WOW64)\\&quot; \\&quot;-\\&quot; rt=1.088 uct=0.000 uht=1.088 urt=1.088&quot;, &quot;host&quot;: “172.31.81.81”, &quot;httpd_timestamp&quot;: &quot;06/May/2020:11:44:41&quot;, &quot;method&quot;: &quot;POST&quot;, &quot;path&quot;: &quot;/owners/new&quot;, &quot;header&quot;: &quot;HTTP/1.1&quot;, &quot;code&quot;: 201, &quot;size&quot;: 24, &quot;agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; WOW64)&quot; } Copy Example 2 EG(message, /TTY=\\w+ ; PWD=([^\\s]+) ; USER=(\\w+) ; COMMAND=(.*)$/, path, username, cmd) Copy { &quot;message&quot;: &quot;root : TTY=unknown ; PWD=/home/centos ; USER=root ; COMMAND=/bin/rm –rf jmeter.log&quot;, &quot;pwd&quot;: &quot;/home/centos&quot;, &quot;username&quot;: &quot;root&quot;, &quot;cmd&quot;: &quot;/bin/rm -rf jmeter.log&quot; } { &quot;message&quot;: &quot;centos : TTY=unknown ; PWD=/home/kevin ; USER=adam ; COMMAND=/bin/rm - rf jmeter.log &quot;, &quot;pwd&quot;: &quot;/home/kevin&quot;, &quot;username&quot;: &quot;adam&quot;, &quot;cmd&quot;: &quot;/bin/rm -rf jmeter.log&quot; } Copy "},{"title":"Extract Pair​","type":1,"pageTitle":"Feature Extraction","url":"docs/Log_management/feature_extraction#extract-pair","content":"Applications often use logs to dump their internal statistics for debugging purposes. Most often, such logs will contain expressions like: &lt;name1&gt;:&lt;value1&gt;, &lt;name2&gt;:&lt;value2&gt;, ….. Copy OR &lt;name1&gt;=&lt;value1&gt;; &lt;name2&gt;=&lt;value2&gt;;…. Copy In such logs, the field-value pairs can be identified with a value separator and a pair delimiter. In the first example value separator is “:” and pair delimiter is “,”; whereas in the second example, value separator is “=” and pair delimiter is “;”. General syntax used in Extract Pairs is: EP(field, /pair_delimiter/, /value_separator/, convert=[..], include=[..], exclude=[..]) Parameter\tUsefield\tfield name to which the extraction is applied, example: *message* field of the log pair_delimiter\tREGEX pattern which identifies the field-value pair delimiter. Pattern is enclosed in a pair of “/”. value_separator\tREGEX pattern which identifies the separator between field and value. Pattern is enclosed in a pair of “/”. convert=[]\ta list of field:type tuples, which specify how to convert the field values. If nothing is specified, all fields are considered have a string type value. Note: the field names are the names extracted from the log message itself. Example is convert=(field1:int,field2:float). This is an optional field, all values are converted to string type, if not specified. include=[]\tif only selected fields from the extraction are to be added to the log document, list those field names. Note: include and exclude lists can not both be present at the same time. An optional field and can be omitted. exclude=[]\tif selected fields from the extraction are not to be added to the log document, list those field names. Note: include and exclude lists can not both be present at the same time. An optional field and can be omitted. Example 1 EP(message, /,/, /=/, convert=[price:int]) Copy From the field message, extract field-value pairs where pair_delimiter is “,” and value separator is “=”. Convert the value for field “price” to integer. { &quot;message&quot;: &quot;name=Kevin,user_id=3212,order_id=234,price=240.56&quot;, &quot;name&quot;: &quot;Kevin&quot;, &quot;user_id&quot;: &quot;3212&quot;, &quot;order_id&quot;: &quot;234&quot;, &quot;price&quot;: 241 } { &quot;message&quot;: &quot;name=larry,user_id=1111,order_id=100,price=123&quot;, &quot;name&quot;: &quot;larry&quot;, &quot;user_id&quot;: &quot;1111&quot;, &quot;order_id&quot;: &quot;100&quot;, &quot;price&quot;: 123 } { &quot;message&quot;: &quot;process completed in 20s, and details=HEXA3413&quot;, &quot;and details&quot;: &quot;HEXA3413&quot; } Copy Note: in the above example Field “price”, if present is converted to Integer. In the 3 rd message field, “price” is not presentIn the 3 rd message, notice that the field name is taken as “and details”. This is because the pair delimiter was specified as “,” and anything between “,” (pair delimiter) and “=” (value separator) is taken as a field name Example 2 EP(message, /,|(.*,\\s+and)/, /=/, exclude=[name], convert=[price:float, order_id]) Copy From the field message, extract field-value pairs delimited by text matching the pattern /,|(.*,\\s+and)/ and has a value separator “=”. Note that the pair delimiter has a REGEX pattern which defines multiple delimiters. Each delimiter is separated by “|”. Options specified here are: “,” – comma OR “.*,\\s+and’ - a string with any number of characters(.*), followed by a “,” followed by any number of white space characters(\\s+) and followed by the string “and”. Copy Following log messages will illustrate the use of this extraction { &quot;message&quot;: &quot;name=Kevin,user_id=3212,order_id=234,price=240.56&quot;, &quot;user_id&quot;: &quot;3212&quot;, &quot;order_id&quot;: 234, &quot;price&quot;: 240.56 } { &quot;message&quot;: &quot;name=larry,user_id=1111,order_id=100,price=123&quot;, &quot;user_id&quot;: &quot;1111&quot;, &quot;order_id&quot;: 100, &quot;price&quot;: 123 } { &quot;message&quot;: &quot;process completed in 20s, and details=HEXA3413&quot;, &quot;details&quot;: &quot;HEXA3413&quot; } Copy In the 1 st and 2 nd example messages, pair delimiter used was “,”.In the 3 rd example message, the pair delimiter is “process completed in 20s, and”Also note, in messages 1 and 2, the field “name” was extracted, but was not added to the output document, because of the “exclude” option. Example 3 message: (user_id &amp;&amp; price) with EP(message, /,/, /=/, include=[price]) Copy { &quot;message&quot;: &quot;name=Kevin,user_id=3212,order_id=234,price=240.56&quot;, &quot;price&quot;: “240.56” } { &quot;message&quot;: &quot;name=larry,user_id=1111,order_id=100,price=123&quot;, &quot;price&quot;: “123” } { &quot;message&quot;: &quot;process completed in 20s, and details=HEXA3413&quot; } Copy Since include option had the field “price”, only those documents where field “price” was found were updated. 3 rd document though matched the delimiter and separator, did not have the field “price”. Example 4 EP(message, /(.*PID.*?(?=\\w+=))|(\\})|(\\{)|(\\s(?=\\w+=))/, /=/, exclude=[ startTime, endTime], convert=[count, minimum: float, mean: float, maximum: float]) Copy Another example using a complex delimiter, with exclude option and convert options. Multiple pair delimiters are used. { &quot;message&quot;: &quot;StatisticsLogTask - PID1 - context=Execution Fill {subContext=Order Update section=Top Level startTime=2019-12-17 23:59 endTime=2019-12-20 00:00} count=3 minimum=1.0 mean=5.0 maximum=21.0&quot;, &quot;context&quot;: &quot;Execution Fill&quot;, &quot;subContext&quot;: &quot;Order Update&quot;, &quot;section&quot;: &quot;Top Level&quot;, &quot;count&quot;: 3, &quot;minimum&quot;: 1, &quot;maximum&quot;: 21, &quot;mean&quot;: 5 } { &quot;message&quot;: &quot;StatisticsLogTask - PID2 - context=Execution Fill {subContext=Order Placed section=Mid Level startTime=2019-12-17 23:59 endTime=2019-12-20 00:00 count=6 minimum=0.813 mean=7.9 maximum=13.0}&quot;, &quot;context&quot;: &quot;Execution Fill&quot;, &quot;subContext&quot;: &quot;Order Placed&quot;, &quot;section&quot;: &quot;Mid Level&quot;, &quot;count&quot;: 6, &quot;minimum&quot;: 0.813, &quot;maximum&quot;: 13, &quot;mean&quot;: 7.9 } Copy In the extraction, multiple pair delimiters were specified /(.*PID.*?(?=\\w+=))|(\\})|(\\{)|(\\s(?=\\w+=))/ Copy StatisticsLogTask - PID1 - context=Execution Fill (subContext=Order Update section=Top Level startTime=2019-12-17 23:59 endTime=2019-12-20 00:00} count=3 minimum=1.0 mean=5.0 maximum=21.0 StatisticsLogTask - PID2 - context=Execution Fill (subContext=Order Place section=Mid Level startTime=2019-12-17 23:59 endTime=2019-12-20 00:00} count=6 minimum=0.813 mean=7.3 maximum=13.0 Value pairs identified by the pair delimiter pattern is shown in the above picture. Though startTime and endTime are extracted, but they are excluded. "},{"title":"Tools and Tips​","type":1,"pageTitle":"Feature Extraction","url":"docs/Log_management/feature_extraction#tools-and-tips","content":"Java REGEX testing tools can be used to validate the REGEX patterns used for extractions. A web tool like https://www.freeformatter.com/java-regex-tester.html can be used for this. Pattern used in extract values (EV) should match the sub-strings you intend to extract. For example, to extract the timestamps from the following message, a pattern like below can be used /\\d+-\\d+- \\d+\\s+\\d+:\\d+/ Copy Extract Value Pattern \\d+-\\d+-\\d+\\s+\\d+:\\d+ Copy String: StatisticsLogTask - PID1 - context=Execution Fill {subContext=Order Update section=Top Level startTime=2019-12-17 23:59 endTime=2019-12-20 00:00} count=3 minimum=1.0 mean=5.0 maximum=21.0 Copy StatisticsLogTask - PID2 - Context = Execution Fill (subContext=Order Place section=Mid Level startTime=2019-12-17 23:59 endTime=2019-12-20 00:00 count=6 minimum=0.813 mean=7.9 maximum=13.0 Copy Match #\tGroup index\tStart index\tEnd index\tGroup content1\t0\t103\t119\t2019-12-17 23:59 2\t0\t128\t144\t2019-12-20 00:00 Extract Group allows to apply a REGEX to the field and extract only the groups identified (i.e. patterns enclosed in parentheses). As an example the following group REGEX when applied to the string below will result in the following groups. Extract group pattern (\\d+\\.\\d+\\.\\d+\\.\\d+) - - \\[[^\\]]* \\+\\d+\\] \\\\\\\\\\\\\\&quot;POST \\/topics\\/(\\w+)-(\\w+) HTTP/1.0\\\\\\\\\\\\\\&quot;(\\d+) (\\d+) (\\d+) Copy String: 10.233.117.0 - - [26/Oct/2020:22:30:00 +0000] \\\\\\&quot;POST /topics/metric-grqqwwi7 HTTP/1.0\\\\\\&quot;200 12602 12 (io.confluent.rest-utils.requests) Copy Copy the pattern and string to the java-regex-tester. Observe the resultant matches and groups. In Extract Group, the values extracted in non-zero group index are used. 10.233.177.0 - - [26/Oct/2020:22:30:00 +0000] \\\\\\&quot;Post /topica/metric-grqqwwi7 HTTP/1.0\\\\\\&quot; 200 12602 12(io.confluent.reat-utile.requests) Copy Match #\tGroup index\tStart index\tEnd index\tGroup content1\t0\t0\t106\t10.233.117.0--[26/Oct/2000:22:30:00 +0000] \\\\&quot;POST /topics/metric-grqqwwi7 HTTP/1.0\\\\* 200 1260212 1\t1\t0\t12\t10.233.117.0 1\t2\t63\t69\tmetric 1\t3\t70\t78\tgrqqwwi7 1\t4\t92\t95\t200 1\t5\t96\t101\t12602 1\t6\t103\t106\t12 To extract value pairs, pair-delimiter should match the boundaries and all other symbols which are needed to split the string into an array of pairs. Each pair is again split, based on value- separator pattern SnappyFlow provides following built-in regex patterns, these built-in patterns can be used in place of a REGEX required in EV, EP or EG feature extraction constructs. Built-in pattern names are encapsulated in “$” $hostname$ - hostname string, e.g.apmmanager.snappyflow.io $url$ - URL, e.g. https://apmmanager.snappyflow.io $file_path$ - UNIX file path, e.g. /usr/share/nginx/html/theme-chrome.js $float$ - floating point number, e.g.31.45 $integer$ - integer number, e.g. 19345 $alphanumeric$ - alpha-numeric characters, e.g. admin1 $alphanumericspecial$ - alpha-numeric with hyphen and underscore, e.g. date_time $string$ - a string encapsulated in ‘ (single quote) or “ (double quote) $date$ - date string, e.g. 02-04-2020 $datetime$ - date with time string, e.g. 02-04-20 21:41:59 $time$ - time string, e.g. 21:41:59 $ipv4addr$ - IPv4 Address, e.g. 172.31.22.98 Copy Example extraction with built-in regex patterns message: &quot;*72281 open() &quot;/usr/share/nginx/html/theme-chrome.js&quot; failed (2: No such file or directory), client: 172.31.22.98, server: _, request: &quot;GET /theme-chrome.js HTTP/1.1&quot;, host: &quot;apmmanager.snappyflow.io&quot;, referrer: “https://apmmanager.snappyflow.io/” Extraction message:&quot;No such file or directory&quot; with EG(message,/($integer$) open\\(\\) \\&quot;($file_path$)\\&quot; failed \\(2: No such file or directory\\), client: ($ipv4addr$), server: ([^,]*), request: \\&quot;([^\\&quot;]*)\\&quot;, host: \\&quot;($hostname$)\\&quot;, referrer: \\&quot;($url$)\\&quot;/,m_size:int, m_file, m_client, m_server, m_request, m_host, m_referrer) The extraction operation above will extract the fields m_size: 72281, m_file: /usr/share/nginx/html/theme-chrome.js, m_client: 172.31.22.98, m_server: _, m_request: GET /theme-chrome.js HTTP/1.1, m_host: apmmanager.snappyflow.io and m_referrer: https://apmmanager.snappyflow.io/ Copy "},{"title":"SnappyFlow SfPoller Setup In AWS","type":0,"sectionRef":"#","url":"docs/sfPoller/aws_setup","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"SnappyFlow SfPoller Setup In AWS","url":"docs/sfPoller/aws_setup#prerequisites","content":""},{"title":"Create IAM Role:​","type":1,"pageTitle":"SnappyFlow SfPoller Setup In AWS","url":"docs/sfPoller/aws_setup#create-iam-role","content":"Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ In the navigation pane of the IAM console, click Policies, and then click Create policy In the JSON tab , add policy json: { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Action&quot;: [ &quot;cloudwatch:Describe*&quot;, &quot;cloudwatch:Get*&quot;, &quot;cloudwatch:List*&quot;, &quot;cloudwatch:GetMetricStatistics*&quot;, &quot;elasticloadbalancing:Describe*&quot;, &quot;s3:Get*&quot;, &quot;s3:List*&quot;, &quot;rds:Describe*&quot;, &quot;rds:ListTagsForResource&quot;, &quot;logs:Get*&quot;, &quot;logs:Describe*&quot;, &quot;ecs:Describe*&quot;, &quot;ecs:List*&quot;, &quot;pi:*&quot; ], &quot;Effect&quot;: &quot;Allow&quot;, &quot;Resource&quot;: &quot;*&quot; } ] } Copy Click Review Policy Add a Policy name (must be unique) (Optional) For description, type a description for the new policy Click Create Policy In the navigation pane of the IAM console, click Roles, and then click Create role For Select type of trusted entity, click AWS service For click the use case for your service, click EC2 Click Next: Permissions Search for the policy created in step 3. Select the check box next to that policy. Click Next: Review Add a Role name (must be unique)(Optional) For Role description, type a description for the new roleReview the role and then click Create role Refrences: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-service.html "},{"title":"Create AWS Security Group:​","type":1,"pageTitle":"SnappyFlow SfPoller Setup In AWS","url":"docs/sfPoller/aws_setup#create-aws-security-group","content":"Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ In the navigation pane, click Security Groups Click Create security group In the Basic details section, do the following: Enter a descriptive name and brief description for the security group. For VPC, click the VPC in which to create the security group. The security group can only be used in the VPC in which it is created Add security group rules now Add inbound rules: Type\tProtocol\tPort\tSource\tDescriptionHTTP\tTCP\t80\t0.0.0.0,::/0\tsfPoller http HTTPS\tTCP\t443\t0.0.0.0,::/0\tsfPoller https SSH\tTCP\t22\t0.0.0.0,::/0\tsfPoller ssh Note: Define Source as per user's requirements. Use anywhere if sfPoller has to be accessed from multiple locations. Use Custom if sfPoller will be used from Office/VPN (also provide IP address). Click Create. Refrences: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/working-with-security-groups.html#creating-security-group Be ready with SSH keypair to be used for launching instance. "},{"title":"Launching SfPoller Instance​","type":1,"pageTitle":"SnappyFlow SfPoller Setup In AWS","url":"docs/sfPoller/aws_setup#launching-sfpoller-instance","content":"Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ In the navigation pane, click Instances Click Launch Instance Click on AWS Marketplace and search for SnappyFlow Poller Appliance Select SnappyFlow sfPoller Appliance AMI Read SnappyFlow sfPoller Appliance usage policy and detail. Click continue to proceed Select the instance type (Min Recommended: t2.medium ) Configure instance: Provide network, subnet and IAM role (IAM role created in prerequisite) (Optional) Add tags for sfpoller instance Configure security group: Click the security group created in the prerequisite stepReview changes before clicking launch Click the SSH key pair and launch Wait till the instance state goes to ready and then open the launched instance on EC2 panel. Copy Public DNS (IPv4) from instance description in AWS console Paste Public DNS (IPv4) of the launched instance to the browser and open the sfPoller Launcher. Enter username: admin and password: instanceID of launched instance and login to sfPoller Note: Instance ID can be copied from instance description (as used in step 7). sfPoller installation is complete now "},{"title":"Configure sfPoller​","type":1,"pageTitle":"SnappyFlow SfPoller Setup In AWS","url":"docs/sfPoller/aws_setup#configure-sfpoller","content":"Add Profile Key: Copy profile key from &quot;Profiles&quot; section under &quot;Manage&quot; tab of your SnappyFlow SaaS account or SnappyFlow server and save. Add Cloud Account: Click cloud type and enter its details. (Optional) Add cloud metric plugins under &quot;Plugins&quot; tab if it is required to monitor account as well. Create Project: Go to Application tabClick &quot;Create project&quot; and provide project name Add Applications: Add application by clicking add application button under &quot;Actions&quot; column (icon with + sign)Users can either discover applications from cloud using resource tags or manually add application and endpointsClick discover and provide tag keys for the account (If resources are tagged properly on the cloud, then Discovery feature discovers all end points)Add endpoints after selection (by default all endpoints are visible) After adding endpoint click the save button Go to SnappyFlow dashboard (from where profile key was copied) and wait for sometime to get your applications discovered sfPoller Installation ​ "},{"title":"SnappyFlow sfPoller Setup In Azure","type":0,"sectionRef":"#","url":"docs/sfPoller/azure_setup","content":"","keywords":""},{"title":"Launching sfPoller Instance​","type":1,"pageTitle":"SnappyFlow sfPoller Setup In Azure","url":"docs/sfPoller/azure_setup#launching-sfpoller-instance","content":"Login to Azure portal. Goto marketplace and select SnappyFlow sfPoller at azure portal.Click create.Create a new resource group.Provide Virtual Machine name and select region.Select the deployment size of the VM. (Recommended standard_B2s- 2 vcpu and 4GiB RAM).Select generate new key pair option for SSH public key resource or use existing public key.If you select generate new key pair option provide key pair names. Select all three ports (HTTP 80, HTTPS 443, SSH 22) in inbound ports which are mentioned in the image below. Click on next and select OS disk type.Click on next and in the Management tab enable the identity check box.(Optional) Add tags for sfpoller instance Click on review+create and click create. After successfully deploying of server follow the below steps. Navigate to the virtual machines and select the VM. Search for Identity and click on it.Click on add role assignments.Select scope as subscription and role as an owner. Click on save. After completing the above steps, to access the sfPoller UI copy the public IP of VM and to login the username is admin, the password is a private IP address of the VM.  "},{"title":"Configure sfPoller​","type":1,"pageTitle":"SnappyFlow sfPoller Setup In Azure","url":"docs/sfPoller/azure_setup#configure-sfpoller","content":"Add Profile Key: Copy profile key from &quot;Profiles&quot; section under &quot;Manage&quot; tab of your SnappyFlow SaaS account or SnappyFlow server and save. Add Cloud Account: Click cloud type and enter its details. (Optional) Add cloud metric plugins under &quot;Plugins&quot; tab if it is required to monitor account as well. Create Project: Go to Application tabClick &quot;Create project&quot; and provide project name Add Applications: Add application by clicking add application button under &quot;Actions&quot; column (icon with + sign)Users can either discover applications from cloud using resource tags or manually add application and endpointsClick discover and provide tag keys for the account (If resources are tagged properly on the cloud, then Discovery feature discovers all end points)Add endpoints after selection (by default all endpoints are visible) After adding endpoint click the save button Go to SnappyFlow dashboard (from where profile key was copied) and wait for sometime to get your applications discovered "},{"title":"C# tracing","type":0,"sectionRef":"#","url":"docs/Tracing/csharp","content":"","keywords":""},{"title":"Available application types​","type":1,"pageTitle":"C# tracing","url":"docs/Tracing/csharp#available-application-types","content":"ASP.NET Core .NET Framework "},{"title":"Performance Metrics","type":0,"sectionRef":"#","url":"docs/RUM/RUM_Metrics","content":"Performance Metrics Apdex Rating​ Apdex (Application Performance Index) is a measure of user satisifaction for a website that varies from 0 to 1 (0 = no users satisfied, 1 = all users satisfied). This rating is a numerical measure of user satisfaction with the performance of enterprise applications. Below is the formula for Apdex Rating Apdex(t)=SatisifiedCount+(ToleratedCount∗0.5)+(FrustratedCount∗0)numTotalTransactionsApdex(t) = \\frac{ SatisifiedCount + (ToleratedCount * 0.5) + (FrustratedCount * 0)}{ numTotalTransactions }Apdex(t)=numTotalTransactionsSatisifiedCount+(ToleratedCount∗0.5)+(FrustratedCount∗0)​ where ttt stands for the target response time If ResponseTimeResponseTimeResponseTime is the time taken by the system to respond, SatisifiedCountSatisifiedCountSatisifiedCount stands for the number of transactions where ResponseTimeResponseTimeResponseTime ≤\\le≤ t ToleratedCountToleratedCountToleratedCount stands for the number of transactions where t &lt;\\lt&lt; ResponseTimeResponseTimeResponseTime ≤\\le≤ 4t FrustratedCountFrustratedCountFrustratedCount stands for the number of transactions where ResponseTimeResponseTimeResponseTime &gt;\\gt&gt; 4t note In the Snappyflow RUM Dashboard, Apdex Rating is calculated by setting the target response time value as 500ms Page load metrics​ During loading of a page, resource timing information of all the dependent resources that are included as a part of the page such as JavaScript, stylesheets, images, etc. are captured. This information explains in details about how the page is loaded. Below are some of the page load metrics Domain lookup This event is triggered when a DNS query is fired for the current page. The DurationDurationDuration of this event is calculated as Duration=domainLookupEnd−domainLookupStartDuration = domainLookupEnd - domainLookupStartDuration=domainLookupEnd−domainLookupStartwhere domainLookupStartdomainLookupStartdomainLookupStart and domainLookupEnddomainLookupEnddomainLookupEnd are the start and end times of the domain Lookup event. Making a connection to the server This event is triggered when a TCP connection is established to the server. The DurationDurationDuration of this event includes the TLS negotiation time for HTTPS pages and is calculated as Duration=connectEnd−connectStartDuration = connectEnd - connectStartDuration=connectEnd−connectStart where connectStartconnectStartconnectStart and connectEndconnectEndconnectEnd are the start and end times of establishing the TCP connection. Requesting and receiving the document This event is triggered when a document is requested to the server. The DurationDurationDuration of this event is calculated as Duration=responseEnd−requestStartDuration = responseEnd - requestStartDuration=responseEnd−requestStart where requestStartrequestStartrequestStart is the time when the request for the document is initiated andresponseEndresponseEndresponseEnd is the time when the requested document is received completely. Parsing the document, executing sync. scripts This event is triggered when any document is parsed in order to render the DOM or when executing synchronous scripts or when the stylesheets are loaded. The DurationDurationDuration of this event is calculated as Duration=tagsdomInteractive−domLoadingDuration = tagsdomInteractive - domLoadingDuration=tagsdomInteractive−domLoading where domLoadingdomLoadingdomLoading is the time when the DOM loads and tagsdomInteractivetagsdomInteractivetagsdomInteractive is the time when the DOM tag elements turn interactive. Fire &quot;DOMContentLoaded&quot; event This event is triggered when the browser completes parsing the document. This event is helpful when there are multiple listeners, or logic is executed. The DurationDurationDuration of this event is calculated as Duration=domContentLoadedEventEnd−domContentLoadedEventStartDuration = domContentLoadedEventEnd - domContentLoadedEventStartDuration=domContentLoadedEventEnd−domContentLoadedEventStart where domContentLoadedEventStartdomContentLoadedEventStartdomContentLoadedEventStart and domContentLoadedEventEnddomContentLoadedEventEnddomContentLoadedEventEnd are the start and end times of initating the &quot;DOMContentLoaded&quot; event. Fire &quot;load&quot; event This event is trigged when the browser finishes loading its document and dependent resources. The DurationDurationDuration of this event is calculated as Duration=loadEventEnd−loadEventStartDuration = loadEventEnd - loadEventStartDuration=loadEventEnd−loadEventStart where loadEventStartloadEventStartloadEventStart and loadEventEndloadEventEndloadEventEnd are the start and end times of loading the document and dependent resources. Time to first byte (TTFB) TTFB is the duration between the browser making an HTTP request for the initial document to the first byte of the page being received. TTFB is calculated as TTFB=firstByteReceived−initialDocRequestTTFB = firstByteReceived - initialDocRequestTTFB=firstByteReceived−initialDocRequest where initialDocRequestinitialDocRequestinitialDocRequest is the time at which the request for the initial document is made and firstByteReceivedfirstByteReceivedfirstByteReceived` is the time at which first byte is received note The page-load transaction duration might not always reflect the Load event of the browser and can extend beyond the event. This is because in order to capture the overall user experience of the page including all of the above information plus additional resource requests that might be triggered during the execution of dependent resources. User interactions​ The click event listeners that are registered by the application are automatically instrumented by the sf-apm-rum agent. These click events are captured as user-interaction transactions. However, in order to limit the number of user transactions, the agent discards transactions with no spans (e.g. no network activity). In case a click event results in a route change, such transactions are captured as route-change transactions instead of user-interaction transaction. The name of the user-interaction transaction is influenced by either name or preferably the data-transaction-name attribute of the HTML element. User Centric Metrics​ The sf-apm-rum agent supports capturing of the below responsiveness metrics in order to understand the performance characteristics of a web page beyond the page load and how the user perceives performance. First contentful paint (FCP) This metric is a measure of the time from when the page starts loading to when any part of the page’s content is displayed on the screen. The sf-apm-rum agent uses the Paint timing API available in the browser to capture the timing information. FCP is captured as transaction mark for page-load transaction for all chromium-based browsers (Chrome &gt;60, Edge &gt;79, Opera &gt;47, etc.). Largest contentful paint (LCP) This metric is a measure of the time from when the page starts loading to when the critical element (largest text, image, video elements) is displayed on the screen. The sf-apm-rum agent uses the LargestContentfulPaint API which relies on the draft Element Timing API. LCP is one of the core web vitals metrics and available only in Chrome &gt;77. This metric is captured as transaction mark for page-load transaction, maintain LCP within the first 2.5 seconds of page-load to provide a good user experience. First input delay (FID) This metric quantifies the experience of the user when they interact with the page during the page load. It is measured as the time between when a user first interacts with your site (mouse clicks, taps, select dropdowns, etc.) to the time when the browser can respond to that interaction. FID is one of the core web vitals metrics and available only in Chrome &gt;85 via Event Timing API. FID is captured as First Input Delay span for page-load transaction. FID value below 100 milliseconds has to be maintained to provide a good user experience. Total blocking time (TBT) This metric is calculated as the sum of the blocking time (duration above 50 ms) for each long task that occurs between the First contentful paint and the time when the transaction is completed. Total blocking time is a great companion metric for Time to interactive (TTI) which is lab metric and not available in the field through browser APIs. The sf-apm-rum agent captures TBT based on the number of long tasks that occurred during the page load lifecycle. This metric is captured as Total Blocking Time span for page-load transaction. Cumulative layout shift (CLS) This is a metric that represents the cumulative score of all unexpected layout shifts that occur during the entire lifespan of the page. CLS is one of the core web vitals metrics. A CLS score of less than 0.1 has to be maintained to provide a good user experience. Long Tasks A long task is any user activity or browser task that monopolize the UI thread for extended periods (greater than 50 milliseconds) and block other critical tasks (frame rate or input latency) from being executed. The sf-apm-rum agent uses the Long Task API which is only available in chromium-based browsers (Chrome &gt;58, Edge &gt;79, Opera &gt;69). It is captured as Longtask &lt;name&gt; span for all managed transactions. JavaScript errors​ The javascript errors that occur in the application are captured. Using the captured information, the root cause of the issue can be analysed.","keywords":""},{"title":"Java tracing","type":0,"sectionRef":"#","url":"docs/Tracing/java_v2","content":"","keywords":""},{"title":"Available Platforms​","type":1,"pageTitle":"Java tracing","url":"docs/Tracing/java_v2#available-platforms","content":"InstanceKubernetesDockerECSAWS Lambda Tracing JAVA applications on instances is a 2 step process involving installation of SnappyFlow agent on the instance and configuring the app to allow metrics to be collected. Install agentConfigure application for tracing Configuring the app allows the collection of metrics by the SnappyFlow trace agent. There are multiple ways to configure JAVA apps depending on the type of JAVA application. Choose the type below or follow the command line instructions for generic JAVA applications. Command Line​ Use the following arguments while starting your application using java –jar command, in IDE, Maven or Gradle script: java -javaagent:/opt/sfagent/sftrace/java/sftrace-java-agent.jar -Dsftrace.service_name=&lt;my-service&gt; -jar &lt;application jar&gt; Copy Note: If service_name is not provided, an auto discovered service name will be added. Service_name is used to identify and filter the traces related to an application and should be named appropriately to distinctly identify it. Service name must only contain characters from the ASCII alphabet, numbers, dashes, underscores and spaces. Additional features available for Spring Boot Applications​ By default, transaction names of unsupported Servlet API-based frameworks are in the form of methodunknownroute.Tomodifythisandtoreportthetransactionsnamesintheformof method unknown route. To modify this and to report the transactions names in the form of methodunknownroute.Tomodifythisandtoreportthetransactionsnamesintheformof method $path, use the following in javaagent configuration. This option is applicable only for spring-boot based applications. -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true Copy Normalizing Transaction URLs​ If your URLs contain path parameters like /user/$userId, it can lead to an explosion of transaction types. This can be avoided by using URL groups. For example, if the application supports urls like: /owners, /owners/&lt;owner_id&gt;, /owners/&lt;owner_id&gt;/edit, /owners/&lt;owner_id&gt;/pets, Copy then url groups would be configured as: url_groups=/owners/*,/owner/*/edit,/owners/*/pets Copy Apache Tomcat​ Add the agent configuration in setenv.sh. If this file is not present, create the file in below folder &lt;tomcat installation path&gt;/bin Copy Refer to tomcat_setenv.sh for tracing specific configuration that needs to be copied to setenv.sh file. Make the file executable using chmod +x bin/setenv.sh and start the server Additional features available for Spring Boot Applications​ By default, transaction names of unsupported Servlet API-based frameworks are in the form of methodunknownroute.Tomodifythisandtoreportthetransactionsnamesintheformof method unknown route. To modify this and to report the transactions names in the form of methodunknownroute.Tomodifythisandtoreportthetransactionsnamesintheformof method $path, use the following in javaagent configuration. This option is applicable only for spring-boot based applications. -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true Copy Normalizing Transaction URLs​ If your URLs contain path parameters like /user/$userId, it can lead to an explosion of transaction types. This can be avoided by using URL groups. For example, if the application supports urls like: /owners, /owners/&lt;owner_id&gt;, /owners/&lt;owner_id&gt;/edit, /owners/&lt;owner_id&gt;/pets, Copy then url groups would be configured as: url_groups=/owners/*,/owner/*/edit,/owners/*/pets Copy Add the agent configuration in setenv.sh. If this file is not present, create the file in below folder &lt;tomcat installation path&gt;/bin Copy Refer to tomcat_setenv.sh for tracing specific configuration that needs to be copied to setenv.sh file. Make the file executable using chmod +x bin/setenv.sh and start the server JBOSS/EAP​ Standalone Mode​ Add the agent configuration in standalone.conf file and start the server Refer to JBOSS_standalone.conf for tracing specific configuration. Copy from section with “SFTRACE-CONFIG” in comments Domain Mode​ Add the agent configuration in domain.xml and start the server Refer to JBOSS_domain.xml for tracing specific configuration. Copy from section with “SFTRACE-CONFIG” in comments After updating the configuration, restart the application. Additional features available for Spring Boot Applications​ By default, transaction names of unsupported Servlet API-based frameworks are in the form of methodunknownroute.Tomodifythisandtoreportthetransactionsnamesintheformof method unknown route. To modify this and to report the transactions names in the form of methodunknownroute.Tomodifythisandtoreportthetransactionsnamesintheformof method $path, use the following in javaagent configuration. This option is applicable only for spring-boot based applications. -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true Copy Normalizing Transaction URLs​ If your URLs contain path parameters like /user/$userId, it can lead to an explosion of transaction types. This can be avoided by using URL groups. For example, if the application supports urls like: /owners, /owners/&lt;owner_id&gt;, /owners/&lt;owner_id&gt;/edit, /owners/&lt;owner_id&gt;/pets, Copy then url groups would be configured as: url_groups=/owners/*,/owner/*/edit,/owners/*/pets Copy Example of running java application via command line using these parameters​ java -javaagent:/opt/sfagent/sftrace/java/sftrace-java-agent.jar -Dsftrace.service_name=my-service -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true -Delastic.apm.url_groups=/owners/*,/owner/*/edit,/owners/*/pets -jar &lt;application jar&gt; Copy "},{"title":"Java tracing","type":0,"sectionRef":"#","url":"docs/Tracing/java","content":"","keywords":""},{"title":"Instances​","type":1,"pageTitle":"Java tracing","url":"docs/Tracing/java#instances","content":"Install sfAgent which automatically installs sfTrace agent as well. Link the application with sfTrace Java Agent "},{"title":"Command Line​","type":1,"pageTitle":"Java tracing","url":"docs/Tracing/java#command-line","content":"Use the following arguments while starting your application using java –jar command, in IDE, Maven or Gradle script: java -javaagent:/opt/sfagent/sftrace/java/sftrace-java-agent.jar -Dsftrace.service_name=&lt;my-service&gt; -jar &lt;application jar&gt; Copy note If service_name is not provided, an auto discovered service name will be added. Service_name is used to identify and filter the traces related to an application and should be named appropriately to distinctly identify it. Service name must only contain characters from the ASCII alphabet, numbers, dashes, underscores and spaces. Additional features available for Spring Boot Applications​ By default, transaction names of unsupported Servlet API-based frameworks are in the form of methodunknownroute.Tomodifythisandtoreportthetransactionsnamesintheformof method unknown route. To modify this and to report the transactions names in the form of methodunknownroute.Tomodifythisandtoreportthetransactionsnamesintheformof method $path, use the following in javaagent configuration. This option is applicable only for spring-boot based applications. -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true Copy Normalizing Transaction URLs​ If your URLs contain path parameters like /user/$userId, it can lead to an explosion of transaction types. This can be avoided by using URL groups. For example, if the application supports urls like: /owners, /owners/&lt;owner_id&gt;, /owners/&lt;owner_id&gt;/edit, /owners/&lt;owner_id&gt;/pets, Copy then url groups would be configured as: url_groups=/owners/*,/owner/*/edit,/owners/*/pets Copy Example of running java application via command line using these parameters​ java -javaagent:/opt/sfagent/sftrace/java/sftrace-java-agent.jar -Dsftrace.service_name=my-service -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true -Delastic.apm.url_groups=/owners/*,/owner/*/edit,/owners/*/pets -jar &lt;application jar&gt; Copy Enable Trace to Log feature​ Add below system property to enable the feature: -Delastic.apm.global_labels=&quot;_tag_redact_body=true&quot; Copy We also provide custom document type and destination index configs for this feature.(Optional) a. Add below label to provide custom documentType (Default:&quot;log&quot;): -Delastic.apm.global_labels=&quot;_tag_redact_body=true,_tag_IndexType=metric&quot; // Applicable values(log, metric) Copy b. Add below label to provide custom documentType (Default:&quot;user-input&quot;): -Delastic.apm.global_labels=&quot;_tag_redact_body=true,_tag_documentType=&lt;document-type&gt;&quot; Copy Note : -Delastic.apm.capture_body=all properties needs to added when enabling trace to log feature Run Command: java -javaagent:/opt/sfagent/sftrace/java/sftrace-java-agent.jar -Dsftrace.service_name=spring-service -Delastic.apm.use_path_as_transaction_name=true -Delastic.apm.capture_body=all -Delastic.apm.global_labels=&quot;_tag_redact_body=true,_tag_IndexType=&lt;index_type&gt;,_tag_documentType=&lt;document_type&gt;&quot; -Delastic.apm.metrics_interval=0s -jar target/spring-petclinic-2.1.0.BUILD-SNAPSHOT.jar Copy "},{"title":"Apache Tomcat​","type":1,"pageTitle":"Java tracing","url":"docs/Tracing/java#apache-tomcat","content":"Add the agent configuration in setenv.sh. If this file is not present, create the file in below folder &lt;tomcat installation path&gt;/bin Copy Refer to tomcat_setenv.sh for tracing specific configuration that needs to be copied to setenv.sh file. Make the file executable using chmod +x bin/setenv.sh and start the server Additional features available for Spring Boot Applications​ By default, transaction names of unsupported Servlet API-based frameworks are in the form of methodunknownroute.Tomodifythisandtoreportthetransactionsnamesintheformof method unknown route. To modify this and to report the transactions names in the form of methodunknownroute.Tomodifythisandtoreportthetransactionsnamesintheformof method $path, use the following in javaagent configuration. This option is applicable only for spring-boot based applications. -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true Copy Normalizing Transaction URLs​ If your URLs contain path parameters like /user/$userId, it can lead to an explosion of transaction types. This can be avoided by using URL groups. For example, if the application supports urls like: /owners, /owners/&lt;owner_id&gt;, /owners/&lt;owner_id&gt;/edit, /owners/&lt;owner_id&gt;/pets, Copy then url groups would be configured as: url_groups=/owners/*,/owner/*/edit,/owners/*/pets Copy Example of running java application via command line using these parameters​ java -javaagent:/opt/sfagent/sftrace/java/sftrace-java-agent.jar -Dsftrace.service_name=my-service -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true -Delastic.apm.url_groups=/owners/*,/owner/*/edit,/owners/*/pets -jar &lt;application jar&gt; Copy Enable Trace to Log feature​ Add below system property to enable the feature: -Delastic.apm.global_labels=&quot;_tag_redact_body=true&quot; Copy We also provide custom document type and destination index configs for this feature.(Optional) a. Add below label to provide custom documentType (Default:&quot;log&quot;): -Delastic.apm.global_labels=&quot;_tag_redact_body=true,_tag_IndexType=metric&quot; // Applicable values(log, metric) Copy b. Add below label to provide custom documentType (Default:&quot;user-input&quot;): -Delastic.apm.global_labels=&quot;_tag_redact_body=true,_tag_documentType=&lt;document-type&gt;&quot; Copy Note : -Delastic.apm.capture_body=all properties needs to added when enabling trace to log feature Run Command: java -javaagent:/opt/sfagent/sftrace/java/sftrace-java-agent.jar -Dsftrace.service_name=spring-service -Delastic.apm.use_path_as_transaction_name=true -Delastic.apm.capture_body=all -Delastic.apm.global_labels=&quot;_tag_redact_body=true,_tag_IndexType=&lt;index_type&gt;,_tag_documentType=&lt;document_type&gt;&quot; -Delastic.apm.metrics_interval=0s -jar target/spring-petclinic-2.1.0.BUILD-SNAPSHOT.jar Copy "},{"title":"JBOSS EAP​","type":1,"pageTitle":"Java tracing","url":"docs/Tracing/java#jboss-eap","content":"Standalone Mode​ Add the agent configuration in standalone.conf file and start the server Refer to JBOSS_standalone.conf for tracing specific configuration. Copy from section with SFTRACE-CONFIG in comments Domain Mode​ Add the agent configuration in domain.xml and start the server Refer to JBOSS_domain.xml for tracing specific configuration. Copy from section with SFTRACE-CONFIG in comments After updating the configuration, restart the application. Additional features available for Spring Boot Applications​ By default, transaction names of unsupported Servlet API-based frameworks are in the form of methodunknownroute.Tomodifythisandtoreportthetransactionsnamesintheformof method unknown route. To modify this and to report the transactions names in the form of methodunknownroute.Tomodifythisandtoreportthetransactionsnamesintheformof method $path, use the following in javaagent configuration. This option is applicable only for spring-boot based applications. -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true Copy Normalizing Transaction URLs​ If your URLs contain path parameters like /user/$userId, it can lead to an explosion of transaction types. This can be avoided by using URL groups. For example, if the application supports urls like: /owners, /owners/&lt;owner_id&gt;, /owners/&lt;owner_id&gt;/edit, /owners/&lt;owner_id&gt;/pets, Copy then url groups would be configured as: url_groups=/owners/*,/owner/*/edit,/owners/*/pets Copy Example of running java application via command line using these parameters​ java -javaagent:/opt/sfagent/sftrace/java/sftrace-java-agent.jar -Dsftrace.service_name=my-service -Delastic.apm.disable_instrumentations=spring-mvc -Delastic.apm.use_path_as_transaction_name=true -Delastic.apm.url_groups=/owners/*,/owner/*/edit,/owners/*/pets -jar &lt;application jar&gt; Copy "},{"title":"Docker​","type":1,"pageTitle":"Java tracing","url":"docs/Tracing/java#docker","content":"Refer to java_Dockerfile. Look at sections with SFTRACE-CONFIG description. Installation steps are provided. Copy the trace agent to the container and start the container by attaching the agent to the application. Additionally, user has to add SnappyFlow configurations for profile_key, projectName, appName to the docker file Once updated, build and start the container. "},{"title":"Kubernetes​","type":1,"pageTitle":"Java tracing","url":"docs/Tracing/java#kubernetes","content":"sfTrace is run as an initContainer in the application pod. User can deploy this either using a manifest yaml or a Helm chart. "},{"title":"Example of Manifest yaml​","type":1,"pageTitle":"Java tracing","url":"docs/Tracing/java#example-of-manifest-yaml","content":"java_k8s_standalone_deployment.yaml  "},{"title":"Example of a Helm chart​","type":1,"pageTitle":"Java tracing","url":"docs/Tracing/java#example-of-a-helm-chart","content":"Update values.yaml: Refer to java_k8s_with_helm_chart_values.yaml to configure agent specific properties. Look at sections with SFTRACE-CONFIG description Update deployment.yaml: Refer to java_k8s_with_helm_chart_deployment.yaml to copy trace agent to the container and start the container by attaching the agent. Look at sections with SFTRACE-CONFIG description "},{"title":"ECS​","type":1,"pageTitle":"Java tracing","url":"docs/Tracing/java#ecs","content":""},{"title":"Create the Task definition​","type":1,"pageTitle":"Java tracing","url":"docs/Tracing/java#create-the-task-definition","content":"Open Amazon ECS, in navigation pane, choose task definition and click on Create New Task Definition and select the launch type as EC2 or Fargate, click on Next step. Give the Task definition Name Task Role, choose an IAM role that provides permissions for containers in your task to make calls to AWS APIs on your behalf and Network Mode Click on Add containers. Give a Container name, and give the Image of your Java Application. Set Memory limit and port mappings as per your task requirements. In the environment section, for Entry Point give sh , -c For Command paste the following lines mkdir /sfagent &amp;&amp; wget -O /sfagent/sftrace-agent.tar.gz https://github.com/snappyflow/apm-agent/releases/download/latest/sftrace-agent.tar.gz &amp;&amp; cd /sfagent &amp;&amp; tar -xvzf sftrace-agent.tar.gz &amp;&amp; java -javaagent:/sfagent/sftrace/java/sftrace-java-agent.jar -jar &lt;your_jar_name&gt; Copy note Some EC2 task definitions may be running on host containers that don’t recoginise the wget command in such case, add below lines in the above command, apt update &amp;&amp; apt -y upgrade. Add the following Environment Variables: SFTRACE_PROJECT_NAME &lt;project_name&gt; SFTRACE_APP_NAME &lt;app_name&gt; SFTRACE_SERVICE_NAME &lt;service_name&gt; SFTRACE_PROFILE_KEY &lt;profile_key&gt; Copy The below environment variables are only applicable for springmvc and optional. ELASTIC_APM_DISABLE_INSTRUMENTATIONS spring-mvc ELASTIC_APM_USE_PATH_AS_TRANSACTION_NAME &quot;true&quot; Copy "},{"title":"Create the Cluster​","type":1,"pageTitle":"Java tracing","url":"docs/Tracing/java#create-the-cluster","content":"In the Navigation pane, select Clusters and click on Create ClusterSelect the template as per your requirementGive a Cluster name and give instance, networking Configurations IAM role as per your task requirements "},{"title":"Create the Service​","type":1,"pageTitle":"Java tracing","url":"docs/Tracing/java#create-the-service","content":"Click on the Cluster Name you created in the step2Click on Create , Select the Launch type matching to your task definition. Select the Task Definition Name and Version in the Drop down matching to the task definition you created in step 1Give a Service Name and select other requirements as per your task compatibilityClick on next step and start your service "},{"title":"Tracing in SnappyFlow","type":0,"sectionRef":"#","url":"docs/Tracing/overview","content":"Tracing in SnappyFlow SnappyFlow supports distributed tracing compliant with Opentracing standard. Tracing allows users to visualize the sequence of steps a transaction (whether API or non-API such as a Celery job) takes during its execution. This analysis is extremely powerful and allows pinpointing the source of problems such as abnormal time being spent on an execution step or identifying point of failure in a transaction. SnappyFlow refers to distributed tracing as sfTrace. Select your language​ Java​ Python​ Ruby​ Go​ Node.js​ C#​","keywords":""},{"title":"Tracing History","type":0,"sectionRef":"#","url":"docs/Tracing/advanced/trace_history","content":"Tracing History Trace collection generates large amount of data. All these historical trace data is not required. This is why the retention period of trace data is very low, compared to metrics or logs retention. The default configuration of trace retention is one hour. Trace data is automatically removed once it ages. Though there is a trace retention configuration, SnappyFlow preserves important traces for a longer duration. These are called historical traces and are retained along with metrics. These remain in the system as long as metrics are preserved and are controlled through metric retention configuration. These important traces or selection of traces to be preserved as histroical traces is controlled by trace filter rules. The default rule is defined as, HTTP 4xx, HTTP 5XX error codes requests. Http request where duration is greater than 90th percentile of duration of the URL (For each URL present in the application). Max. 5 records per URL, every half an hour: Users can define custom rule to collect specific traces into historical traces. These rules are executed every 30 minutes and all matching traces will be stored. These rules are available under, History Pipeline Rules, Steps to create a History Pipeline Rule Select appropriate filters in Aggregate/ Realtime page. View the result. Click Save Filter As History Pipeline Rule link in the top right corner. Provide the configuration details for the History Pipeline Rule and click save. Note: To avoid overhead of data we are limiting the #Max Records to 50. To Manage the list of History Pipeline Rules, Click History Pipeline Rules link in Aggregate/ Realtime page","keywords":""},{"title":"Snappyflow sfPoller Server for vCenter","type":0,"sectionRef":"#","url":"docs/sfPoller/vcenter_setup","content":"","keywords":""},{"title":"Pre-Requisites:​","type":1,"pageTitle":"Snappyflow sfPoller Server for vCenter","url":"docs/sfPoller/vcenter_setup#pre-requisites","content":"vCenter Server 5.5 and above.vCenter login credentials for a user having administrative privileges or enough rights to provision VMs in a data center.If your datacenter does not support DHCP, static IP address details for SnappyFlow server. "},{"title":"Resource Requirement For SnappyFlow Server:​","type":1,"pageTitle":"Snappyflow sfPoller Server for vCenter","url":"docs/sfPoller/vcenter_setup#resource-requirement-for-snappyflow-server","content":"vCPU: 2RAM: 2 GBDisk: 20 GB, (IDE, SCSI disks supported) "},{"title":"Instructions:​","type":1,"pageTitle":"Snappyflow sfPoller Server for vCenter","url":"docs/sfPoller/vcenter_setup#instructions","content":"Import OVA into your vCenter using vSphere client.Verify IP address of the SnappyFlow VM/Server in summary section of vSphere client.Connect to server using http:// ip addressLogin using admin/adminFollow the instructions to setup server. "},{"title":"What Is SfPoller?","type":0,"sectionRef":"#","url":"docs/sfPoller/overview","content":"What Is SfPoller? During the course of instrumenting an application for monitoring, an SRE or an architect needs a repertoire of tools to cater to varied requirements of monitoring. sfPoller is a powerful and extremely useful component of SnappyFlow and when used in conjunction with sfAgent and sfPod, allow users to reach all interfaces of an application that need to be monitored. A few example scenarios where sfPoller is indispensable are mentioned below: Use Cases\tsfPoller featuresMonitoring Cloud Services such as ELB, S3, RDS etc.\tIncludes plugins for most commonly used cloud components. Plugin connects to a cloud account, discover inventory and collect static &amp; dynamic parameters using Public cloud APIs, Cloudwatch and Azure Monitor. Database monitoring\tIncludes plugins for most popular databases e.g. Oracle, MySQL, MS-SQL, Postgres, Redis, Elasticsearch etc. Service Monitoring using Synthetics\tUsers can deploy postman like collections on sfPoller to monitor health of API end-points. Alternatively, users can run synthetic monitoring on Blazemeter &amp; Taurus and poll these external load generators to get results of various runs and correlate the results with application performance. Monitoring of Cloud Inventory, Usage and Pricing\tIncludes plugins for AWS, Azure and GCP to provide analysis of usage and billing at a very granular level. Datacenter monitoring and analysis\tsfPoller contains a very highly scalable vCenter, HyperV and Baremetal pollers that can monitor extremely large deployments and provide analysis of performance and usage.","keywords":""},{"title":".NET Framework application","type":0,"sectionRef":"#","url":"docs/Tracing/dotnetframework","content":"","keywords":""},{"title":"Supported web frameworks​","type":1,"pageTitle":".NET Framework application","url":"docs/Tracing/dotnetframework#supported-web-frameworks","content":"Framework\tSupported versions.NET Framework\t4.6.1 and later "},{"title":"Prerequisite​","type":1,"pageTitle":".NET Framework application","url":"docs/Tracing/dotnetframework#prerequisite","content":"Install the following Nuget packages Elastic.Apm.AspNetFullFrameworkSnappyflow.NetCoretrace.Utility These packages can be installed using Nuget package manager in Visual Studio or using .NET CLI commands given below. dotnet add package SnappyFlow.NetCoretrace.Utility --version 0.1.5 Copy dotnet add package Elastic.Apm.AspNetFullFramework --version 1.12.1 Copy "},{"title":"Steps to configure Application​","type":1,"pageTitle":".NET Framework application","url":"docs/Tracing/dotnetframework#steps-to-configure-application","content":"Create a folder wwwroot inside System path of application. info If you don't know which is system path, you can print the code below to identify it Directory.GetCurrentDirectory(); Copy For Instance based application, system path will be IIS Express folder. Common location for IIS Express Folder in windows is C:\\Program Files (x86)\\IIS Express. Create sftraceConfig.yaml file inside wwwroot directory and copy below sftraceConfig.yaml content. Configure it with correct profile key and tags. Get profile key from the Manage--&gt;profile in snappyflow portal. Create project and application(or use existing project and applicaition) using your profile. You can provide any name to service, this will be displayed in Trace Dashboard. sftraceConfig.yaml​ tags: projectName: CHANGEME appName: CHANGEME serviceName: CHANGEME key: CHANGEME Copy info If system path is IIS Express as above, the whole path for sftraceConfig.yaml should be C:\\Program Files (x86)\\IIS Express\\wwwroot\\sftraceConfig.yaml Create a new class file called Sftrace_class.cs in location where web.config file is present. Copy the code below and don't forget to change CHANGENAMESPACE. caution Add the content below to new class file Sftrace_class.cs and don't forget to rename CHANGENAMESPACE to your namespace. Sftrace_class.cs​ using System.Configuration; using SftraceDotNetcore; namespace CHANGENAMESPACE { public class Sftrace_class { public void Sftrace_method() { var configFile = System.Web.Configuration.WebConfigurationManager.OpenWebConfiguration(&quot;~/&quot;); var settings = configFile.AppSettings.Settings; string sftraceConfigfile = $&quot;sftraceConfig.yaml&quot;; var sfdetails = sftracedecrypt.Trace(sftraceConfigfile); foreach(var kvp in sfdetails) { if (settings[kvp.Key] == null) { settings.Add(kvp.Key, kvp.Value); } } configFile.Save(ConfigurationSaveMode.Modified); } } } Copy Global.asax.cs file changes​ The Global. asax file is a special file that contains event handlers for ASP.NET application lifecycle events. Below is a example Global.asax.cs with required code changes highlighted in blue colour. Global.asax.cs​  Create object for Sftrace_class and call the method Sftrace_method in Global.asax.cs file. Verify Example Global.asax.cs to identify in which place below code should be placed. Sftrace_class sftrace_obj = new Sftrace_class(); sftrace_obj.Sftrace_method(); Copy caution Make sure to add Sftrace function call as first line above other register calls. web.config file changes​ web.config file is used to manage various settings that define a website. Below is a example web.config with required code changes highlighted in blue colour. web.config​  Add ElasticApm module in web.config file. Refer Example web.config . &lt;add name=&quot;ElasticApmModule&quot; type=&quot;Elastic.Apm.AspNetFullFramework.ElasticApmModule, Elastic.Apm.AspNetFullFramework&quot; /&gt; Copy note If system.webserver or modules are not already in your web.config file, you can add them also. Instructions to change profile key​ warning Proceed with caution Remove preexisting elasticapm data from web.config file and rebuild the project. info Below code needs to be removed in web.config under &lt;appsettings&gt; if profile key is to be changed  &lt;add key=&quot;ElasticApm:ServerUrl&quot; value=&quot;&quot; /&gt; &lt;add key=&quot;ElasticApm:GlobalLabels&quot; value=&quot;&quot; /&gt; &lt;add key=&quot;ElasticApm:CentralConfig&quot; value=&quot;&quot; /&gt; &lt;add key=&quot;ElasticApm:VerifyServerCert&quot; value=&quot;&quot; /&gt; &lt;add key=&quot;ElasticApm:DisableMetrics&quot; value=&quot;&quot; /&gt; &lt;add key=&quot;ElasticApm:ServiceName&quot; value=&quot;&quot; /&gt; &lt;add key=&quot;ElasticApm:StackTraceLimit&quot; value=&quot;&quot; /&gt; &lt;add key=&quot;ElasticApm:SpanFramesMinDuration&quot; value=&quot;&quot; /&gt; Copy "},{"title":"Ruby tracing","type":0,"sectionRef":"#","url":"docs/Tracing/ruby","content":"","keywords":""},{"title":"Instances​","type":1,"pageTitle":"Ruby tracing","url":"docs/Tracing/ruby#instances","content":"sfTrace Ruby Agent automatically instruments applications, based on web frameworks Ruby on Rails and other Rack-compatible applications. It uses the hooks and signals provided by these frameworks to trace the application. Installation steps​ Install sfAgent (if not already installed)Confirm that /opt/sfagent/config.yaml is configured with correct profile key and tags. Trace setup for Ruby on Rails Applications​ Install sftrace agent by either adding the gem to Gemfile gem sftrace-agent and then execute the command bundle install or install the agent yourself using the command gem install sftrace-agent.Add the agent configuration file in application’s config folder. Refer to application.rb for tracing specific configuration. Search for SFTRACE-CONFIG in sample application.rb  "},{"title":"Kubernetes​","type":1,"pageTitle":"Ruby tracing","url":"docs/Tracing/ruby#kubernetes","content":"Follow the steps below to enable tracing in Ruby on Rails applications running as a Kubernetes pod Follow the first 2 steps in Trace setup for Ruby on Rails Applications to update the application with agent specific configuration. sfTrace ruby agent configuration can be set to the application running in Kubernetes pod. This can be done in 2 ways: Option 1: manifest deployment​ Refer to ruby_k8s_manifest_deployment.yaml to copy trace agent configuration to the application container and start the container with trace agent configurations. Search for SFTRACE-CONFIG in sample deployment.yaml file Once updated, deploy the pod. Option 2: Deploy using helm chart​ Step 1: Update values.yaml Refer to k8s_with_helm_chart_values.yaml to configure agent specific properties. Search for SFTRACE-CONFIG in sample values.yaml file Step 2: Update deployment.yaml Refer to ruby_k8s_with_helm_chart_deployment.yaml to copy trace agent to the application container and start the container using the trace agent configurations. Search for SFTRACE-CONFIG in sample deployment.yaml file Once updated, deploy the pod. "},{"title":"Go Tracing","type":0,"sectionRef":"#","url":"docs/Tracing/go","content":"","keywords":""},{"title":"Available Platforms​","type":1,"pageTitle":"Go Tracing","url":"docs/Tracing/go#available-platforms","content":"Instances Kubernetes Docker ECS For Log Correlation, scroll to the bottom of this page or click here "},{"title":"Instances​","type":1,"pageTitle":"Go Tracing","url":"docs/Tracing/go#instances","content":""},{"title":"Supported web frameworks​","type":1,"pageTitle":"Go Tracing","url":"docs/Tracing/go#supported-web-frameworks","content":"Framework\tSupported versionsGoji\tv1.0.1 "},{"title":"Goji​","type":1,"pageTitle":"Go Tracing","url":"docs/Tracing/go#goji","content":"Pre-requisite Go of version &gt;= 1.15.0 installed. Install the Elastic APM Go Agent. go get go.elastic.co/apm/v2 Copy Handling the environment variables for Elastic APM Go Agent Import the github.com/snappyflow/go-sf-apm-lib package as blank import in your project. import _ &quot;github.com/snappyflow/go-sf-apm-lib&quot; Copy Set the SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY environment variables with project name, app name, and profile key respectively. If environment variables are not provided, the values are alternatively taken from sfagent's config.yaml file instead if it exists. Always import this package before any elastic instrumentation packages. Also import this package in all the files wherever any elastic instrumentation package is imported to avoid re-initialisation of environment variables. Example: import ( _ &quot;github.com/snappyflow/go-sf-apm-lib&quot; // rest of the packages ) Copy Adding instrumentation for Goji application Import the apmgoji package in your current project. github.com/snappyflow/sf-elastic-apm-go/module/apmgoji Copy Add the middleware provided by the ampgoji instrumentation module to the middleware stack. import ( _ &quot;github.com/snappyflow/go-sf-apm-lib&quot; &quot;github.com/snappyflow/sf-elastic-apm-go/module/apmgoji&quot; ) func main() { goji.Use(goji.DefaultMux.Router) goji.Use(apmgoji.Middleware()) ... } Copy Use the Router middleware to let the tracer determine the routes. By default, the service name is taken as the executable name. If you want to provide a different service name, set the ELASTIC_APM_SERVICE_NAME environment variable before running your Goji application. Once your application is up and running you can check the trace data on Snappyflow Server. For viewing trace in snappyflow server make sure project and app are created or discovered with the project name and app name specified in the environment variables SF_PROJECT_NAME and SF_APP_NAME. Once project and app are created, go to View Dashboard -&gt; Click on Tracing on left side bar -&gt; Click on View Transaction -&gt; Go to Real Time tab. To know how to trace outgoing HTTP requests and DB transactions, click here To know how to trace internal function calls or code blocks in your application code, click here To know how to capture and trace errors, click here To know about context propagation, click here For complete code, refer sample Goji app at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-goji "},{"title":"Kubernetes​","type":1,"pageTitle":"Go Tracing","url":"docs/Tracing/go#kubernetes","content":""},{"title":"Supported web frameworks​","type":1,"pageTitle":"Go Tracing","url":"docs/Tracing/go#supported-web-frameworks-1","content":"Framework\tSupported versionsGoji\tv1.0.1 "},{"title":"Goji​","type":1,"pageTitle":"Go Tracing","url":"docs/Tracing/go#goji-1","content":"Pre-requisite Go of version &gt;= 1.15.0 installed. Install the Elastic APM Go Agent. go get go.elastic.co/apm/v2 Copy Handling the environment variables for Elastic APM Go Agent Import the github.com/snappyflow/go-sf-apm-lib package as blank import in your project. import _ &quot;github.com/snappyflow/go-sf-apm-lib&quot; Copy The values for SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY must be provided as environment variables in Kubernetes deployment file. You can refer: https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/ If deploying with helm provide above variables in values.yaml and use them in deployment file of charts. You can refer: https://phoenixnap.com/kb/helm-environment-variables Always import this package before any elastic instrumentation packages. Also import this package in all the files wherever any elastic instrumentation package is imported to avoid re-initialisation of environment variables. Example: import ( _ &quot;github.com/snappyflow/go-sf-apm-lib&quot; // rest of the packages ) Copy Adding instrumentation for Goji application Import the apmgoji package in your current project. github.com/snappyflow/sf-elastic-apm-go/module/apmgoji Copy Add the middleware provided by the ampgoji instrumentation module to the middleware stack. import ( _ &quot;github.com/snappyflow/go-sf-apm-lib&quot; &quot;github.com/snappyflow/sf-elastic-apm-go/module/apmgoji&quot; ) func main() { goji.Use(goji.DefaultMux.Router) goji.Use(apmgoji.Middleware()) ... } Copy Use the Router middleware to let the tracer determine the routes. By default, the service name is taken as the executable name. If you want to provide a different service name, set the ELASTIC_APM_SERVICE_NAME environment variable before running your Goji application. Once your application is up and running you can check the trace data on Snappyflow Server. For viewing trace in snappyflow server make sure project and app are created or discovered with the project name and app name specified in the environment variables SF_PROJECT_NAME and SF_APP_NAME. Once project and app are created, go to View Dashboard -&gt; Click on Tracing on left side bar -&gt; Click on View Transaction -&gt; Go to Real Time tab. To know how to trace outgoing HTTP requests and DB transactions, click here To know how to trace internal function calls or code blocks in your application code, click here To know how to capture and trace errors, click here To know about context propagation, click here For complete code, refer sample Goji app at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-goji "},{"title":"Docker​","type":1,"pageTitle":"Go Tracing","url":"docs/Tracing/go#docker","content":""},{"title":"Supported web frameworks​","type":1,"pageTitle":"Go Tracing","url":"docs/Tracing/go#supported-web-frameworks-2","content":"Framework\tSupported versionsGoji\tv1.0.1 "},{"title":"Goji​","type":1,"pageTitle":"Go Tracing","url":"docs/Tracing/go#goji-2","content":"Pre-requisite Go of version &gt;= 1.15.0 installed. Install the Elastic APM Go Agent. go get go.elastic.co/apm/v2 Copy Handling the environment variables for Elastic APM Go Agent Import the github.com/snappyflow/go-sf-apm-lib package as blank import in your project. import _ &quot;github.com/snappyflow/go-sf-apm-lib&quot; Copy The values for SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY must be provided as environment variables in docker-compose.yml or at command line when using docker run command for deployment. Docker run command example: docker run -d -t -i -e SF_PROJECT_NAME='' \\ -e SF_APP_NAME='' \\ -e SF_PROFILE_KEY='' \\ -p 80:80 \\ --name &lt;container_name&gt; &lt;image_name&gt; Copy Always import this package before any elastic instrumentation packages. Also import this package in all the files wherever any elastic instrumentation package is imported to avoid re-initialisation of environment variables. Example: import ( _ &quot;github.com/snappyflow/go-sf-apm-lib&quot; // rest of the packages ) Copy Adding instrumentation for Goji application Import the apmgoji package in your current project. github.com/snappyflow/sf-elastic-apm-go/module/apmgoji Copy Add the middleware provided by the ampgoji instrumentation module to the middleware stack. import ( _ &quot;github.com/snappyflow/go-sf-apm-lib&quot; &quot;github.com/snappyflow/sf-elastic-apm-go/module/apmgoji&quot; ) func main() { goji.Use(goji.DefaultMux.Router) goji.Use(apmgoji.Middleware()) ... } Copy Use the Router middleware to let the tracer determine the routes. By default, the service name is taken as the executable name. If you want to provide a different service name, set the ELASTIC_APM_SERVICE_NAME environment variable before running your Goji application. Once your application is up and running you can check the trace data on Snappyflow Server. For viewing trace in snappyflow server make sure project and app are created or discovered with the project name and app name specified in the environment variables SF_PROJECT_NAME and SF_APP_NAME. Once project and app are created, go to View Dashboard -&gt; Click on Tracing on left side bar -&gt; Click on View Transaction -&gt; Go to Real Time tab. To know how to trace outgoing HTTP requests and DB transactions, click here To know how to trace internal function calls or code blocks in your application code, click here To know how to capture and trace errors, click here To know about context propagation, click here For complete code, refer sample Goji app at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-goji "},{"title":"ECS​","type":1,"pageTitle":"Go Tracing","url":"docs/Tracing/go#ecs","content":""},{"title":"Supported web frameworks​","type":1,"pageTitle":"Go Tracing","url":"docs/Tracing/go#supported-web-frameworks-3","content":"Framework\tSupported versionsGoji\tv1.0.1 "},{"title":"Goji​","type":1,"pageTitle":"Go Tracing","url":"docs/Tracing/go#goji-3","content":"Pre-requisite Go of version &gt;= 1.15.0 installed. Install the Elastic APM Go Agent. go get go.elastic.co/apm/v2 Copy Handling the environment variables for Elastic APM Go Agent Import the github.com/snappyflow/go-sf-apm-lib package as blank import in your project. import _ &quot;github.com/snappyflow/go-sf-apm-lib&quot; Copy Provide SF_PROJECT_NAME, SF_APP_NAME, and SF_PROFILE_KEY as environment variables in add container section of task definitions. Always import this package before any elastic instrumentation packages. Also import this package in all the files wherever any elastic instrumentation package is imported to avoid re-initialisation of environment variables. Example: import ( _ &quot;github.com/snappyflow/go-sf-apm-lib&quot; // rest of the packages ) Copy Adding instrumentation for Goji application Import the apmgoji package in your current project. github.com/snappyflow/sf-elastic-apm-go/module/apmgoji Copy Add the middleware provided by the ampgoji instrumentation module to the middleware stack. import ( _ &quot;github.com/snappyflow/go-sf-apm-lib&quot; &quot;github.com/snappyflow/sf-elastic-apm-go/module/apmgoji&quot; ) func main() { goji.Use(goji.DefaultMux.Router) goji.Use(apmgoji.Middleware()) ... } Copy Use the Router middleware to let the tracer determine the routes. By default, the service name is taken as the executable name. If you want to provide a different service name, set the ELASTIC_APM_SERVICE_NAME environment variable before running your Goji application. Once your application is up and running you can check the trace data on Snappyflow Server. For viewing trace in snappyflow server make sure project and app are created or discovered with the project name and app name specified in the environment variables SF_PROJECT_NAME and SF_APP_NAME. Once project and app are created, go to View Dashboard -&gt; Click on Tracing on left side bar -&gt; Click on View Transaction -&gt; Go to Real Time tab. To know how to trace outgoing HTTP requests and DB transactions, click here To know how to trace internal function calls or code blocks in your application code, click here To know how to capture and trace errors, click here To know about context propagation, click here For complete code, refer sample Goji app at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-goji "},{"title":"Outgoing HTTP requests and DB transactions​","type":1,"pageTitle":"Go Tracing","url":"docs/Tracing/go#outgoing-http-requests-and-db-transactions","content":"You can follow these steps only if you are making outgoing HTTP requsts and DB transactions in your application and want to trace them. This is independent of the web framework used. Tracing outgoing HTTP requests Import the following packages. go.elastic.co/apm/module/apmhttp/v2 go.elastic.co/apm/v2 Copy Instrument the HTTP client using apmhttp.WrapClient function. client := apmhttp.WrapClient(http.DefaultClient) Copy Start a span with the current request context. span, ctx := apm.StartSpan(req.Context(), &quot;newSpan&quot;, &quot;custom&quot;) defer span.End() Copy Propagate this context to the outgoing request. resp, err := client.Do(req.WithContext(ctx)) Copy These lines have to be added in the handler functions wherever outgoing HTTP requests are being made. You can refer the following handler function from the Goji reference app: https://github.com/snappyflow/tracing-reference-apps/blob/master/refapp-goji/handlers.go#L48 Tracing SQL DB transactions Import the required database instrumentation module, say postgres for example. go.elastic.co/apm/module/apmsql/v2/pq Copy Open a database connection using the apmsql.Open function. import ( &quot;go.elastic.co/apm/module/apmsql/v2&quot; _ &quot;go.elastic.co/apm/module/apmsql/v2/pq&quot; ) func main() { db, err := apmsql.Open(&quot;postgres&quot;, &quot;postgres://...&quot;) ... } Copy Create spans Spans will be created for queries and other statement executions if the context methods are used, and the context includes a transaction. func handleRequest(w http.ResponseWriter, r *http.Request) { tx, err := db.BeginTx(r.Context(), nil) if err != nil { // handle error } result, err := tx.QueryContext(r.Context(), &quot;SELECT * FROM customers&quot;) ... } Copy apmsql provides support for the following popular drivers. module/apmsql/pq (github.com/lib/pq)module/apmsql/pgxv4 (github.com/jackc/pgx/v4/stdlib)module/apmsql/mysql (github.com/go-sql-driver/mysql)module/apmsql/sqlite3 (github.com/mattn/go-sqlite3) Tracing MongoDB transactions Import the following instrumentation module. github.com/snappyflow/sf-elastic-apm-go/module/apmmongo Copy This package provides the means of instrumenting the mongodb/mongo-go-driver, so that MongoDB commands are reported as spans within the current transaction. To create spans for MongoDB commands, pass in a CommandMonitor created with apmmongo.CommandMonitor as an option when constructing a client. var client, _ = mongo.Connect( context.Background(), options.Client().SetMonitor(apmmongo.CommandMonitor()).ApplyURI(&quot;mongodb://localhost:27017&quot;), ) Copy When executing the commands, pass in a context containing the current transaction to capture spans within this transaction. import ( &quot;context&quot; &quot;net/http&quot; &quot;go.mongodb.org/mongo-driver/bson&quot; &quot;go.mongodb.org/mongo-driver/mongo&quot; &quot;go.mongodb.org/mongo-driver/mongo/options&quot; &quot;go.elastic.co/apm/v2&quot; &quot;github.com/snappyflow/sf-elastic-apm-go/module/apmmongo&quot; ) var client, _ = mongo.Connect( context.Background(), options.Client().SetMonitor(apmmongo.CommandMonitor()).ApplyURI(&quot;mongodb://localhost:27017&quot;), ) func handleRequest(w http.ResponseWriter, req *http.Request) { collection := client.Database(&quot;db&quot;).Collection(&quot;coll&quot;) cur, err := collection.Find(req.Context(), bson.D{}) ... } Copy Tracing Redis transactions Import the following instrumentation module. github.com/snappyflow/sf-elastic-apm-go/module/apmgoredisv8 Copy This package provides a means of instrumenting go-redis/redis for v8 so that Redis commands are reported as spans within the current transaction. To report Redis commands as spans, call AddHook(apmgoredis.NewHook()) from instance of the Redis client to use the hook provided by apmgoredisv8 module. redisClient.AddHook(apmgoredisv8.NewHook()) Copy When executing the commands, pass in a context containing the current transaction to capture spans within this transaction. import ( &quot;github.com/go-redis/redis/v8&quot; &quot;github.com/snappyflow/sf-elastic-apm-go/module/apmgoredisv8&quot; ) var redisClient = redis.NewClient(&amp;redis.Options{ Addr: &quot;localhost:6379&quot;, Password: &quot;&quot;, DB: 0, }) func main() { redisClient.AddHook(apmgoredisv8.NewHook()) ... } func handleRequest(w http.ResponseWriter, req *http.Request) { val, err := redisClient.Get(req.Context(), &quot;key&quot;).Result() ... } Copy Tracing ElasticSearch DB transactions Import the following instrumentation module. go.elastic.co/apm/module/apmelasticsearch/v2 Copy Wrap the client’s HTTP transport using the apmelasticsearch.WrapRoundTripper function. Associate the elasticsearch request with the current transaction context. import ( &quot;net/http&quot; &quot;github.com/olivere/elastic&quot; &quot;go.elastic.co/apm/module/apmelasticsearch/v2&quot; ) var client, _ = elastic.NewClient(elastic.SetHttpClient(&amp;http.Client{ Transport: apmelasticsearch.WrapRoundTripper(http.DefaultTransport), })) func handleRequest(w http.ResponseWriter, req *http.Request) { result, err := client.Search(&quot;index&quot;).Query(elastic.NewMatchAllQuery()).Do(req.Context()) ... } Copy For other instrumentation modules, refer https://www.elastic.co/guide/en/apm/agent/go/2.x/builtin-modules.html "},{"title":"Tracing Errors​","type":1,"pageTitle":"Go Tracing","url":"docs/Tracing/go#tracing-errors","content":"Follow these steps only if you want to trace errors in your application. Import the following package. go.elastic.co/apm/v2 Copy Use apm.CaptureError function and pass the current request context to report errors to APM server. if err != nil { e := apm.CaptureError(req.Context(), err) e.Send() } Copy "},{"title":"Custom Instrumentation​","type":1,"pageTitle":"Go Tracing","url":"docs/Tracing/go#custom-instrumentation","content":"Spans can be created to trace a function call or operation/activity within a transaction. apm.StartSpan starts and returns a new span within the transaction, with the specified name and type. span, ctx := apm.StartSpan(req.Context, &quot;myFunc&quot;, &quot;custom.internal.functionCall&quot;) // Here, &quot;muFunc&quot; is the span name, &quot;custom&quot; is the span type, &quot;internal&quot; is the subtype, and &quot;functionCall&quot; is the action. // If the span type contains two dots, they are assumed to separate the span type, subtype, and action // A single dot separates span type and subtype, and the action will not be set. // span type can also be set to just a string with no dots. In this case subtype and action are set to null. Copy So you can use the following pattern to trace internal operations or a block of code. span, ctx := apm.StartSpan(req.Context(), &quot;internal operation&quot;, &quot;custom&quot;) // ... code or operation you want to trace here ... // It is important to end the span to mark it as complete. span.End() Copy The span’s duration will be calculated as the amount of time elapsed since the span was started until this call. You can also add nested spans by passing the parent span's context in the child span creation. The span being ended must be the most recently started span. parentSpan, ctx := apm.StartSpan(req.Context(), &quot;myFunc&quot;, &quot;custom&quot;) // ... code or operation you want to trace here ... childSpan, _ := apm.StartSpan(ctx, &quot;internal operation&quot;, &quot;custom&quot;) // ... code or operation you want to trace here ... // childSpan must end before parentSpan childSpan.End() parentSpan.End() Copy If the context contains neither a transaction nor a span, then the span will be dropped (i.e. will not be reported to the SnappyFlow server). You can also trace the function calls and there duration within a transaction using the same steps. import ( &quot;go.elastic.co/apm/v2&quot; ) func handleTest(w http.ResponseWriter, req *http.Request) { myFunc(req.Context()) ... } func myFunc(ctx context.Context) { span, _ := apm.StartSpan(ctx, &quot;myFunc&quot;, &quot;custom.internal.functionCall&quot;) defer span.End() ... } Copy You can refer the reference Goji app for an example: https://github.com/snappyflow/tracing-reference-apps/blob/master/refapp-goji/handlers.go#L39 "},{"title":"Context Propagation​","type":1,"pageTitle":"Go Tracing","url":"docs/Tracing/go#context-propagation","content":"In Go, for each incoming request, a transaction will be started and added to the request context automatically. So, the http.Request object will contain the current transaction context. This context needs to be passed into method calls within the handler manually in order to create spans within that transaction, e.g. to trace DB queries. Note: If the context contains neither a transaction nor a span, then the span will be dropped (i.e. will not be reported to the SnappyFlow server.) For example, import ( &quot;context&quot; &quot;net/http&quot; _ &quot;github.com/snappyflow/go-sf-apm-lib&quot; &quot;github.com/snappyflow/sf-elastic-apm-go/module/apmgoji&quot; &quot;github.com/zenazn/goji&quot; &quot;go.mongodb.org/mongo-driver/bson&quot; &quot;go.mongodb.org/mongo-driver/mongo&quot; &quot;go.mongodb.org/mongo-driver/mongo/options&quot; &quot;github.com/snappyflow/sf-elastic-apm-go/module/apmmongo&quot; ) var mongoClient, _ = mongo.Connect( context.Background(), options.Client().SetMonitor(apmmongo.CommandMonitor()).ApplyURI(&quot;mongodb://localhost:27017&quot;), ) func main() { goji.Use(goji.DefaultMux.Router) goji.Use(apmgoji.Middleware()) goji.Get(&quot;/mongo&quot;, handleMongoDB) goji.Serve() } func handleMongoDB(w http.ResponseWriter, req *http.Request) { // By passing the request context down to getDocs, getDocs can make DB queries which will be traced within this context. ctx := req.Context() getDocs(ctx) ... } func getDocs(ctx context.Context) { collection := mongoClient.Database(&quot;db&quot;).Collection(&quot;coll&quot;) cur, err := collection.Find(ctx, bson.D{}) ... cur.Close(ctx) } Copy For more details refer the documentation at: https://www.elastic.co/guide/en/apm/agent/go/2.x/custom-instrumentation-propagation.html "},{"title":"Log Correlation​","type":1,"pageTitle":"Go Tracing","url":"docs/Tracing/go#log-correlation","content":"For enabling log correlation, follow below instructions. "},{"title":"For Go's standard log package​","type":1,"pageTitle":"Go Tracing","url":"docs/Tracing/go#for-gos-standard-log-package","content":"Import the following package to get the trace IDs. go.elastic.co/apm/v2 Copy The following fields must be added in the logs: trace.id transaction.id These values are available from the current transaction context. You can use a middleware function to get the current transaction context and trace IDs. Example middleware function in Goji: // GetContext is a middleware that gets the current request context and trace IDs func GetContext(h http.Handler) http.Handler { fn := func(w http.ResponseWriter, r *http.Request) { labels := make(map[string]string) tx := apm.TransactionFromContext(r.Context()) if tx != nil { traceContext := tx.TraceContext() labels[&quot;trace.id&quot;] = traceContext.Trace.String() labels[&quot;transaction.id&quot;] = traceContext.Span.String() if span := apm.SpanFromContext(ctx); span != nil { labels[&quot;span.id&quot;] = span.TraceContext().Span } else { labels[&quot;span.id&quot;] = &quot;None&quot; } } h.ServeHTTP(w, r) } return http.HandlerFunc(fn) } Copy Here apm.TransactionFromContext and apm.SpanFromContext functions are called to get the current trace ID, transaction ID, and span ID. You can then add these trace IDs obtained from the current transaction context to your log lines. Middleware structure may differ for other web frameworks, but only the current transaction context is needed in all the cases. You can refer this middleware function from reference Goji app: https://github.com/snappyflow/tracing-reference-apps/blob/master/refapp-goji/middleware.go#L8 If you do not want to use a middleware function, you will have to call the apm.TransactionFromContext function with the current request context everytime in all the handler functions to get the trace IDs. You can refer code at: https://github.com/snappyflow/tracing-reference-apps/blob/master/refapp-goji/logger.go "},{"title":"For Logrus logging framework​","type":1,"pageTitle":"Go Tracing","url":"docs/Tracing/go#for-logrus-logging-framework","content":"Import the apmlogrus module. go.elastic.co/apm/module/apmlogrus/v2 Copy Add the following trace context fields to the logs. trace.id transaction.id These values are available from the current transaction context. You can use a middleware function to get the current transaction context and trace IDs. Example middleware function in Goji: // GetContext is a middleware that gets the current request context and trace IDs func GetContext(h http.Handler) http.Handler { fn := func(w http.ResponseWriter, r *http.Request) { labels := make(map[string]string) traceContextFields := apmlogrus.TraceContext(r.Context()) labels[&quot;trace.id&quot;] = traceContextFields[&quot;trace.id&quot;].String() labels[&quot;transaction.id&quot;] = traceContextFields[&quot;transaction.id&quot;].String() if _, ok := traceContextFields[&quot;span.id&quot;]; ok { labels[&quot;span.id&quot;] = traceContextFields[&quot;span.id&quot;].String() } else { labels[&quot;span.id&quot;] = &quot;None&quot; } h.ServeHTTP(w, r) } return http.HandlerFunc(fn) } Copy Here apmlogrus.TraceContext function is called to get the current trace ID, transaction ID, and span ID. You can then add these trace IDs obtained from the current transaction context to your log lines. Middleware structure may differ for other web frameworks, but only the current transaction context is needed in all the cases. You can refer to this middleware function from reference Goji app: https://github.com/snappyflow/tracing-reference-apps/blob/master/refapp-goji/middleware.go#L17 If you do not want to use a middleware, you will have to call the apm.TransactionFromContext function with the current request context everytime in all the handler functions to get the trace IDs. Example: import ( &quot;github.com/sirupsen/logrus&quot; &quot;go.elastic.co/apm/module/apmlogrus/v2&quot; ) func handleRequest(w http.ResponseWriter, req *http.Request) { traceContextFields := apmlogrus.TraceContext(req.Context()) logrus.WithFields(traceContextFields).Debug(&quot;handling request&quot;) } Copy You can refer the code at: https://github.com/snappyflow/tracing-reference-apps/blob/master/refapp-goji/logrus.go "},{"title":"Send log correlation data to snappyflow server​","type":1,"pageTitle":"Go Tracing","url":"docs/Tracing/go#send-log-correlation-data-to-snappyflow-server","content":""},{"title":"For Appliance​","type":1,"pageTitle":"Go Tracing","url":"docs/Tracing/go#for-appliance","content":"Install sfagent and create config file. Refer: https://docs.snappyflow.io/docs/Integrations/os/linux/sfagent_linux Enable the elasticApmTraceLog plugin and restart sfagent service. Example config.yaml: key: &lt;SF_PROFILE_KEY&gt; tags: Name: &lt;any-name&gt; appName: &lt;SF_APP_NAME&gt; projectName: &lt;SF_PROJECT_NAME&gt; logging: plugins: - name: elasticApmTraceLog enabled: true config: log_level: - error - warning - info log_path: /var/log/trace/goji.log # Your app log file path Copy Note: sfagent is required only to send log correlation data to the snappyflow server. It is not needed to trace transactions. "},{"title":"For Kubernetes​","type":1,"pageTitle":"Go Tracing","url":"docs/Tracing/go#for-kubernetes","content":"Specify following values in metadata labels section of deployment file. snappyflow/appname: &lt;SF_APP_NAME&gt; snappyflow/projectname: &lt;SF_PROJECT_NAME&gt; snappyflow/component: gen-elastic-apm-log # This is must for tracing log correlation snappyflow/component: aks-gen-elastic-apm-log # Set this value if deploying app in Azure Kubernetes Copy Sample deployment file: apiVersion: apps/v1 kind: Deployment metadata: labels: io.kompose.service: python-app snappyflow/appname: '&lt;sf_app_name&gt;' snappyflow/projectname: '&lt;sf_project_name&gt;' snappyflow/component: gen-elastic-apm-log name: python-app spec: replicas: 1 selector: matchLabels: io.kompose.service: python-app strategy: {} template: metadata: labels: io.kompose.service: python-app snappyflow/appname: '&lt;sf_app_name&gt;' snappyflow/projectname: '&lt;sf_project_name&gt;' snappyflow/component: gen-elastic-apm-log spec: containers: - env: - name: SF_APP_NAME value: '&lt;sf_app_name&gt;' - name: SF_PROFILE_KEY value: '&lt;sf_profile_key&gt;' - name: SF_PROJECT_NAME value: '&lt;sf_project_name&gt;' image: refapp-node:latest imagePullPolicy: Always name: python-app ports: - containerPort: 3000 resources: requests: cpu: 10m memory: 10Mi limits: cpu: 50m memory: 50Mi restartPolicy: Always Copy Note: For kubernetes mode we need sfagent pods to be running inside kubernetes cluster where your application pods are deployed. For viewing the logs in Snappyflow server make sure project and app are created or discovered with the same names as SF_PROJECT_NAME and SF_APP_NAME. Once project and app name is created, Go to: View App Dashboard -&gt; Click on Tracing on left side bar -&gt; Click on view Transaction -&gt; Go to Real Time tab. Then click on any trace and go to Logs tab to see the correlated logs to trace. NOTE: To get trace logs in snappyflow server we need log entries to have the following log format: &lt;date in following format and in UTC&gt; [10/Aug/2021 10:51:16] [&lt;log_level&gt;] [&lt;message&gt;] | elasticapm transaction.id=&lt;transaction_id&gt; trace.id=&lt;trace_id&gt; span.id=&lt;span_id&gt; Copy "},{"title":"ASP.NET Core application","type":0,"sectionRef":"#","url":"docs/Tracing/aspdotnetcore","content":"","keywords":""},{"title":"Available Platforms​","type":1,"pageTitle":"ASP.NET Core application","url":"docs/Tracing/aspdotnetcore#available-platforms","content":"Instances Containers "},{"title":"Instances​","type":1,"pageTitle":"ASP.NET Core application","url":"docs/Tracing/aspdotnetcore#instances","content":""},{"title":"Supported web frameworks​","type":1,"pageTitle":"ASP.NET Core application","url":"docs/Tracing/aspdotnetcore#supported-web-frameworks","content":"Framework\tSupported versionsASP.NET Core\t.NET core 2.1, 3.1, .NET 5.0 "},{"title":"Prerequisite​","type":1,"pageTitle":"ASP.NET Core application","url":"docs/Tracing/aspdotnetcore#prerequisite","content":"Install the following Nuget packages Elastic.Apm.NetCoreAllSnappyflow.NetCoretrace.Utility These packages can be installed using Nuget package manager in Visual Studio or using .NET CLI commands given below. dotnet add package SnappyFlow.NetCoretrace.Utility --version 0.1.6 Copy dotnet add package Elastic.Apm.NetCoreAll --version 1.12.1 Copy "},{"title":"Steps to configure application​","type":1,"pageTitle":"ASP.NET Core application","url":"docs/Tracing/aspdotnetcore#steps-to-configure-application","content":"Create sftraceConfig.&lt;env_name&gt;.yaml file inside wwwroot directory. Copy below sftraceConfig.&lt;env_name&gt;.yaml content and configure it with correct profile key and tags. Get profile key from the Manage--&gt;profile in snappyflow portal. Create project and application(or use existing project and applicaition) using your profile. You can provide any name to service, this will be displayed in Trace Dashboard. Change &lt;env_name&gt; to your current working environment. For example if your environment is Development, your file name should be sftraceConfig.Development.yaml sftraceConfig.&lt;env_name&gt;.yaml​ tags: projectName: CHANGEME appName: CHANGEME serviceName: CHANGEME key: CHANGEME Copy note Development is a common environment used while developing a web application, when you are using it in Production or any custom environmnent, you can change the yaml file name. Startup.cs file changes​ ASP.NET Core application contains Startup class. It is like Global.asax in the traditional .NET application. As the name suggests, it is executed first when the application starts. Below is a example Startup.cs with required code changes highlighted in blue colour. Startup.cs​  Copy the below codes in your Startup.cs file similar to the example above. using Elastic.Apm.NetCoreAll; Copy Replace CHANGEME in code below with your application's environment (Development/Production) and add it inside configure method. if (env.EnvironmentName == &quot;CHANGEME&quot;) { app.UseAllElasticApm(Configuration); } Copy info If you don't want to instrument for specific environment, add below line alone inside configure method. app.UseAllElasticApm(Configuration); Copy caution Make sure to add app.UseAllElasticApm(Configuration) as first line in configuration method, otherwise the agent won’t be able to properly measure the timing of requests. Program.cs file changes​ ASP.NET Core web application is actually a console project which starts executing from the entry point public static void Main() in Program class where we can create a host for the web application. Below is a example Program.cs with required code changes highlighted in blue colour. Program.cs​  Copy the below codes in your Program.cs file similar to the example above. using SftraceDotNetcore; Copy The below code should be added inside public static IHostBuilder CreateHostBuilder method in the same way how it's done in example file. Verify Example Program.cs to identify in which place below code should be placed. .ConfigureAppConfiguration((hostingContext, config) =&gt; { try { var env = hostingContext.HostingEnvironment; string sftraceConfigfile = $&quot;sftraceConfig.{env.EnvironmentName}.yaml&quot;; config.AddInMemoryCollection(sftracedecrypt.Trace(sftraceConfigfile)); } catch (Exception err) { Console.WriteLine(&quot;Error occurred in Snappyflow application trace&quot; + err.Message); } }) Copy "},{"title":"Containers​","type":1,"pageTitle":"ASP.NET Core application","url":"docs/Tracing/aspdotnetcore#containers","content":""},{"title":"Supported Web frameworks​","type":1,"pageTitle":"ASP.NET Core application","url":"docs/Tracing/aspdotnetcore#supported-web-frameworks-1","content":"Framework\tSupported versionsASP.NET Core\t.NET core 2.1, 3.1, .NET 5.0 "},{"title":"Prerequisite​","type":1,"pageTitle":"ASP.NET Core application","url":"docs/Tracing/aspdotnetcore#prerequisite-1","content":"Install the following Nuget packages Elastic.Apm.AspNetFullFrameworkSnappyflow.NetCoretrace.Utility These packages can be installed using Nuget package manager in Visual Studio or using .NET CLI commands given below. dotnet add package SnappyFlow.NetCoretrace.Utility --version 0.1.6 Copy dotnet add package Elastic.Apm.AspNetFullFramework --version 1.12.1 Copy Enable the Docker in project. For referrence follow the official documentation (Click Here) "},{"title":"Steps to configure Application​","type":1,"pageTitle":"ASP.NET Core application","url":"docs/Tracing/aspdotnetcore#steps-to-configure-application-1","content":"Copy below docker-compose.override.yml content and place it in under the environment variables in docker-compose.yml or docker-compose.override.yml. configure it with the correct profile key and tags. Get profile key from the Manage--&gt;profile in snappyflow portal. Create a project and application using your profile (or use an existing project and application). You can provide any name to service, this will be displayed in Trace Dashboard. Below is an example docker-compose.override.yml with required code changes highlighted in blue color. docker-compose.override.yml​  Copy the below lines in your docker-compose.override.yml file similar to the example above. - PROJECTNAME=CHANGEME - APPNAME=CHANGEME - SERVICENAME=CHANGEME - PROFILEKEY=CHANGEME Copy note Similar to configuring the environment variable into the docker, the same way you can configure the Kubernetes environment also. Wherever you are using the environment variable there you can add the above variables Copy Create a new class file called Sftrace_class.cs in location where web.config file is present. Copy the code below and don't forget to change CHANGENAMESPACE. Startup.cs file changes​ ASP.NET Core application contains Startup class. It is like Global.asax in the traditional .NET application. As the name suggests, it is executed first when the application starts. Below is a example Startup.cs with required code changes highlighted in blue colour. Startup.cs​  Copy the below codes in your Startup.cs file similar to the example above. using Elastic.Apm.NetCoreAll; Copy Replace CHANGEME in code below with your application's environment (Development/Production) and add it inside configure method. if (env.EnvironmentName == &quot;CHANGEME&quot;) { app.UseAllElasticApm(Configuration); } Copy info If you don't want to instrument for specific environment, add below line alone inside configure method. app.UseAllElasticApm(Configuration); Copy caution Make sure to add app.UseAllElasticApm(Configuration) as first line in configuration method, otherwise the agent won’t be able to properly measure the timing of requests. Program.cs file changes​ ASP.NET Core web application is actually a console project which starts executing from the entry point public static void Main() in the Program class where we can create a host for the web application. Below is an example Program.cs with required code changes highlighted in blue color.  Copy the below codes in your Program.cs file similar to the example above. using SftraceDotNetcore; Copy The below code should be added inside public static IHostBuilder CreateHostBuilder method in the same way how it's done in example file. Verify Example Program.cs to identify in which place below code should be placed. .ConfigureAppConfiguration((hostingContext, config) =&gt; { try { var profilekey = Environment.GetEnvironmentVariable(&quot;PROFILEKEY&quot;); var projectname = Environment.GetEnvironmentVariable(&quot;PROJECTNAME&quot;); var appname = Environment.GetEnvironmentVariable(&quot;APPNAME&quot;); var servicename = Environment.GetEnvironmentVariable(&quot;SERVICENAME&quot;); config.AddInMemoryCollection(sftracedecrypt.TraceEnv(profilekey, projectname, appname, servicename)); } catch (Exception err) { Console.WriteLine(&quot;Error occurred in Snappyflow application trace&quot; +err.Message); } } ) Copy Now run your application with the F5 key, or the Ctrl+F5 key, selecting the docker-compose project, as shown below image  Change Profile key instructions (Only when changing profile key)​ If you want to change your profile key in future, you need to remove the preexisting elasticapm data from web.config file and rebuild the project. Instructions to change profile key​ warning Proceed with caution Remove preexisting elasticapm data from web.config file and rebuild the project. info Below code needs to be removed in web.config under &lt;appsettings&gt; if profile key is to be changed  &lt;add key=&quot;ElasticApm:ServerUrl&quot; value=&quot;&quot; /&gt; &lt;add key=&quot;ElasticApm:GlobalLabels&quot; value=&quot;&quot; /&gt; &lt;add key=&quot;ElasticApm:CentralConfig&quot; value=&quot;&quot; /&gt; &lt;add key=&quot;ElasticApm:VerifyServerCert&quot; value=&quot;&quot; /&gt; &lt;add key=&quot;ElasticApm:DisableMetrics&quot; value=&quot;&quot; /&gt; &lt;add key=&quot;ElasticApm:ServiceName&quot; value=&quot;&quot; /&gt; &lt;add key=&quot;ElasticApm:StackTraceLimit&quot; value=&quot;&quot; /&gt; &lt;add key=&quot;ElasticApm:SpanFramesMinDuration&quot; value=&quot;&quot; /&gt; Copy "},{"title":"NodeJS tracing","type":0,"sectionRef":"#","url":"docs/Tracing/nodejs","content":"","keywords":""},{"title":"Instances​","type":1,"pageTitle":"NodeJS tracing","url":"docs/Tracing/nodejs#instances","content":""},{"title":"Node.JS Express​","type":1,"pageTitle":"NodeJS tracing","url":"docs/Tracing/nodejs#nodejs-express","content":"Install nodejs dependencies and save it in package.json using npm install --save elastic-apm-node@^3.20.0 npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries &quot;elastic-apm-node&quot;: &quot;^3.20.0&quot; &quot;sf-apm-lib&quot;: &quot;^1.0.2&quot; Copy and run npm install to install dependencies Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variable in .env file and load it using require('dotenv').config() and access it in code using process.env.&lt;ENV_VAR&gt; Add initilization code at start of the file Get Snappyflow trace config using const Snappyflow = require('sf-apm-lib'); var sfObj = new Snappyflow(); // Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml. // Add below part to manually configure the initialization let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; sfObj.init(profileKey, projectName, appName); // Manual override let sfTraceConfig = sfObj.getTraceConfig(); // Start Trace to log feature section // Add below line of code to enable Trace to log feature: sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_redact_body=true' // Option Configs for trace to log // Add below line to provide custom documentType (Default:&quot;user-input&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_documentType=&lt;document-type&gt;' // Add below line to provide destination index (Default:&quot;log&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_IndexType=&lt;index-type&gt;' // Applicable values(log, metric) // End trace to log section Copy Initialize apm object using var apm; try { apm = require('elastic-apm-node').start({ serviceName: '&lt;SERVICE_NAME&gt;', // Specify your service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'], metricsInterval: '0s', usePathAsTransactionName: true }) } catch (e) { console.log(e); } Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in add container section of task definitions. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in Snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to: View dashboard -&gt; Click on Tracing on left side bar -&gt; Click on view transaction -&gt; Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-express Note: 'captureBody':'all' config should be present in apm agent code instrumentation for Trace to Log feature.  "},{"title":"Node.JS Script​","type":1,"pageTitle":"NodeJS tracing","url":"docs/Tracing/nodejs#nodejs-script","content":"Install nodejs dependencies and save it in package.json using npm install --save elastic-apm-node@^3.20.0 npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries: &quot;elastic-apm-node&quot;: &quot;^3.20.0&quot; &quot;sf-apm-lib&quot;: &quot;^1.0.2&quot; Copy And run npm install to install dependencies Add initilization code at start of the file Get Snappyflow trace config using const Snappyflow = require('sf-apm-lib'); let projectName = &lt;SF_PROJECT_NAME&gt;; //replace with appropriate project name let appName = &lt;SF_APP_NAME&gt;; //replace with appropriate application name let profileKey = &lt;SF_PROFILE_KEY&gt;; //replace with key copied from SF profile var sfObj = new Snappyflow(); sfObj.init(profileKey, projectName, appName); let sfTraceConfig = sfObj.getTraceConfig(); Copy Initialize apm object using var apm; try { apm = require('elastic-apm-node').start({ serviceName: '&lt;SERVICE_NAME&gt;', // Specify service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'], metricsInterval: '0s', usePathAsTransactionName: true }) } catch (e) { console.log(e); } Copy Create a custom transaction and span within transaction using following code var trans = apm.startTransaction('json transaction', 'reference-app'); var span = apm.startSpan('parse json'); try { JSON.parse('{&quot;app&quot;: &quot;test&quot;}') } catch (e) { apm.captureError(e); // Capture the error using apm.captureError(e) method. } // when we've processed, stop the custom span if (span) span.end() trans.result = err ? 'error' : 'success'; // end the transaction trans.end(); Copy For more info refer https://www.elastic.co/guide/en/apm/agent/nodejs/current/custom-transactions.html https://www.elastic.co/guide/en/apm/agent/nodejs/current/custom-spans.html Run you script using node file_name.js you should see trace data in Snappyflow server. For viewing trace in Snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to: View dashboard -&gt; Click on Tracing on left side bar -&gt; Click on view transaction -&gt; Go to real time tab Refer sample script file at: [https://github.com/snappyflow/tracing-reference-apps/blob/master/refapp- express/node_trace_script.js](https://github.com/snappyflow/tracing-reference-apps/blob/master/refapp- express/node_trace_script.js ) "},{"title":"Node.JS Sails​","type":1,"pageTitle":"NodeJS tracing","url":"docs/Tracing/nodejs#nodejs-sails","content":"Install nodejs dependencies and save it in package.json using npm install --save elastic-apm-node@^3.20.0 npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries &quot;elastic-apm-node&quot;: &quot;^3.20.0&quot; &quot;sf-apm-lib&quot;: &quot;^1.0.2&quot; Copy And run npm install to install dependencies Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variable in .env file and load it using require('dotenv').config() and access it in code using process.env.&lt;ENV_VAR&gt; Add initilization code at start of the file in globals.js present in config folder. Get Snappyflow trace config using: const Snappyflow = require('sf-apm-lib'); var sfObj = new Snappyflow(); // Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml. // Add below part to manually configure the initialization let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; sfObj.init(profileKey, projectName, appName); // Manual override let sfTraceConfig = sfObj.getTraceConfig(); // Start Trace to log feature section // Add below line of code to enable Trace to log feature: sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_redact_body=true' // Option Configs for trace to log // Add below line to provide custom documentType (Default:&quot;user-input&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_documentType=&lt;document-type&gt;' // Add below line to provide destination index (Default:&quot;log&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_IndexType=&lt;index-type&gt;' // Applicable values(log, metric) // End trace to log section Copy Initialize apm object using: var apm; try { apm = require('elastic-apm-node').start({ serviceName: '&lt;SERVICE_NAME&gt;', // Specify your service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'], metricsInterval: '0s', usePathAsTransactionName: true }) } catch (e) { console.log(e); } Copy Attach apm object to globals – This is required so we can use apm variable in other files as part of global sails object. module.exports.globals = { _: require('@sailshq/lodash'), async: false, models: true, sails: true, apm : apm, logger: logger }; Copy Also add middleware in http.js file present in config folder. Which allows to instrument our code. module.exports.http = { middleware: { order: [ 'elasticAPM' ], elasticAPM: (function () { return function (err, req, res, next) { apm.middleware.connect(); if (typeof err !== 'undefined') apm.captureError(err); return next(); }; })() } }; Copy Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -&gt; Click on Tracing on lef side bar -&gt; Click on view transaction -&gt; Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/RefappNodeSail Note: 'captureBody':'all' config should be present in apm agent code instrumentation for Trace to Log feature.  "},{"title":"Kubernetes​","type":1,"pageTitle":"NodeJS tracing","url":"docs/Tracing/nodejs#kubernetes","content":""},{"title":"Node.JS Express​","type":1,"pageTitle":"NodeJS tracing","url":"docs/Tracing/nodejs#nodejs-express-1","content":"Install nodejs dependencies and save it in package.json using npm install --save elastic-apm-node@^3.20.0 npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries: &quot;elastic-apm-node&quot;: &quot;^3.20.0&quot; &quot;sf-apm-lib&quot;: &quot;^1.0.2&quot; Copy and run npm install to install dependencies Add initilization code at start of the file in app.js Get Snappyflow trace config using const Snappyflow = require('sf-apm-lib'); var sfObj = new Snappyflow(); // Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml. // Add below part to manually configure the initialization let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; sfObj.init(profileKey, projectName, appName); // Manual override let sfTraceConfig = sfObj.getTraceConfig(); // Start Trace to log feature section // Add below line of code to enable Trace to log feature: sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_redact_body=true' // Option Configs for trace to log // Add below line to provide custom documentType (Default:&quot;user-input&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_documentType=&lt;document-type&gt;' // Add below line to provide destination index (Default:&quot;log&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_IndexType=&lt;index-type&gt;' // Applicable values(log, metric) // End trace to log section Copy Initialize apm object using var apm; try { apm = require('elastic-apm-node').start({ serviceName: '&lt;SERVICE_NAME&gt;', // Specify your service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'], metricsInterval: '0s', usePathAsTransactionName: true }) } catch (e) { console.log(e); } Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in Kubernetes deployment file. https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/ Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in Snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to: View dashboard -&gt; Click on Tracing on left side bar -&gt; Click on view transaction -&gt; Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-express Note: 'captureBody':'all' config should be present in apm agent code instrumentation for Trace to Log feature.  "},{"title":"Node.JS Sails​","type":1,"pageTitle":"NodeJS tracing","url":"docs/Tracing/nodejs#nodejs-sails-1","content":"Install nodejs dependencies and save it in package.json using npm install --save elastic-apm-node@^3.20.0 npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries: &quot;elastic-apm-node&quot;: &quot;^3.20.0&quot; &quot;sf-apm-lib&quot;: &quot;^1.0.2&quot; Copy And run npm install to install dependencies Add initilization code at start of the file in globals.js present in config folder. Get Snappyflow trace config using const Snappyflow = require('sf-apm-lib'); var sfObj = new Snappyflow(); // Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml. // Add below part to manually configure the initialization let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; sfObj.init(profileKey, projectName, appName); // Manual override let sfTraceConfig = sfObj.getTraceConfig(); // Start Trace to log feature section // Add below line of code to enable Trace to log feature: sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_redact_body=true' // Option Configs for trace to log // Add below line to provide custom documentType (Default:&quot;user-input&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_documentType=&lt;document-type&gt;' // Add below line to provide destination index (Default:&quot;log&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_IndexType=&lt;index-type&gt;' // Applicable values(log, metric) // End trace to log section Copy Initialize apm object using var apm; try { apm = require('elastic-apm-node').start({ serviceName: '&lt;SERVICE_NAME&gt;', // Specify your service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'], metricsInterval: '0s', usePathAsTransactionName: true }) } catch (e) { console.log(e); } Copy Attach apm object to globals – This is required so we can use apm variable in other files as part of global sails object module.exports.globals = { _: require('@sailshq/lodash'), async: false, models: true, sails: true, apm : apm, logger: logger }; Copy Also add middleware in http.js file present in config folder. Which allows to instrument our code module.exports.http = { middleware: { order: [ 'elasticAPM' ], elasticAPM: (function () { return function (err, req, res, next) { apm.middleware.connect(); if (typeof err !== 'undefined') apm.captureError(err); return next(); }; })() } }; Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in Kubernetes deployment file. https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/ If deploying with helm provide above variables in values.yaml and use them in deployment file of charts. https://phoenixnap.com/kb/helm-environment-variables Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -&gt; Click on Tracing on lef side bar -&gt; Click on view transaction -&gt; Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/RefappNodeSail Note: 'captureBody':'all' config should be present in apm agent code instrumentation for Trace to Log feature.  "},{"title":"Docker​","type":1,"pageTitle":"NodeJS tracing","url":"docs/Tracing/nodejs#docker","content":""},{"title":"Node.JS Express​","type":1,"pageTitle":"NodeJS tracing","url":"docs/Tracing/nodejs#nodejs-express-2","content":"Install nodejs dependencies and save it in package.json using RUN npm install --save elastic-apm-node@^3.20.0 RUN npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries: &quot;elastic-apm-node&quot;: &quot;^3.20.0&quot; &quot;sf-apm-lib&quot;: &quot;^1.0.2&quot; Copy And run npm install to install dependencies Add initilization code at start of the file in app.js Get Snappyflow trace config using const Snappyflow = require('sf-apm-lib'); var sfObj = new Snappyflow(); // Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml. // Add below part to manually configure the initialization let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; sfObj.init(profileKey, projectName, appName); // Manual override let sfTraceConfig = sfObj.getTraceConfig(); // Start Trace to log feature section // Add below line of code to enable Trace to log feature: sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_redact_body=true' // Option Configs for trace to log // Add below line to provide custom documentType (Default:&quot;user-input&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_documentType=&lt;document-type&gt;' // Add below line to provide destination index (Default:&quot;log&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_IndexType=&lt;index-type&gt;' // Applicable values(log, metric) // End trace to log section Copy Initialize apm object using var apm; try { apm = require('elastic-apm-node').start({ serviceName: '&lt;SERVICE_NAME&gt;', // Specify your service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'], metricsInterval: '0s', usePathAsTransactionName: true }) } catch (e) { console.log(e); } Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in docker-compose.yml or docker stack deployment file or at command line when using docker run command for deployment. Eg: Docker-compose and stack: https://docs.docker.com/compose/environment-variables/ Docker run cli command: docker run -d -t -i -e SF_PROJECT_NAME='&lt;Project name&gt;' \\ -e SF_APP_NAME='&lt;SF_APP_NAME&gt;' \\ -e SF_PROFILE_KEY='&lt;snappyflow profile key&gt;' \\ --name &lt;container_name&gt; &lt;dockerhub_id/image_name&gt; Copy Once your server is up and running you can check trace in Snappyflow Server. // Project related info For viewing trace in Snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -&gt; Click on Tracing on lef side bar -&gt; Click on view transaction -&gt; Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-express Note: 'captureBody':'all' config should be present in apm agent code instrumentation for Trace to Log feature.  "},{"title":"Node.JS Sails​","type":1,"pageTitle":"NodeJS tracing","url":"docs/Tracing/nodejs#nodejs-sails-2","content":"Install nodejs dependencies and save it in package.json using RUN npm install --save elastic-apm-node@^3.20.0 RUN npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries: &quot;elastic-apm-node&quot;: &quot;^3.20.0&quot; &quot;sf-apm-lib&quot;: &quot;^1.0.2&quot; Copy And run ‘npm install’ to install dependencies Add initilization code at start of the file in globals.js present in config folder. Get Snappyflow trace config using const Snappyflow = require('sf-apm-lib'); var sfObj = new Snappyflow(); // Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml. // Add below part to manually configure the initialization let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; sfObj.init(profileKey, projectName, appName); // Manual override let sfTraceConfig = sfObj.getTraceConfig(); // Start Trace to log feature section // Add below line of code to enable Trace to log feature: sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_redact_body=true' // Option Configs for trace to log // Add below line to provide custom documentType (Default:&quot;user-input&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_documentType=&lt;document-type&gt;' // Add below line to provide destination index (Default:&quot;log&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_IndexType=&lt;index-type&gt;' // Applicable values(log, metric) // End trace to log section Copy Initialize apm object using var apm; try { apm = require('elastic-apm-node').start({ serviceName: '&lt;SERVICE_NAME&gt;', // Specify your service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'], metricsInterval: '0s', usePathAsTransactionName: true }) } catch (e) { console.log(e); } Copy Attach apm object to globals – This is required so we can use apm variable in other files as part of global sails object. module.exports.globals = { _: require('@sailshq/lodash'), async: false, models: true, sails: true, apm : apm, logger: logger }; Copy Also add middleware in http.js file present in config folder. Which allows to instrument our code. module.exports.http = { middleware: { order: [ 'elasticAPM' ], elasticAPM: (function () { return function (err, req, res, next) { apm.middleware.connect(); if (typeof err !== 'undefined') apm.captureError(err); return next(); }; })() } }; Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in docker-compose.yml or docker stack deployment file or at command line when using docker run command for deployment. Eg: Docker-compose and stack: https://docs.docker.com/compose/environment-variables/ Docker run cli command: docker run -d -t -i -e SF_PROJECT_NAME='&lt;SF_PROJECT_NAME&gt;' \\ -e SF_APP_NAME='&lt;SF_APP_NAME&gt;' \\ -e SF_PROFILE_KEY='&lt;snappyflow profile key&gt;' \\ --name &lt;container_name&gt; &lt;dockerhub_id/image_name&gt; Copy Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -&gt; Click on Tracing on lef side bar -&gt; Click on view transaction -&gt; Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/RefappNodeSail Note: 'captureBody':'all' config should be present in apm agent code instrumentation for Trace to Log feature.  "},{"title":"ECS​","type":1,"pageTitle":"NodeJS tracing","url":"docs/Tracing/nodejs#ecs","content":""},{"title":"Node.JS Express​","type":1,"pageTitle":"NodeJS tracing","url":"docs/Tracing/nodejs#nodejs-express-3","content":"Install nodejs dependencies and save it in package.json using npm install --save elastic-apm-node@^3.20.0 npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries &quot;elastic-apm-node&quot;: &quot;^3.20.0&quot; &quot;sf-apm-lib&quot;: &quot;^1.0.2&quot; Copy And run npm install to install dependencies Add initilization code at start of the file in app.js Get Snappyflow trace config using: const Snappyflow = require('sf-apm-lib'); var sfObj = new Snappyflow(); // Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml. // Add below part to manually configure the initialization let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; sfObj.init(profileKey, projectName, appName); // Manual override let sfTraceConfig = sfObj.getTraceConfig(); // Start Trace to log feature section // Add below line of code to enable Trace to log feature: sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_redact_body=true' // Option Configs for trace to log // Add below line to provide custom documentType (Default:&quot;user-input&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_documentType=&lt;document-type&gt;' // Add below line to provide destination index (Default:&quot;log&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_IndexType=&lt;index-type&gt;' // Applicable values(log, metric) // End trace to log section Copy Initialize apm object using var apm; try { apm = require('elastic-apm-node').start({ serviceName: '&lt;SERVICE_NAME&gt;', // Specify your service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'], metricsInterval: '0s', usePathAsTransactionName: true }) } catch (e) { console.log(e); } Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in add container section of task definitions. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in Snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to: View dashboard -&gt; Click on Tracing on left side bar -&gt; Click on view transaction -&gt; Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-express Note: 'captureBody':'all' config should be present in apm agent code instrumentation for Trace to Log feature.  "},{"title":"Node.JS Sails​","type":1,"pageTitle":"NodeJS tracing","url":"docs/Tracing/nodejs#nodejs-sails-3","content":"Install nodejs dependencies and save it in package.json using npm install --save elastic-apm-node@^3.20.0 npm install --save sf-apm-lib@^1.0.2 Copy or update package.json file with following entries &quot;elastic-apm-node&quot;: &quot;^3.20.0&quot; &quot;sf-apm-lib&quot;: &quot;^1.0.2&quot; Copy And run npm install to install dependencies Add initialization code at start of the file in globals.js present in config folder. Get Snappyflow trace config using const Snappyflow = require('sf-apm-lib'); var sfObj = new Snappyflow(); // Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml. // Add below part to manually configure the initialization let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; sfObj.init(profileKey, projectName, appName); // Manual override let sfTraceConfig = sfObj.getTraceConfig(); // Start Trace to log feature section // Add below line of code to enable Trace to log feature: sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_redact_body=true' // Option Configs for trace to log // Add below line to provide custom documentType (Default:&quot;user-input&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_documentType=&lt;document-type&gt;' // Add below line to provide destination index (Default:&quot;log&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_IndexType=&lt;index-type&gt;' // Applicable values(log, metric) // End trace to log section Copy Initialize apm object using var apm; try { apm = require('elastic-apm-node').start({ serviceName: '&lt;SERVICE_NAME&gt;', // Specify your service name for tracing serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'], metricsInterval: '0s', usePathAsTransactionName: true }) } catch (e) { console.log(e); } Copy Attach apm object to globals – This is required so we can use apm variable in other files as part of global sails object module.exports.globals = { _: require('@sailshq/lodash'), async: false, models: true, sails: true, apm : apm, logger: logger }; Copy Also add middleware in http.js file present in config folder. Which allows to instrument our code module.exports.http = { middleware: { order: [ 'elasticAPM' ], elasticAPM: (function () { return function (err, req, res, next) { apm.middleware.connect(); if (typeof err !== 'undefined') apm.captureError(err); return next(); }; })() } }; Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in add container section of task definitions. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -&gt; Click on Tracing on lef side bar -&gt; Click on view transaction -&gt; Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/RefappNodeSail Note: 'captureBody':'all' config should be present in apm agent code instrumentation for Trace to Log feature.  "},{"title":"AWS Lambda​","type":1,"pageTitle":"NodeJS tracing","url":"docs/Tracing/nodejs#aws-lambda","content":"Install dependency libraries in the node_modules directory using the npm install command npm install sf-apm-lib@^1.0.2 npm install elastic-apm-node@^3.20.0 Copy Ref: https://docs.aws.amazon.com/lambda/latest/dg/nodejs-package.html Instrument lambda function to enable tracing Add code outside lambda handler method to get tracing config and create trace client // SnappyFlow Tracing config const Snappyflow = require('sf-apm-lib'); let projectName = process.env.SF_PROJECT_NAME; let appName = process.env.SF_APP_NAME; let profileKey = process.env.SF_PROFILE_KEY; var sfObj = new Snappyflow(); sfObj.init(profileKey, projectName, appName); var apm; try { var sfTraceConfig = sfObj.getTraceConfig(); apm = require('elastic-apm-node').start({ serviceName: '&lt;SERVICE_NAME_CHANGEME&gt;', serverUrl: sfTraceConfig['SFTRACE_SERVER_URL'], globalLabels: sfTraceConfig['SFTRACE_GLOBAL_LABELS'], verifyServerCert: sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'] === undefined ? false : sfTraceConfig['SFTRACE_VERIFY_SERVER_CERT'], active: sfTraceConfig['SFTRACE_SERVER_URL'] === undefined ? false : true, stackTraceLimit: sfTraceConfig['SFTRACE_STACK_TRACE_LIMIT'], captureSpanStackTraces: sfTraceConfig['SFTRACE_CAPTURE_SPAN_STACK_TRACES'], captureBody: 'all' , metricsInterval: '0s', usePathAsTransactionName: true }) } catch (e) { console.log(e) } Copy Add custom instrumentation inside lambda handler method Ref: https://www.elastic.co/guide/en/apm/agent/nodejs/current/custom-transactions.html https://www.elastic.co/guide/en/apm/agent/nodejs/current/custom-spans.html // Create custom transaction var trans = apm.startTransaction('lambda handler', 'lambda'); //Create custom span is needed var span = apm.startSpan('parse json'); // your CODE here // End of span if (span) span.end() //Some more code part of the transaction or add more spans here. Don’t RETURN/EXIT //end custom transaction trans.result = 'success'; trans.end(); // RETURN code Copy Deploy the lambda app. Follow README to test sample app Reference app: https://github.com/upendrasahu/aws-lambda-nodejs-tracing-sample Configure Lambda function before trigger/invoke. Add the environment variable SF_PROFILE_KEY and set the value to your profile key copied from SnappyFlow. Add environment variables SF_APP_NAME and SF_PROJECT_NAME with appropriate values. Create this Project and Application in SnappyFlow if not already present. At this point you can trigger lambda function and get tracing data in SnappyFlow. "},{"title":"Log Correlation​","type":1,"pageTitle":"NodeJS tracing","url":"docs/Tracing/nodejs#log-correlation","content":"If you are using existing logger in your application then embed transaction id, trace id and span id using elastic apm node client object which was created at the start of the application. For more info refer apm initialization code. Eg.  var traceId = 'None'; var transactionId = 'None'; var spanId = 'None'; if (typeof(apm) !== 'undefined') { var apmTraceObj = apm.currentTraceIds; // Apm object having current trace ids transactionId = apmTraceObj['transaction.id'] || 'None'; traceId = apmTraceObj['trace.id'] || 'None'; spanId = apmTraceObj['span.id'] || 'None'; } var msg = `[${moment().format('DD/MMM/YYYY hh:mm:ss')}] [${level}] [${msg}] | elasticapm transaction.id=${transactionId} trace.id=${traceId} span.id=${spanId}\\n` Copy For using Log correlation in application log file refer:https://www.elastic.co/guide/en/apm/agent/nodejs/current/log-correlation.html "},{"title":"Adding custom Snappyflow logger for log correlation​","type":1,"pageTitle":"NodeJS tracing","url":"docs/Tracing/nodejs#adding-custom-snappyflow-logger-for-log-correlation","content":" //Copy logger file from snappyflow reference app to location where you want to put. Initialize logger in your app using following code: const logger = require(&quot;./logger&quot;).Logger; logger.attachAPM(apm); logger.setLogFilePath('/var/log/trace/ntrace.log'); //Put log file in /var/log/trace folder logger.init(); //Write log logger.debug('Hello world get api called') logger.info('Hello world get api called') logger.error('Some error ocurred') Copy For code reference refer: https://github.com/snappyflow/tracing-reference-apps/blob/master/refapp-express/logger.js  "},{"title":"Send log correlation data to snappyflow server​","type":1,"pageTitle":"NodeJS tracing","url":"docs/Tracing/nodejs#send-log-correlation-data-to-snappyflow-server","content":"Below are the modes for sending log correlated data to snappyflow server "},{"title":" For Appliance ​","type":1,"pageTitle":"NodeJS tracing","url":"docs/Tracing/nodejs#-for-appliance-","content":"Install sfagent and create config file. Refer: https://docs.snappyflow.io/docs/Integrations/os/linux/sfagent_linux Add elasticApmLog plugin to sfagent config.yaml and restart sfagent service. Eg. Config.yaml key: &lt;SF_PROFILE_KEY&gt; tags: Name: &lt;any-name&gt; appName: &lt;SF_APP_NAME&gt; projectName: &lt;SF_PROJECT_NAME&gt; logging: plugins: - name: elasticApmTraceLog enabled: true config: log_level: - error - warning - info log_path: /var/log/trace/ntrace.log # Your app log file path Copy "},{"title":" For Kubernetes ​","type":1,"pageTitle":"NodeJS tracing","url":"docs/Tracing/nodejs#-for-kubernetes-","content":"Specify following values in metadata labels section of deployment file. snappyflow/appname: &lt;SF_APP_NAME&gt; snappyflow/projectname: &lt;SF_PROJECT_NAME&gt; snappyflow/component: gen-elastic-apm-log # This is must for tracing log correlation Copy "},{"title":"Sample deployment file​","type":1,"pageTitle":"NodeJS tracing","url":"docs/Tracing/nodejs#sample-deployment-file","content":"apiVersion: apps/v1 kind: Deployment metadata: labels: io.kompose.service: express-node snappyflow/appname: '&lt;sf_app_name&gt;' snappyflow/projectname: '&lt;sf_project_name&gt;' snappyflow/component: gen-elastic-apm-log name: express-node spec: replicas: 1 selector: matchLabels: io.kompose.service: express-node strategy: {} template: metadata: labels: io.kompose.service: express-node snappyflow/appname: '&lt;sf_app_name&gt;' snappyflow/projectname: '&lt;sf_project_name&gt;' snappyflow/component: gen-elastic-apm-log spec: containers: - env: - name: SF_APP_NAME value: '&lt;sf_app_name&gt;' - name: SF_PROFILE_KEY value: '&lt;sf_profile_key&gt;' - name: SF_PROJECT_NAME value: '&lt;sf_project_name&gt;' image: refapp-node:latest imagePullPolicy: Always name: express-node ports: - containerPort: 3000 resources: requests: cpu: 10m memory: 10Mi limits: cpu: 50m memory: 50Mi restartPolicy: Always Copy Note: For kubernetes mode we need sfagent pods to be running inside kubernetes cluster where your application pods are deployed. For viewing trace and logs in Snappyflow server make sure project and app name is created or discovered. Once project and app name is created. Go to: View App dashboard -&gt; Click on Tracing on left side bar -&gt; Click on view transaction -&gt; Go to real time tab Then click on any trace and go to logs tab to see the correlated logs to trace. // Note: To get trace in snappyflow server we need log entries to adhere following log format: &lt;date in following format&gt; [10/Aug/2021 10:51:16] [&lt;log_level&gt;] [&lt;message&gt;] | elasticapm transaction.id=&lt;transaction_id&gt; trace.id=&lt;trace_id&gt; span.id=&lt;span_id&gt; Copy "},{"title":"Python tracing","type":0,"sectionRef":"#","url":"docs/Tracing/python","content":"","keywords":""},{"title":"Instances​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#instances","content":""},{"title":"Django​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#django","content":"Add sf-elastic-apm==6.7.2 sf-apm-lib==0.1.1 Copy entries in requirements.txt file and install these in your project environment or Install through CLI using pip install sf-elastic-apm==6.7.2 pip install sf-apm-lib==0.1.1 Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variable. Add following entries in settings.py Add import statement from sf_apm_lib.snappyflow import Snappyflow Copy Add following entry in INSTALLED_APPS 'elasticapm.contrib.django' Copy Add following entry in MIDDLEWARE 'elasticapm.contrib.django.middleware.TracingMiddleware' Copy Add this entry for instrumenting Django app try: sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = os.getenv('SF_PROJECT_NAME') SF_APP_NAME = os.getenv('SF_APP_NAME') SF_PROFILE_KEY = os.getenv('SF_PROFILE_KEY') sf.init(SF_PROFILE_KEY, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() # Start Trace to log feature section # Add below line of code to enable Trace to log feature: sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_redact_body=true' # Option Configs for trace to log # Add below line to provide custom documentType (Default:&quot;user-input&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_documentType=&lt;document-type&gt;' # Add below line to provide destination index (Default:&quot;log&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_IndexType=&lt;index-type&gt;' # Applicable values(log, metric) # End trace to log section ELASTIC_APM={ 'SERVICE_NAME': &quot;&lt;Service name&gt;&quot; , # Specify your service name for tracing 'SERVER_URL': SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), 'GLOBAL_LABELS': SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), 'VERIFY_SERVER_CERT': SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT'), 'SPAN_FRAMES_MIN_DURATION': SFTRACE_CONFIG.get('SFTRACE_SPAN_FRAMES_MIN_DURATION'), 'STACK_TRACE_LIMIT': SFTRACE_CONFIG.get('SFTRACE_STACK_TRACE_LIMIT'), 'CAPTURE_SPAN_STACK_TRACES': SFTRACE_CONFIG.get('SFTRACE_CAPTURE_SPAN_STACK_TRACES'), 'DJANGO_TRANSACTION_NAME_FROM_ROUTE': True, 'CENTRAL_CONFIG': False, 'DEBUG': True, 'METRICS_INTERVAL': '0s' } except Exception as error: print(&quot;Error while fetching snappyflow tracing configurations&quot;, error) Copy Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -&gt; Click on Tracing on lef side bar -&gt; Click on view transaction -&gt; Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-django Note: 'CAPTURE_BODY':'all' config should be present in apm agent code instrumentation for Trace to Log feature.  "},{"title":"Flask​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#flask","content":"Add sf-elastic-apm[flask]==6.7.2 sf-apm-lib==0.1.1 Copy entries in requirements.txt file and install these in your project environment or Install through CLI using pip install sf-elastic-apm[flask]==6.7.2 pip install sf-apm-lib==0.1.1 Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variable. Add following entries in app.py Add imports statement from elasticapm.contrib.flask import ElasticAPM from sf_apm_lib.snappyflow import Snappyflow Copy Get trace config sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = os.getenv('SF_PROJECT_NAME') SF_APP_NAME = os.getenv('SF_APP_NAME') SF_PROFILE_KEY = os.getenv('SF_PROFILE_KEY') sf.init(SF_PROFILE_KEY, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() # Start Trace to log feature section # Add below line of code to enable Trace to log feature: sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_redact_body=true' # Option Configs for trace to log # Add below line to provide custom documentType (Default:&quot;user-input&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_documentType=&lt;document-type&gt;' # Add below line to provide destination index (Default:&quot;log&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_IndexType=&lt;index-type&gt;' # Applicable values(log, metric) # End trace to log section Copy Initialize elastic apm and instrument it to flask app app.config['ELASTIC_APM'] = { 'SERVICE_NAME': '&lt;SERVICE_NAME&gt;', # Specify your service name for tracing 'SERVER_URL': SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), 'GLOBAL_LABELS': SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), 'VERIFY_SERVER_CERT': SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT'), 'SPAN_FRAMES_MIN_DURATION': SFTRACE_CONFIG.get('SFTRACE_SPAN_FRAMES_MIN_DURATION'), 'STACK_TRACE_LIMIT': SFTRACE_CONFIG.get('SFTRACE_STACK_TRACE_LIMIT'), 'CAPTURE_SPAN_STACK_TRACES': SFTRACE_CONFIG.get('SFTRACE_CAPTURE_SPAN_STACK_TRACES'), 'DEBUG': True, 'METRICS_INTERVAL': '0s' } apm = ElasticAPM(app) Copy Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -&gt; Click on Tracing on lef side bar -&gt; Click on view transaction -&gt; Go to real time tab Note: 'CAPTURE_BODY':'all' config should be present in apm agent code instrumentation for Trace to Log feature.  "},{"title":"Script​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#script","content":"Install following requirements pip install sf-elastic-apm==6.7.2 pip install sf-apm-lib==0.1.1 Copy Add following code at start of script file to setup elastic apm client import elasticapm from sf_apm_lib.snappyflow import Snappyflow sf = Snappyflow() # Initialize Snappyflow. By default intialization will pick profileKey, projectName and appName from sfagent config.yaml. # Add below part to manually configure the initialization SF_PROJECT_NAME = '&lt;Snappyflow Project Name&gt;' SF_APP_NAME = '&lt;Snappyflow App Name&gt;' SF_PROFILE_KEY = '&lt;Snappyflow Profile Key&gt;' sf.init(SF_PROFILE_KEY, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration trace_config = sf.get_trace_config() # Returns trace config client = elasticapm.Client( service_name=&quot;&lt;Service name&gt; &quot;,# Specify service name for tracing server_url=trace_config['SFTRACE_SERVER_URL'], verify_cert=trace_config['SFTRACE_VERIFY_SERVER_CERT'], global_labels=trace_config['SFTRACE_GLOBAL_LABELS'] ) elasticapm.instrument() # Only call this once, as early as possible. Copy Once instrumentation is completed we can create custom transaction and span Example def main(): sess = requests.Session() for url in [ 'https://www.elastic.co', 'https://benchmarks.elastic.co' ]: resp = sess.get(url) time.sleep(1) client.begin_transaction(transaction_type=&quot;script&quot;) main() # Record an exception try: 1/0 except ZeroDivisionError: ident = client.capture_exception() print (&quot;Exception caught; reference is %s&quot; % ident) client.end_transaction(name=__name__, result=&quot;success&quot;) Copy Refer link to know more: https://www.elastic.co/guide/en/apm/agent/python/master/instrumenting-custom-code.html Now run you script and test your trace in snappyflow server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -&gt; Click on Tracing on left side bar -&gt; Click on view transaction -&gt; Go to real time tab Refer complete script: https://github.com/snappyflow/tracing-reference-apps/blob/master/refapp-django/python_script_trace.py "},{"title":"Celery​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#celery","content":"Install following requirements (Following example is based on redis broker) pip install sf-elastic-apm==6.7.2 pip install redis pip install sf-apm-lib==0.1.1 Copy Add following code at start of the file where celery app is initialized to setup elastic apm client from sf_apm_lib.snappyflow import Snappyflow from elasticapm import Client, instrument from elasticapm.contrib.celery import register_exception_tracking, register_instrumentation instrument() try: sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = '&lt;SF_PROJECT_NAME&gt;' # Replace with appropriate Snappyflow project name SF_APP_NAME = '&lt;SF_APP_NAME&gt;' # Replace with appropriate Snappyflow app name SF_PROFILE_KEY = '&lt;SF_PROFILE_KEY&gt;' # Replace Snappyflow Profile key sf.init(SF_PROFILE_KEY, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() apm_client = Client( service_name= '&lt;Service_Name&gt;', # Specify service name for tracing server_url= SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), global_labels= SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), verify_server_cert= SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT') ) register_exception_tracking(apm_client) register_instrumentation(apm_client) except Exception as error: print(&quot;Error while fetching snappyflow tracing configurations&quot;, error) Copy Once instrumentation is done and celery worker is running we can see trace for each celery task in Snappyflow server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -&gt; Click on Tracing on left side bar -&gt; Click on view transaction -&gt; Go to real time tab Refer complete code: https://github.com/snappyflow/tracing-reference-apps/blob/master/ref-celery/tasks.py "},{"title":"Kubernetes​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#kubernetes","content":""},{"title":"Django​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#django-1","content":"Add sf-elastic-apm==6.7.2 sf-apm-lib==0.1.1 Copy entries in requirements.txt file and install these in your project environment or Install through CLI using pip install sf-elastic-apm==6.7.2 pip install sf-apm-lib==0.1.1 Copy Add following entries in settings.py Add import statement from sf_apm_lib.snappyflow import Snappyflow Copy Add following entry in INSTALLED_APPS 'elasticapm.contrib.django' Copy Add following entry in MIDDLEWARE 'elasticapm.contrib.django.middleware.TracingMiddleware' Copy Add this entry for instrumenting Django app try: sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = os.getenv('SF_PROJECT_NAME') SF_APP_NAME = os.getenv('SF_APP_NAME') SF_PROFILE_KEY = os.getenv('SF_PROFILE_KEY') sf.init(SF_PROFILE_KEY, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() # Start Trace to log feature section # Add below line of code to enable Trace to log feature: sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_redact_body=true' # Option Configs for trace to log # Add below line to provide custom documentType (Default:&quot;user-input&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_documentType=&lt;document-type&gt;' # Add below line to provide destination index (Default:&quot;log&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_IndexType=&lt;index-type&gt;' # Applicable values(log, metric) # End trace to log section ELASTIC_APM={ 'SERVICE_NAME': &quot;&lt;Service name&gt;&quot; , # Specify your service name for tracing 'SERVER_URL': SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), 'GLOBAL_LABELS': SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), 'VERIFY_SERVER_CERT': SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT'), 'SPAN_FRAMES_MIN_DURATION': SFTRACE_CONFIG.get('SFTRACE_SPAN_FRAMES_MIN_DURATION'), 'STACK_TRACE_LIMIT': SFTRACE_CONFIG.get('SFTRACE_STACK_TRACE_LIMIT'), 'CAPTURE_SPAN_STACK_TRACES': SFTRACE_CONFIG.get('SFTRACE_CAPTURE_SPAN_STACK_TRACES'), 'DJANGO_TRANSACTION_NAME_FROM_ROUTE': True, 'CENTRAL_CONFIG': False, 'DEBUG': True, 'METRICS_INTERVAL': '0s' } except Exception as error: print(&quot;Error while fetching snappyflow tracing configurations&quot;, error) Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in Kubernetes deployment file. https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/ If deploying with helm provide above variables in values.yaml and use them in deployment file of charts. https://phoenixnap.com/kb/helm-environment-variables Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -&gt; Click on Tracing on lef side bar -&gt; Click on view transaction -&gt; Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-django Note: 'CAPTURE_BODY':'all' config should be present in apm agent code instrumentation for Trace to Log feature.  "},{"title":"Flask​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#flask-1","content":"Add sf-elastic-apm[flask]==6.7.2 sf-apm-lib==0.1.1 Copy entries in requirements.txt file and install these in your project environment or Install through CLI using RUN pip install sf-elastic-apm[flask]==6.7.2 RUN pip install sf-apm-lib==0.1.1 Copy Add following entries in app.py Add imports statement from elasticapm.contrib.flask import ElasticAPM from sf_apm_lib.snappyflow import Snappyflow Copy Get trace config sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = os.getenv('SF_PROJECT_NAME') SF_APP_NAME = os.getenv('SF_APP_NAME') SF_PROFILE_KEY = os.getenv('SF_PROFILE_KEY') sf.init(SF_PROFILE_KEY, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() # Start Trace to log feature section # Add below line of code to enable Trace to log feature: sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_redact_body=true' # Option Configs for trace to log # Add below line to provide custom documentType (Default:&quot;user-input&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_documentType=&lt;document-type&gt;' # Add below line to provide destination index (Default:&quot;log&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_IndexType=&lt;index-type&gt;' # Applicable values(log, metric) # End trace to log section Copy Initialize elastic apm and instrument it to flask app app.config['ELASTIC_APM'] = { 'SERVICE_NAME': '&lt;SERVICE_NAME&gt;', # Specify your service name for tracing 'SERVER_URL': SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), 'GLOBAL_LABELS': SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), 'VERIFY_SERVER_CERT': SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT'), 'SPAN_FRAMES_MIN_DURATION': SFTRACE_CONFIG.get('SFTRACE_SPAN_FRAMES_MIN_DURATION'), 'STACK_TRACE_LIMIT': SFTRACE_CONFIG.get('SFTRACE_STACK_TRACE_LIMIT'), 'CAPTURE_SPAN_STACK_TRACES': SFTRACE_CONFIG.get('SFTRACE_CAPTURE_SPAN_STACK_TRACES'), 'DEBUG': True, 'METRICS_INTERVAL': '0s' } apm = ElasticAPM(app) Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in Kubernetes deployment file. https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/ If deploying with helm provide above variables in values.yaml and use them in deployment file of charts. https://phoenixnap.com/kb/helm-environment-variables Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -&gt; Click on Tracing on lef side bar -&gt; Click on view transaction -&gt; Go to real time tab Note: 'CAPTURE_BODY':'all' config should be present in apm agent code instrumentation for Trace to Log feature.  "},{"title":"Celery​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#celery-1","content":"Install following requirements (Following example is based on redis broker) pip install sf-elastic-apm==6.7.2 pip install redis pip install sf-apm-lib==0.1.1 Copy Add following code at start of the file where celery app is initialized to setup elastic apm client from sf_apm_lib.snappyflow import Snappyflow from elasticapm import Client, instrument from elasticapm.contrib.celery import register_exception_tracking, register_instrumentation instrument() try: sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = '&lt;SF_PROJECT_NAME&gt;' # Replace with appropriate Snappyflow project name SF_APP_NAME = '&lt;SF_APP_NAME&gt;' # Replace with appropriate Snappyflow app name SF_PROFILE_KEY = '&lt;SF_PROFILE_KEY&gt;' # Replace Snappyflow Profile key sf.init(SF_PROFILE_KEY, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() apm_client = Client(service_name= '&lt;Service_Name&gt;', # Specify service name for tracing server_url= SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), global_labels= SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), verify_server_cert= SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT') ) register_exception_tracking(apm_client) register_instrumentation(apm_client) except Exception as error: print(&quot;Error while fetching snappyflow tracing configurations&quot;, error) Copy Once instrumentation is done and celery worker is running we can see trace for each celery task in Snappyflow server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -&gt; Click on Tracing on left side bar -&gt; Click on view transaction -&gt; Go to real time tab Refer complete code: https://github.com/snappyflow/tracing-reference-apps/blob/master/ref-celery/tasks.py "},{"title":"Docker​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#docker","content":""},{"title":"Django​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#django-2","content":"Add sf-elastic-apm==6.7.2 sf-apm-lib==0.1.1 Copy entries in requirements.txt file and install these in your project environment or Install through CLI using RUN pip install sf-elastic-apm==6.7.2 RUN pip install sf-apm-lib==0.1.1 Copy Add following entries in settings.py Add import statement from sf_apm_lib.snappyflow import Snappyflow Copy Add following entry in INSTALLED_APPS 'elasticapm.contrib.django' Copy Add following entry in MIDDLEWARE 'elasticapm.contrib.django.middleware.TracingMiddleware' Copy Add this entry for instrumenting Django app try: sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = os.getenv('SF_PROJECT_NAME') SF_APP_NAME = os.getenv('SF_APP_NAME') SF_PROFILE_KEY = os.getenv('SF_PROFILE_KEY') sf.init(SF_PROFILE_KEY, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() # Start Trace to log feature section # Add below line of code to enable Trace to log feature: sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_redact_body=true' # Option Configs for trace to log # Add below line to provide custom documentType (Default:&quot;user-input&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_documentType=&lt;document-type&gt;' # Add below line to provide destination index (Default:&quot;log&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_IndexType=&lt;index-type&gt;' # Applicable values(log, metric) # End trace to log section ELASTIC_APM={ 'SERVICE_NAME': &quot;&lt;Service name&gt;&quot; , # Specify your service name for tracing 'SERVER_URL': SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), 'GLOBAL_LABELS': SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), 'VERIFY_SERVER_CERT': SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT'), 'SPAN_FRAMES_MIN_DURATION': SFTRACE_CONFIG.get('SFTRACE_SPAN_FRAMES_MIN_DURATION'), 'STACK_TRACE_LIMIT': SFTRACE_CONFIG.get('SFTRACE_STACK_TRACE_LIMIT'), 'CAPTURE_SPAN_STACK_TRACES': SFTRACE_CONFIG.get('SFTRACE_CAPTURE_SPAN_STACK_TRACES'), 'DJANGO_TRANSACTION_NAME_FROM_ROUTE': True, 'CENTRAL_CONFIG': False, 'DEBUG': True, 'METRICS_INTERVAL': '0s' } except Exception as error: print(&quot;Error while fetching snappyflow tracing configurations&quot;, error) Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in docker-compose.yml or docker stack deployment file or at command line when using docker run command for deployment. Eg: Docker-compose and stack: https://docs.docker.com/compose/environment-variables/ Docker RUN: docker run -d -t -i -e SF_PROJECT_NAME='' \\ -e SF_APP_NAME='' \\ -e SF_PROFILE_KEY='' \\ -p 80:80 \\ --link redis:redis \\ --name &lt;container_name&gt; &lt;dockerhub_id/image_name&gt; Copy Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -&gt; Click on Tracing on lef side bar -&gt; Click on view transaction -&gt; Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-django Note: 'CAPTURE_BODY':'all' config should be present in apm agent code instrumentation for Trace to Log feature.  "},{"title":"Flask​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#flask-2","content":"Add sf-elastic-apm[flask]==6.7.2 sf-apm-lib==0.1.1 Copy entries in requirements.txt file and install these in your project environment or Install through CLI using RUN pip install sf-elastic-apm[flask]==6.7.2 RUN pip install sf-apm-lib==0.1.1 Copy Add following entries in app.py Add imports statement from elasticapm.contrib.flask import ElasticAPM from sf_apm_lib.snappyflow import Snappyflow Copy Get trace config sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = os.getenv('SF_PROJECT_NAME') SF_APP_NAME = os.getenv('SF_APP_NAME') SF_PROFILE_KEY = os.getenv('SF_PROFILE_KEY') sf.init(SF_PROFILE_KEY, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() # Start Trace to log feature section # Add below line of code to enable Trace to log feature: sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_redact_body=true' # Option Configs for trace to log # Add below line to provide custom documentType (Default:&quot;user-input&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_documentType=&lt;document-type&gt;' # Add below line to provide destination index (Default:&quot;log&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_IndexType=&lt;index-type&gt;' # Applicable values(log, metric) # End trace to log section Copy Initialize elastic apm and instrument it to flask app app.config['ELASTIC_APM'] = { 'SERVICE_NAME': '&lt;SERVICE_NAME&gt;', # Specify your service name for tracing 'SERVER_URL': SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), 'GLOBAL_LABELS': SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), 'VERIFY_SERVER_CERT': SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT'), 'SPAN_FRAMES_MIN_DURATION': SFTRACE_CONFIG.get('SFTRACE_SPAN_FRAMES_MIN_DURATION'), 'STACK_TRACE_LIMIT': SFTRACE_CONFIG.get('SFTRACE_STACK_TRACE_LIMIT'), 'CAPTURE_SPAN_STACK_TRACES': SFTRACE_CONFIG.get('SFTRACE_CAPTURE_SPAN_STACK_TRACES'), 'DEBUG': True, 'METRICS_INTERVAL': '0s' } apm = ElasticAPM(app) Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in docker-compose.yml or docker stack deployment file or at command line when using docker run command for deployment. Eg: Docker-compose and stack: https://docs.docker.com/compose/environment-variables/ Docker run cli command: docker run -d -t -i -e SF_PROJECT_NAME='&lt;SF_PROJECT_NAME&gt;' \\ -e SF_APP_NAME='&lt;SF_APP_NAME&gt;' \\ -e SF_PROFILE_KEY='&lt;snappyflow profile key&gt;' \\ --name &lt;container_name&gt; &lt;dockerhub_id/image_name&gt; Copy Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -&gt; Click on Tracing on lef side bar -&gt; Click on view transaction -&gt; Go to real time tab Note: 'CAPTURE_BODY':'all' config should be present in apm agent code instrumentation for Trace to Log feature.  "},{"title":"Celery​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#celery-2","content":"Install following requirements (Following example is based on redis broker) pip install sf-elastic-apm==6.7.2 pip install redis pip install sf-apm-lib==0.1.1 Copy Add following code at start of the file where celery app is initialized to setup elastic apm client from sf_apm_lib.snappyflow import Snappyflow from elasticapm import Client, instrument from elasticapm.contrib.celery import register_exception_tracking, register_instrumentation instrument() try: sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = '&lt;SF_PROJECT_NAME&gt;' # Replace with appropriate Snappyflow project name SF_APP_NAME = '&lt;SF_APP_NAME&gt;' # Replace with appropriate Snappyflow app name SF_PROFILE_KEY = '&lt;SF_PROFILE_KEY&gt;' # Replace Snappyflow Profile key sf.init(SF_PROFILE_KEY, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() apm_client = Client( service_name= '&lt;Service_Name&gt;', # Specify service name for tracing server_url= SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), global_labels= SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), verify_server_cert= SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT') ) register_exception_tracking(apm_client) register_instrumentation(apm_client) except Exception as error: print(&quot;Error while fetching snappyflow tracing configurations&quot;, error) Copy Once instrumentation is done and celery worker is running we can see trace for each celery task in Snappyflow server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -&gt; Click on Tracing on left side bar -&gt; Click on view transaction -&gt; Go to real time tab Refer complete code: https://github.com/snappyflow/tracing-reference-apps/blob/master/ref-celery/tasks.py "},{"title":"ECS​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#ecs","content":""},{"title":"Django​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#django-3","content":"Add sf-elastic-apm==6.7.2 sf-apm-lib==0.1.1 Copy entries in requirements.txt file and install these in your project environment or Install through CLI using pip install sf-elastic-apm==6.7.2 pip install sf-apm-lib==0.1.1 Copy Add following entries in settings.py Add import statement from sf_apm_lib.snappyflow import Snappyflow Copy Add following entry in INSTALLED_APPS 'elasticapm.contrib.django' Copy Add following entry in MIDDLEWARE 'elasticapm.contrib.django.middleware.TracingMiddleware' Copy Add this entry for instrumenting Django app try: sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = os.getenv('SF_PROJECT_NAME') SF_APP_NAME = os.getenv('SF_APP_NAME') SF_PROFILE_KEY = os.getenv('SF_PROFILE_KEY') sf.init(SF_PROFILE_KEY, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() # Start Trace to log feature section # Add below line of code to enable Trace to log feature: sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_redact_body=true' # Option Configs for trace to log # Add below line to provide custom documentType (Default:&quot;user-input&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_documentType=&lt;document-type&gt;' # Add below line to provide destination index (Default:&quot;log&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_IndexType=&lt;index-type&gt;' # Applicable values(log, metric) # End trace to log section ELASTIC_APM={ 'SERVICE_NAME': &quot;&lt;Service name&gt;&quot; , # Specify your service name for tracing 'SERVER_URL': SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), 'GLOBAL_LABELS': SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), 'VERIFY_SERVER_CERT': SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT'), 'SPAN_FRAMES_MIN_DURATION': SFTRACE_CONFIG.get('SFTRACE_SPAN_FRAMES_MIN_DURATION'), 'STACK_TRACE_LIMIT': SFTRACE_CONFIG.get('SFTRACE_STACK_TRACE_LIMIT'), 'CAPTURE_SPAN_STACK_TRACES': SFTRACE_CONFIG.get('SFTRACE_CAPTURE_SPAN_STACK_TRACES'), 'DJANGO_TRANSACTION_NAME_FROM_ROUTE': True, 'CENTRAL_CONFIG': False, 'DEBUG': True, 'METRICS_INTERVAL': '0s' } except Exception as error: print(&quot;Error while fetching snappyflow tracing configurations&quot;, error) Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in add container section of task definitions. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created go to View dashboard -&gt; Click on Tracing on lef side bar -&gt; Click on view transaction -&gt; Go to real time tab For complete code refer sample app refer at: https://github.com/snappyflow/tracing-reference-apps/tree/master/refapp-django Note: 'CAPTURE_BODY':'all' config should be present in apm agent code instrumentation for Trace to Log feature.  "},{"title":"Flask​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#flask-3","content":"Add sf-elastic-apm[flask]==6.7.2 sf-apm-lib==0.1.1 Copy entries in requirements.txt file and install these in your project environment or Install through CLI using RUN pip install sf-elastic-apm[flask]==6.7.2 RUN pip install sf-apm-lib==0.1.1 Copy Add following entries in app.py Add imports statement from elasticapm.contrib.flask import ElasticAPM from sf_apm_lib.snappyflow import Snappyflow Copy Get trace config sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = os.getenv('SF_PROJECT_NAME') SF_APP_NAME = os.getenv('SF_APP_NAME') SF_PROFILE_KEY = os.getenv('SF_PROFILE_KEY') sf.init(SF_PROFILE_KEY, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() # Start Trace to log feature section # Add below line of code to enable Trace to log feature: sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_redact_body=true' # Option Configs for trace to log # Add below line to provide custom documentType (Default:&quot;user-input&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_documentType=&lt;document-type&gt;' # Add below line to provide destination index (Default:&quot;log&quot;): sfTraceConfig['SFTRACE_GLOBAL_LABELS'] += ',_tag_IndexType=&lt;index-type&gt;' # Applicable values(log, metric) # End trace to log section Copy  ``` Copy Initialize elastic apm and instrument it to flask app app.config['ELASTIC_APM'] = { 'SERVICE_NAME': '&lt;SERVICE_NAME&gt;', # Specify your service name for tracing 'SERVER_URL': SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), 'GLOBAL_LABELS': SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), 'VERIFY_SERVER_CERT': SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT'), 'SPAN_FRAMES_MIN_DURATION': SFTRACE_CONFIG.get('SFTRACE_SPAN_FRAMES_MIN_DURATION'), 'STACK_TRACE_LIMIT': SFTRACE_CONFIG.get('SFTRACE_STACK_TRACE_LIMIT'), 'CAPTURE_SPAN_STACK_TRACES': SFTRACE_CONFIG.get('SFTRACE_CAPTURE_SPAN_STACK_TRACES'), 'DEBUG': True, 'METRICS_INTERVAL': '0s' } apm = ElasticAPM(app) Copy Provide SF_PROJECT_NAME, SF_APP_NAME, SF_PROFILE_KEY as an environment variables in add container section of task definitions. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html Once your server is up and running you can check trace in Snappyflow Server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -&gt; Click on Tracing on lef side bar -&gt; Click on view transaction -&gt; Go to real time tab Note: 'CAPTURE_BODY':'all' config should be present in apm agent code instrumentation for Trace to Log feature.  "},{"title":"Celery​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#celery-3","content":"Install following requirements (Following example is based on redis broker) pip install sf-elastic-apm==6.7.2 pip install redis pip install sf-apm-lib==0.1.1 Copy Add following code at start of the file where celery app is initialized to setup elastic apm client from sf_apm_lib.snappyflow import Snappyflow from elasticapm import Client, instrument from elasticapm.contrib.celery import register_exception_tracking, register_instrumentation instrument() try: sf = Snappyflow() # Initialize Snappyflow. By default intialization will take profileKey, projectName and appName from sfagent config.yaml # Add below part to manually configure the initialization SF_PROJECT_NAME = '&lt;SF_PROJECT_NAME&gt;' # Replace with appropriate Snappyflow project name SF_APP_NAME = '&lt;SF_APP_NAME&gt;' # Replace with appropriate Snappyflow app name SF_PROFILE_KEY = '&lt;SF_PROFILE_KEY&gt;' # Replace Snappyflow Profile key sf.init(SF_PROFILE_KEY, SF_PROJECT_NAME, SF_APP_NAME) # End of manual configuration SFTRACE_CONFIG = sf.get_trace_config() apm_client = Client( service_name= '&lt;Service_Name&gt;', # Specify service name for tracing server_url= SFTRACE_CONFIG.get('SFTRACE_SERVER_URL'), global_labels= SFTRACE_CONFIG.get('SFTRACE_GLOBAL_LABELS'), verify_server_cert= SFTRACE_CONFIG.get('SFTRACE_VERIFY_SERVER_CERT') ) register_exception_tracking(apm_client) register_instrumentation(apm_client) except Exception as error: print(&quot;Error while fetching snappyflow tracing configurations&quot;, error) Copy Once instrumentation is done and celery worker is running we can see trace for each celery task in Snappyflow server. For viewing trace in snappyflow server make sure project and app name is created or discovered with project name and app name specified in point no.2 Once project and app name is created, Go to View dashboard -&gt; Click on Tracing on left side bar -&gt; Click on view transaction -&gt; Go to real time tab Refer complete code: https://github.com/snappyflow/tracing-reference-apps/blob/master/ref-celery/tasks.py "},{"title":"AWS Lambda​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#aws-lambda","content":""},{"title":"Script​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#script-1","content":"Add these python libraries in requirements.txt file. Follow the AWS lambda doc on adding runtime dependency to lambda function. sf-apm-lib==0.1.1 sf-elastic-apm==6.7.2 Copy Ref: https://docs.aws.amazon.com/lambda/latest/dg/python-package-create.html#python-package-create-with-dependency Instrument lambda function to enable tracing. Import Libraries import elasticapm from sf_apm_lib.snappyflow import Snappyflow Copy Add code to get SnappyFlow Trace config, outside lambda handler method. sf = Snappyflow() SF_PROJECT_NAME = os.environ['SF_PROJECT_NAME'] SF_APP_NAME = os.environ['SF_APP_NAME'] SF_PROFILE_KEY = os.environ['SF_PROFILE_KEY'] sf.init(SF_PROFILE_KEY, SF_PROJECT_NAME, SF_APP_NAME) trace_config = snappyflow.get_trace_config() Copy Add custom instrumentation in lambda handler function def lambda_handler(event, context): client = elasticapm.Client(service_name=&quot;&lt;SERVICE_NAME_CHANGEME&gt;&quot;, server_url=trace_config['SFTRACE_SERVER_URL'], verify_cert=trace_config['SFTRACE_VERIFY_SERVER_CERT'], global_labels=trace_config['SFTRACE_GLOBAL_LABELS'] ) elasticapm.instrument() client.begin_transaction(transaction_type=&quot;script&quot;) # DO SOME WORK. No return statements. client.end_transaction(name=__name__, result=&quot;success&quot;) # RETURN STATEMENT e.g. return response Copy Deploy the Lambda function. Follow README to test sample app Sample code for reference: https://github.com/upendrasahu/aws-lambda-python-tracing-sample Configure Lambda function before trigger/invoke. Add the environment variable SF_PROFILE_KEY and set the value to your profile key copied from SnappyFlow. Add environment variables SF_APP_NAME and SF_PROJECT_NAME with appropriate values. "},{"title":"Log Correlation​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#log-correlation","content":""},{"title":"For enabling log correlation, follow below instructions​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#for-enabling-log-correlation-follow-below-instructions","content":""},{"title":"Django​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#django-4","content":"a. Add import statement in settings.py from elasticapm.handlers.logging import Formatter Copy b. Add following logging configuration in settings.py. LOGGING = { 'version': 1, 'disable_existing_loggers': True, // Disable existing logger 'formatters': { 'elastic': { // Add elastic formatter 'format': '[%(asctime)s] [%(levelname)s] [%(message)s]', 'class': 'elasticapm.handlers.logging.Formatter', 'datefmt': &quot;%d/%b/%Y %H:%M:%S&quot; } }, 'handlers': { 'elasticapm_log': { 'level': 'INFO', 'class': 'logging.handlers.RotatingFileHandler', 'filename': '/var/log/trace/django.log', //specify you log file path 'formatter': 'elastic' } }, 'loggers': { 'elasticapm': { 'handlers': ['elasticapm_log'], 'level': 'INFO', } } } Copy c. Usage: import logging log = logging.getLogger('elasticapm') class ExampleView(APIView): def get(self, request): log.info('Get API called') Copy Refer code: https://github.com/snappyflow/tracing-reference-apps/blob/master/refapp-django "},{"title":"Flask​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#flask-4","content":"Add following code in app.py after import statements to set logger configuration import logging from elasticapm.handlers.logging import Formatter fh = logging.FileHandler('/var/log/trace/flask.log') # we imported a custom Formatter from the Python Agent earlier formatter = Formatter(&quot;[%(asctime)s] [%(levelname)s] [%(message)s]&quot;, &quot;%d/%b/%Y %H:%M:%S&quot;) fh.setFormatter(formatter) logging.getLogger().addHandler(fh) # Once logging is configured get log object using following code log = logging.getLogger() log.setLevel('INFO') @app.route('/') def home(): log.info('Home API called') return 'Welcome to Home' Copy Refer code: https://github.com/snappyflow/tracing-reference-apps/blob/master/refapp-flask/app.py  "},{"title":"Send log correlation data to snappyflow server​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#send-log-correlation-data-to-snappyflow-server","content":"Below are the modes for sending log correlated data to snappyflow server "},{"title":" For Appliance: ​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#-for-appliance-","content":"Install sfagent and create config file. Refer: https://docs.snappyflow.io/docs/Integrations/os/linux/sfagent_linux Add elasticApmLog plugin to sfagent config.yaml and restart sfagent service. Eg. Config.yaml key: &lt;SF_PROFILE_KEY&gt; tags: Name: &lt;any-name&gt; appName: &lt;SF_APP_NAME&gt; projectName: &lt;SF_PROJECT_NAME&gt; logging: plugins: - name: elasticApmTraceLog enabled: true config: log_level: - error - warning - info log_path: /var/log/trace/ntrace.log # Your app log file path Copy "},{"title":" For Kubernetes: ​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#-for-kubernetes-","content":"Specify following values in metadata labels section of deployment file. snappyflow/appname: &lt;SF_APP_NAME&gt; snappyflow/projectname: &lt;SF_PROJECT_NAME&gt; snappyflow/component: gen-elastic-apm-log # This is must for tracing log correlation Copy "},{"title":"Sample deployment file​","type":1,"pageTitle":"Python tracing","url":"docs/Tracing/python#sample-deployment-file","content":"apiVersion: apps/v1 kind: Deployment metadata: labels: io.kompose.service: python-app snappyflow/appname: '&lt;sf_app_name&gt;' snappyflow/projectname: '&lt;sf_project_name&gt;' snappyflow/component: gen-elastic-apm-log name: python-app spec: replicas: 1 selector: matchLabels: io.kompose.service: python-app strategy: {} template: metadata: labels: io.kompose.service: python-app snappyflow/appname: '&lt;sf_app_name&gt;' snappyflow/projectname: '&lt;sf_project_name&gt;' snappyflow/component: gen-elastic-apm-log spec: containers: - env: - name: SF_APP_NAME value: '&lt;sf_app_name&gt;' - name: SF_PROFILE_KEY value: '&lt;sf_profile_key&gt;' - name: SF_PROJECT_NAME value: '&lt;sf_project_name&gt;' image: refapp-node:latest imagePullPolicy: Always name: python-app ports: - containerPort: 3000 resources: requests: cpu: 10m memory: 10Mi limits: cpu: 50m memory: 50Mi restartPolicy: Always Copy Note: For kubernetes mode we need sfagent pods to be running inside kubernetes cluster where your application pods are deployed. For viewing trace and logs in Snappyflow server make sure project and app name is created or discovered. Once project and app name is created. Go to: View App dashboard -&gt; Click on Tracing on left side bar -&gt; Click on view transaction -&gt; Go to real time tab Then click on any trace and go to logs tab to see the correlated logs to trace. # Note: To get trace in snappyflow server we need log entries to adhere following log format: &lt;date in following format&gt; [10/Aug/2021 10:51:16] [&lt;log_level&gt;] [&lt;message&gt;] | elasticapm transaction.id=&lt;transaction_id&gt; trace.id=&lt;trace_id&gt; span.id=&lt;span_id&gt; Copy "}]